{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finetuning Torchvision Models\n",
    "=============================\n",
    "\n",
    "**Author:** `Nathan Inkawhich <https://github.com/inkawhich>`__\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will take a deeper look at how to finetune and\n",
    "feature extract the `torchvision\n",
    "models <https://pytorch.org/docs/stable/torchvision/models.html>`__, all\n",
    "of which have been pretrained on the 1000-class Imagenet dataset. This\n",
    "tutorial will give an indepth look at how to work with several modern\n",
    "CNN architectures, and will build an intuition for finetuning any\n",
    "PyTorch model. Since each model architecture is different, there is no\n",
    "boilerplate finetuning code that will work in all scenarios. Rather, the\n",
    "researcher must look at the existing architecture and make custom\n",
    "adjustments for each model.\n",
    "\n",
    "In this document we will perform two types of transfer learning:\n",
    "finetuning and feature extraction. In **finetuning**, we start with a\n",
    "pretrained model and update *all* of the modelâ€™s parameters for our new\n",
    "task, in essence retraining the whole model. In **feature extraction**,\n",
    "we start with a pretrained model and only update the final layer weights\n",
    "from which we derive predictions. It is called feature extraction\n",
    "because we use the pretrained CNN as a fixed feature-extractor, and only\n",
    "change the output layer. For more technical information about transfer\n",
    "learning see `here <https://cs231n.github.io/transfer-learning/>`__ and\n",
    "`here <https://ruder.io/transfer-learning/>`__.\n",
    "\n",
    "In general both transfer learning methods follow the same few steps:\n",
    "\n",
    "-  Initialize the pretrained model\n",
    "-  Reshape the final layer(s) to have the same number of outputs as the\n",
    "   number of classes in the new dataset\n",
    "-  Define for the optimization algorithm which parameters we want to\n",
    "   update during training\n",
    "-  Run the training step\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.7.0\n",
      "Torchvision Version:  0.8.0a0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count()>1:\n",
    "    device = torch.device('cuda:2')\n",
    "elif torch.cuda.device_count()==1:\n",
    "    device = torch.device('cuda')\n",
    "else: \n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs\n",
    "------\n",
    "\n",
    "Here are all of the parameters to change for the run. We will use the\n",
    "*hymenoptera_data* dataset which can be downloaded\n",
    "`here <https://download.pytorch.org/tutorial/hymenoptera_data.zip>`__.\n",
    "This dataset contains two classes, **bees** and **ants**, and is\n",
    "structured such that we can use the\n",
    "`ImageFolder <https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.ImageFolder>`__\n",
    "dataset, rather than writing our own custom dataset. Download the data\n",
    "and set the ``data_dir`` input to the root directory of the dataset. The\n",
    "``model_name`` input is the name of the model you wish to use and must\n",
    "be selected from this list:\n",
    "\n",
    "::\n",
    "\n",
    "   [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "\n",
    "The other inputs are as follows: ``num_classes`` is the number of\n",
    "classes in the dataset, ``batch_size`` is the batch size used for\n",
    "training and may be adjusted according to the capability of your\n",
    "machine, ``num_epochs`` is the number of training epochs we want to run,\n",
    "and ``feature_extract`` is a boolean that defines if we are finetuning\n",
    "or feature extracting. If ``feature_extract = False``, the model is\n",
    "finetuned and all model parameters are updated. If\n",
    "``feature_extract = True``, only the last layer parameters are updated,\n",
    "the others remain fixed.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mpl = [r'D:\\seva\\hymenoptera_data',  r'D:/data/hymenoptera_data'] \n",
    "# mpl = [s.replace('\\\\','/') for s in mpl]\n",
    "# i = 0\n",
    "# while not os.path.exists(mpl[i]):\n",
    "#     print(i)\n",
    "#     i+=1\n",
    "# data_dir = mpl[i]\n",
    "# print(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "D:/seva/598_processing\n"
     ]
    }
   ],
   "source": [
    "depth = 2**8-1.\n",
    "D_out = 441\n",
    "C_in = 3 #how many views of one seed\n",
    "mpl = [r'/home/cherepashkin/data/598_processing',  r'D:\\data\\seeds\\598', r'D:\\seva\\598_processing'.replace('\\\\','/')] \n",
    "mpl = [s.replace('\\\\','/') for s in mpl]\n",
    "i = 0\n",
    "while not os.path.exists(mpl[i]):\n",
    "    print(i)\n",
    "    i+=1\n",
    "data_dir = mpl[i]\n",
    "print(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms \n",
    "#   to the ImageFolder structure\n",
    "#data_dir = D:\\seva\\hymenoptera_data\n",
    "#    \"D:/data/hymenoptera_data\"\n",
    "    #\"\"./data/hymenoptera_data\"\n",
    "data_dir\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"densenet\"\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 8\n",
    "\n",
    "# Number of epochs to train for \n",
    "num_epochs = 15\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model, \n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True\n",
    "\n",
    "#number of spherical harmonics amplituds to regress\n",
    "ampl = 16\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = ampl\n",
    "\n",
    "#downsample ply, taking point every ds steps\n",
    "ds = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions - train_model\n",
    "----------------\n",
    "\n",
    "Before we write the code for adjusting the models, lets define a few\n",
    "helper functions.\n",
    "\n",
    "Model Training and Validation Code\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "The ``train_model`` function handles the training and validation of a\n",
    "given model. As input, it takes a PyTorch model, a dictionary of\n",
    "dataloaders, a loss function, an optimizer, a specified number of epochs\n",
    "to train and validate for, and a boolean flag for when the model is an\n",
    "Inception model. The *is_inception* flag is used to accomodate the\n",
    "*Inception v3* model, as that architecture uses an auxiliary output and\n",
    "the overall model loss respects both the auxiliary output and the final\n",
    "output, as described\n",
    "`here <https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958>`__.\n",
    "The function trains for the specified number of epochs and after each\n",
    "epoch runs a full validation step. It also keeps track of the best\n",
    "performing model (in terms of validation accuracy), and at the end of\n",
    "training returns the best performing model. After each epoch, the\n",
    "training and validation accuracies are printed.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tmean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-77ff4c98dff3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mampl\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tmean' is not defined"
     ]
    }
   ],
   "source": [
    "print(tmean[:,:ampl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightv = torch.tensor([0.99, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss(output, target):\n",
    "    l = torch.mean(torch.multiply(weightv,(output - target))**2)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-7c6423bafbfc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-6164862e4262>\u001b[0m in \u001b[0;36mmy_loss\u001b[1;34m(output, target)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweightv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "my_loss(torch.zeros([8,9]),torch.zeros([8,9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'coef' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-305deed33ff2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigaspect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprojection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'3d'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m \u001b[0mYx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplot_Y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoef\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'coef' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_Y(ax, coef):\n",
    "    \"\"\"Plot the spherical harmonic of degree el and order m on Axes ax.\"\"\"\n",
    "    # Grids of polar and azimuthal angles\n",
    "    theta = torch.linspace(0, np.pi, 100)\n",
    "    phi = torch.linspace(0, 2*np.pi, 100)\n",
    "    #r = np.linspace(-1, 1, 100)\n",
    "    # Create a 2-D meshgrid of (theta, phi) angles.\n",
    "    theta, phi = torch.meshgrid(theta, phi)\n",
    "    # Calculate the Cartesian coordinates of each point in the mesh.\n",
    "    xyz = torch.tensor([torch.sin(theta) * torch.sin(phi),\n",
    "                    torch.sin(theta) * torch.cos(phi),\n",
    "                    torch.cos(theta)])\n",
    "    # NB In SciPy's sph_harm function the azimuthal coordinate, theta,\n",
    "    # comes before the polar coordinate, phi.\n",
    "    f = torch.zeros([100,100]).astype('complex128')\n",
    "    for l in range(int(np.sqrt(len(coef)))):\n",
    "        for m in range(-l,l+1):\n",
    "            fb = coef[l*(l+1)+m] * sph_harm(abs(m), l, phi, theta)\n",
    "            f += fb\n",
    "    Yx, Yy, Yz = torch.abs(f) * xyz\n",
    "    pc = torch.zeros((np.size(Yx), 3))\n",
    "    pc[:, 0] = torch.reshape(Yx, -1)\n",
    "    pc[:, 1] = torch.reshape(Yy, -1)\n",
    "    pc[:, 2] = torch.reshape(Yz, -1)\n",
    "#     cmap = plt.cm.ScalarMappable(cmap=plt.get_cmap('PRGn'))\n",
    "#     cmap.set_clim(-0.5, 0.5)\n",
    "\n",
    "#     ax.plot_surface(Yx, Yy, Yz,\n",
    "#                     facecolors=cmap.to_rgba(f.real),\n",
    "#                     rstride=2, cstride=2)\n",
    "#     print(Yz.shape)\n",
    "#     # Draw a set of x, y, z axes for reference.\n",
    "#     ax_lim = 50\n",
    "#     ax.plot([-ax_lim, ax_lim], [0,0], [0,0], c='0.5', lw=1, zorder=10)\n",
    "#     ax.plot([0,0], [-ax_lim, ax_lim], [0,0], c='0.5', lw=1, zorder=10)\n",
    "#     ax.plot([0,0], [0,0], [-ax_lim, ax_lim], c='0.5', lw=1, zorder=10)\n",
    "#     # Set the Axes limits and title, turn off the Axes frame.\n",
    "# #    ax.set_title(r'$Y_{{{},{}}}$'.format(el, m))\n",
    "#     ax_lim = 40\n",
    "#     ax.set_xlim(-ax_lim, ax_lim)\n",
    "#     ax.set_ylim(-ax_lim, ax_lim)\n",
    "#     ax.set_zlim(-ax_lim, ax_lim)\n",
    "#     ax.axis('on')\n",
    "    return(Yx, Yy, Yz)\n",
    "fig = plt.figure(figsize=plt.figaspect(1.))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "Yx, Yy, Yz = plot_Y(ax, coef)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand([50000,3])\n",
    "b = torch.rand([50000,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f0b6189c8fdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcurm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpnt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mcurm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpnt\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# for i in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# print(mn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mn = 100\n",
    "curm = torch.ones(a.shape[0])\n",
    "for i, pnt in enumerate(a):\n",
    "    curm[i] = torch.min(torch.sqrt(b**2-pnt**2))\n",
    "# for i in\n",
    "# print(mn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "continuous-multioutput is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-42b72a3fd0b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/p/software/jurecadc/stages/2020/software/scikit/2020-gcccoremkl-9.3.0-2020.2.254-Python-3.8.5/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/p/software/jurecadc/stages/2020/software/scikit/2020-gcccoremkl-9.3.0-2020.2.254-Python-3.8.5/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0mmodified\u001b[0m \u001b[0;32mwith\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mzero_division\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m     \"\"\"\n\u001b[0;32m-> 1044\u001b[0;31m     return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n\u001b[0m\u001b[1;32m   1045\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                        \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/p/software/jurecadc/stages/2020/software/scikit/2020-gcccoremkl-9.3.0-2020.2.254-Python-3.8.5/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/p/software/jurecadc/stages/2020/software/scikit/2020-gcccoremkl-9.3.0-2020.2.254-Python-3.8.5/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \"\"\"\n\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m     _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n\u001b[0m\u001b[1;32m   1169\u001b[0m                                                  \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m                                                  \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/p/software/jurecadc/stages/2020/software/scikit/2020-gcccoremkl-9.3.0-2020.2.254-Python-3.8.5/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/p/software/jurecadc/stages/2020/software/scikit/2020-gcccoremkl-9.3.0-2020.2.254-Python-3.8.5/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[0m\u001b[1;32m   1434\u001b[0m                                     pos_label)\n\u001b[1;32m   1435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/p/software/jurecadc/stages/2020/software/scikit/2020-gcccoremkl-9.3.0-2020.2.254-Python-3.8.5/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1248\u001b[0m                          str(average_options))\n\u001b[1;32m   1249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'binary'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/p/software/jurecadc/stages/2020/software/scikit/2020-gcccoremkl-9.3.0-2020.2.254-Python-3.8.5/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# No metrics support \"multiclass-multioutput\" format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multilabel-indicator\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: continuous-multioutput is not supported"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(a,b,average = 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d\n",
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fscore(gt: open3d.geometry.PointCloud, pr: open3d.geometry.PointCloud, th: float=0.01) -> typing.Tuple[float, float, float]:\n",
    "    '''Calculates the F-score between two point clouds with the corresponding threshold value.'''\n",
    "    d1 = gt.compute_point_cloud_distance(pr)\n",
    "    d2 = pr.compute_point_cloud_distance(gt)\n",
    "    if len(d1) and len(d2):\n",
    "        recall = float(sum(d < th for d in d2)) / float(len(d2))\n",
    "        precision = float(sum(d < th for d in d1)) / float(len(d1))\n",
    "\n",
    "        if recall+precision > 0:\n",
    "            fscore = 2 * recall * precision / (recall + precision)\n",
    "        else:\n",
    "            fscore = 0\n",
    "    else:\n",
    "        fscore = 0\n",
    "        precision = 0\n",
    "        recall = 0\n",
    "\n",
    "    return fscore, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = open3d.io.read_point_cloud('/p/home/jusers/cherepashkin1/jureca/cherepashkin1/598_processing/train/1484727/1493208_Surface.ply')\n",
    "pr = open3d.io.read_point_cloud('/p/home/jusers/cherepashkin1/jureca/cherepashkin1/598_processing/train/1484727/1493210_Surface.ply')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "open3d.cuda.pybind.geometry.PointCloud"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07813787460327148\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "calculate_fscore(gt,pr,th=0.1)\n",
    "print(time.time()-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.arange(0.0001, 0.5, 0.001)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001\n",
      "0.0011\n",
      "0.0021\n",
      "0.0031\n",
      "0.0041\n",
      "0.0051\n",
      "0.0061\n",
      "0.0071\n",
      "0.0081\n",
      "0.0091\n",
      "0.0101\n",
      "0.011099999999999999\n",
      "0.0121\n",
      "0.0131\n",
      "0.0141\n",
      "0.015099999999999999\n",
      "0.0161\n",
      "0.0171\n",
      "0.0181\n",
      "0.0191\n",
      "0.0201\n",
      "0.0211\n",
      "0.022099999999999998\n",
      "0.0231\n",
      "0.0241\n",
      "0.0251\n",
      "0.0261\n",
      "0.0271\n",
      "0.0281\n",
      "0.0291\n",
      "0.0301\n",
      "0.0311\n",
      "0.032100000000000004\n",
      "0.033100000000000004\n",
      "0.034100000000000005\n",
      "0.035100000000000006\n",
      "0.03610000000000001\n",
      "0.0371\n",
      "0.0381\n",
      "0.0391\n",
      "0.040100000000000004\n",
      "0.041100000000000005\n",
      "0.042100000000000005\n",
      "0.043100000000000006\n",
      "0.0441\n",
      "0.0451\n",
      "0.0461\n",
      "0.0471\n",
      "0.048100000000000004\n",
      "0.049100000000000005\n",
      "0.050100000000000006\n",
      "0.051100000000000007\n",
      "0.05210000000000001\n",
      "0.0531\n",
      "0.0541\n",
      "0.0551\n",
      "0.056100000000000004\n",
      "0.057100000000000005\n",
      "0.058100000000000006\n",
      "0.05910000000000001\n",
      "0.0601\n",
      "0.0611\n",
      "0.0621\n",
      "0.0631\n",
      "0.0641\n",
      "0.0651\n",
      "0.0661\n",
      "0.0671\n",
      "0.06810000000000001\n",
      "0.06910000000000001\n",
      "0.07010000000000001\n",
      "0.07110000000000001\n",
      "0.07210000000000001\n",
      "0.0731\n",
      "0.0741\n",
      "0.0751\n",
      "0.0761\n",
      "0.0771\n",
      "0.0781\n",
      "0.0791\n",
      "0.0801\n",
      "0.0811\n",
      "0.0821\n",
      "0.08310000000000001\n",
      "0.08410000000000001\n",
      "0.08510000000000001\n",
      "0.08610000000000001\n",
      "0.08710000000000001\n",
      "0.0881\n",
      "0.0891\n",
      "0.0901\n",
      "0.0911\n",
      "0.0921\n",
      "0.0931\n",
      "0.0941\n",
      "0.0951\n",
      "0.0961\n",
      "0.0971\n",
      "0.0981\n",
      "0.09910000000000001\n",
      "0.10010000000000001\n",
      "0.10110000000000001\n",
      "0.10210000000000001\n",
      "0.10310000000000001\n",
      "0.10410000000000001\n",
      "0.1051\n",
      "0.1061\n",
      "0.1071\n",
      "0.1081\n",
      "0.1091\n",
      "0.1101\n",
      "0.1111\n",
      "0.1121\n",
      "0.1131\n",
      "0.11410000000000001\n",
      "0.11510000000000001\n",
      "0.11610000000000001\n",
      "0.11710000000000001\n",
      "0.11810000000000001\n",
      "0.11910000000000001\n",
      "0.1201\n",
      "0.1211\n",
      "0.1221\n",
      "0.1231\n",
      "0.1241\n",
      "0.1251\n",
      "0.1261\n",
      "0.1271\n",
      "0.1281\n",
      "0.1291\n",
      "0.1301\n",
      "0.1311\n",
      "0.1321\n",
      "0.1331\n",
      "0.1341\n",
      "0.1351\n",
      "0.1361\n",
      "0.1371\n",
      "0.1381\n",
      "0.1391\n",
      "0.1401\n",
      "0.1411\n",
      "0.1421\n",
      "0.1431\n",
      "0.1441\n",
      "0.14509999999999998\n",
      "0.14609999999999998\n",
      "0.14709999999999998\n",
      "0.14809999999999998\n",
      "0.14909999999999998\n",
      "0.15009999999999998\n",
      "0.15109999999999998\n",
      "0.15209999999999999\n",
      "0.15309999999999999\n",
      "0.1541\n",
      "0.1551\n",
      "0.1561\n",
      "0.1571\n",
      "0.1581\n",
      "0.1591\n",
      "0.1601\n",
      "0.1611\n",
      "0.1621\n",
      "0.1631\n",
      "0.1641\n",
      "0.1651\n",
      "0.1661\n",
      "0.1671\n",
      "0.1681\n",
      "0.1691\n",
      "0.1701\n",
      "0.1711\n",
      "0.1721\n",
      "0.1731\n",
      "0.1741\n",
      "0.1751\n",
      "0.17609999999999998\n",
      "0.17709999999999998\n",
      "0.17809999999999998\n",
      "0.17909999999999998\n",
      "0.18009999999999998\n",
      "0.18109999999999998\n",
      "0.18209999999999998\n",
      "0.18309999999999998\n",
      "0.18409999999999999\n",
      "0.1851\n",
      "0.1861\n",
      "0.1871\n",
      "0.1881\n",
      "0.1891\n",
      "0.1901\n",
      "0.1911\n",
      "0.1921\n",
      "0.1931\n",
      "0.1941\n",
      "0.1951\n",
      "0.1961\n",
      "0.1971\n",
      "0.1981\n",
      "0.1991\n",
      "0.2001\n",
      "0.2011\n",
      "0.2021\n",
      "0.2031\n",
      "0.2041\n",
      "0.2051\n",
      "0.2061\n",
      "0.2071\n",
      "0.2081\n",
      "0.20909999999999998\n",
      "0.21009999999999998\n",
      "0.21109999999999998\n",
      "0.21209999999999998\n",
      "0.21309999999999998\n",
      "0.21409999999999998\n",
      "0.21509999999999999\n",
      "0.2161\n",
      "0.2171\n",
      "0.2181\n",
      "0.2191\n",
      "0.2201\n",
      "0.2211\n",
      "0.2221\n",
      "0.2231\n",
      "0.2241\n",
      "0.2251\n",
      "0.2261\n",
      "0.2271\n",
      "0.2281\n",
      "0.2291\n",
      "0.2301\n",
      "0.2311\n",
      "0.2321\n",
      "0.2331\n",
      "0.2341\n",
      "0.2351\n",
      "0.2361\n",
      "0.2371\n",
      "0.2381\n",
      "0.2391\n",
      "0.24009999999999998\n",
      "0.24109999999999998\n",
      "0.24209999999999998\n",
      "0.24309999999999998\n",
      "0.24409999999999998\n",
      "0.24509999999999998\n",
      "0.24609999999999999\n",
      "0.2471\n",
      "0.2481\n",
      "0.2491\n",
      "0.2501\n",
      "0.2511\n",
      "0.2521\n",
      "0.2531\n",
      "0.2541\n",
      "0.2551\n",
      "0.2561\n",
      "0.2571\n",
      "0.2581\n",
      "0.2591\n",
      "0.2601\n",
      "0.2611\n",
      "0.2621\n",
      "0.2631\n",
      "0.2641\n",
      "0.2651\n",
      "0.2661\n",
      "0.2671\n",
      "0.2681\n",
      "0.2691\n",
      "0.2701\n",
      "0.2711\n",
      "0.2721\n",
      "0.2731\n",
      "0.2741\n",
      "0.2751\n",
      "0.2761\n",
      "0.2771\n",
      "0.2781\n",
      "0.2791\n",
      "0.2801\n",
      "0.2811\n",
      "0.2821\n",
      "0.2831\n",
      "0.2841\n",
      "0.2851\n",
      "0.2861\n",
      "0.2871\n",
      "0.2881\n",
      "0.28909999999999997\n",
      "0.29009999999999997\n",
      "0.29109999999999997\n",
      "0.29209999999999997\n",
      "0.29309999999999997\n",
      "0.2941\n",
      "0.2951\n",
      "0.2961\n",
      "0.2971\n",
      "0.2981\n",
      "0.2991\n",
      "0.3001\n",
      "0.3011\n",
      "0.3021\n",
      "0.3031\n",
      "0.3041\n",
      "0.3051\n",
      "0.3061\n",
      "0.3071\n",
      "0.3081\n",
      "0.3091\n",
      "0.3101\n",
      "0.3111\n",
      "0.3121\n",
      "0.3131\n",
      "0.3141\n",
      "0.3151\n",
      "0.3161\n",
      "0.3171\n",
      "0.3181\n",
      "0.3191\n",
      "0.3201\n",
      "0.3211\n",
      "0.3221\n",
      "0.3231\n",
      "0.3241\n",
      "0.3251\n",
      "0.3261\n",
      "0.3271\n",
      "0.3281\n",
      "0.3291\n",
      "0.3301\n",
      "0.3311\n",
      "0.3321\n",
      "0.3331\n",
      "0.3341\n",
      "0.3351\n",
      "0.3361\n",
      "0.3371\n",
      "0.3381\n",
      "0.3391\n",
      "0.3401\n",
      "0.3411\n",
      "0.3421\n",
      "0.3431\n",
      "0.3441\n",
      "0.3451\n",
      "0.3461\n",
      "0.3471\n",
      "0.3481\n",
      "0.3491\n",
      "0.3501\n",
      "0.3511\n",
      "0.35209999999999997\n",
      "0.35309999999999997\n",
      "0.35409999999999997\n",
      "0.35509999999999997\n",
      "0.35609999999999997\n",
      "0.3571\n",
      "0.3581\n",
      "0.3591\n",
      "0.3601\n",
      "0.3611\n",
      "0.3621\n",
      "0.3631\n",
      "0.3641\n",
      "0.3651\n",
      "0.3661\n",
      "0.3671\n",
      "0.3681\n",
      "0.3691\n",
      "0.3701\n",
      "0.3711\n",
      "0.3721\n",
      "0.3731\n",
      "0.3741\n",
      "0.3751\n",
      "0.3761\n",
      "0.3771\n",
      "0.3781\n",
      "0.3791\n",
      "0.3801\n",
      "0.3811\n",
      "0.3821\n",
      "0.3831\n",
      "0.3841\n",
      "0.3851\n",
      "0.3861\n",
      "0.3871\n",
      "0.3881\n",
      "0.3891\n",
      "0.3901\n",
      "0.3911\n",
      "0.3921\n",
      "0.3931\n",
      "0.3941\n",
      "0.3951\n",
      "0.3961\n",
      "0.3971\n",
      "0.3981\n",
      "0.3991\n",
      "0.4001\n",
      "0.4011\n",
      "0.4021\n",
      "0.4031\n",
      "0.4041\n",
      "0.4051\n",
      "0.4061\n",
      "0.4071\n",
      "0.4081\n",
      "0.4091\n",
      "0.4101\n",
      "0.4111\n",
      "0.4121\n",
      "0.4131\n",
      "0.4141\n",
      "0.4151\n",
      "0.4161\n",
      "0.41709999999999997\n",
      "0.41809999999999997\n",
      "0.4191\n",
      "0.4201\n",
      "0.4211\n",
      "0.4221\n",
      "0.4231\n",
      "0.4241\n",
      "0.4251\n",
      "0.4261\n",
      "0.4271\n",
      "0.4281\n",
      "0.4291\n",
      "0.4301\n",
      "0.4311\n",
      "0.4321\n",
      "0.4331\n",
      "0.4341\n",
      "0.4351\n",
      "0.4361\n",
      "0.4371\n",
      "0.4381\n",
      "0.4391\n",
      "0.4401\n",
      "0.4411\n",
      "0.4421\n",
      "0.4431\n",
      "0.4441\n",
      "0.4451\n",
      "0.4461\n",
      "0.4471\n",
      "0.4481\n",
      "0.4491\n",
      "0.4501\n",
      "0.4511\n",
      "0.4521\n",
      "0.4531\n",
      "0.4541\n",
      "0.4551\n",
      "0.4561\n",
      "0.4571\n",
      "0.4581\n",
      "0.4591\n",
      "0.4601\n",
      "0.4611\n",
      "0.4621\n",
      "0.4631\n",
      "0.4641\n",
      "0.4651\n",
      "0.4661\n",
      "0.4671\n",
      "0.4681\n",
      "0.4691\n",
      "0.4701\n",
      "0.4711\n",
      "0.4721\n",
      "0.4731\n",
      "0.4741\n",
      "0.4751\n",
      "0.4761\n",
      "0.4771\n",
      "0.4781\n",
      "0.4791\n",
      "0.48009999999999997\n",
      "0.48109999999999997\n",
      "0.4821\n",
      "0.4831\n",
      "0.4841\n",
      "0.4851\n",
      "0.4861\n",
      "0.4871\n",
      "0.4881\n",
      "0.4891\n",
      "0.4901\n",
      "0.4911\n",
      "0.4921\n",
      "0.4931\n",
      "0.4941\n",
      "0.4951\n",
      "0.4961\n",
      "0.4971\n",
      "0.4981\n",
      "0.4991\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(0.0001, 0.5, 0.001):\n",
    "    print(i)\n",
    "#    print(calculate_fscore(gt,pr,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0282, 0.8806, 0.3572])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9349,    nan, 0.4901],\n",
       "        [0.8366,    nan, 0.4675],\n",
       "        [0.7480, 0.1249, 0.7555],\n",
       "        ...,\n",
       "        [0.7406,    nan, 0.3840],\n",
       "        [0.7658,    nan,    nan],\n",
       "        [0.7561,    nan, 0.6185]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sqrt(b-pnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "    \n",
    "#    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    lossar = np.zeros([2,num_epochs])\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            rloss = 0.0\n",
    "            \n",
    "            # Iterate over data.\n",
    "            for i_batch, sample_batched in enumerate(dataloaders[phase]):\n",
    "                if i_batch == bn:\n",
    "                    break\n",
    "                inputs = sample_batched['image']\n",
    "                labels = sample_batched['landmarks']\n",
    "#             for inputs, labels in dataloaders[phase]:\n",
    "#                 inputs = inputs.to(device)\n",
    "#                 labels = labels.to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        # self-supervised part\n",
    "                        \n",
    "                        loss = criterion(outputs, labels)\n",
    "                        #loss = my_loss(outputs,labels)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                rloss += loss.item()\n",
    "#                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            if phase == 'train':\n",
    "                lossar[0][epoch] = rloss/bn\n",
    "            else: \n",
    "                lossar[1][epoch] = rloss/bn            \n",
    "#            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.6f}'.format(phase, rloss/bn))\n",
    "#            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss,epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "#            if phase == 'val' and epoch_acc > best_acc:\n",
    "#                best_acc = epoch_acc\n",
    "#            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "#             if phase == 'val':\n",
    "#                 val_acc_history.append(epoch_acc)\n",
    "            val_acc_history = []\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "#    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "#    model.load_state_dict(best_model_wts)\n",
    "    return model, lossar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Model Parametersâ€™ .requires_grad attribute\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "This helper function sets the ``.requires_grad`` attribute of the\n",
    "parameters in the model to False when we are feature extracting. By\n",
    "default, when we load a pretrained model all of the parameters have\n",
    "``.requires_grad=True``, which is fine if we are training from scratch\n",
    "or finetuning. However, if we are feature extracting and only want to\n",
    "compute gradients for the newly initialized layer then we want all of\n",
    "the other parameters to not require gradients. This will make more sense\n",
    "later.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize and Reshape the Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes) \n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3 \n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "    \n",
    "    return model_ft, input_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet(\n",
      "  (features): Sequential(\n",
      "    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu0): ReLU(inplace=True)\n",
      "    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (denseblock1): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition1): _Transition(\n",
      "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock2): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition2): _Transition(\n",
      "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock3): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer13): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer14): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer15): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer16): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer17): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer18): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer19): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer20): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer21): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer22): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer23): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer24): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition3): _Transition(\n",
      "      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock4): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer13): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer14): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer15): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer16): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=1024, out_features=16, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## search of the min max of the sh amplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mainpath = data_dir + '\\train'\n",
    "# D_out = 441\n",
    "# if not(os.path.isfile('tmean.csv') and os.path.isfile(mainpath+'/sh.csv')):\n",
    "#     cip = []\n",
    "#     for root, directories, filenames in os.walk(mainpath): \n",
    "#         for filename in filenames:\n",
    "#             if filename[-8:] == 'F_20.csv':\n",
    "#                 cip.append(os.path.join(root,filename))\n",
    "#     labels = np.zeros([len(cip),D_out])\n",
    "#     for i in range(len(cip)):\n",
    "#         labels[i,:] = torch.tensor(np.genfromtxt(cip[i], delimiter='\\n'))\n",
    "#     tmean = np.zeros([4,D_out])\n",
    "#     tmean[0,:] = np.mean(labels, axis = 0)\n",
    "#     tmean[1,:] = np.std(labels, axis = 0)\n",
    "#     tmean[2,:] = np.min(labels, axis = 0)\n",
    "#     tmean[3,:] = np.max(labels, axis = 0)\n",
    "#     numpy.savetxt(\"tmean.csv\", tmean, delimiter=\",\")\n",
    "#     landmarks_frame = pd.DataFrame(data=labels,index=range(len(cip)),\\\n",
    "#                                    columns=['f'+str(i) for i in range(D_out)])\n",
    "#     landmarks_frame.insert(0, 'file_name', \\\n",
    "#                            [cip[i][-24:-9] for i in range(len(cip))])\n",
    "#     landmarks_frame.to_csv(mainpath+'/sh_paramters.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmean = np.genfromtxt(\"tmean.csv\", delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data\n",
    "---------\n",
    "\n",
    "Now that we know what the input size must be, we can initialize the data\n",
    "transforms, image datasets, and the dataloaders. Notice, the models were\n",
    "pretrained with the hard-coded normalization values, as described\n",
    "`here <https://pytorch.org/docs/master/torchvision/models.html>`__.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seed3D_Dataset(Dataset):\n",
    "    \"\"\"seed point cloud dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.landmarks_frame = pd.read_csv(os.path.join(root_dir,'sh.csv'))\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "#        img = np.zeros([40000, 3, 1])\n",
    "        rd = self.root_dir.replace('598test','598_processing')\n",
    "        img_name = \\\n",
    "        os.path.join(rd,\\\n",
    "                     self.landmarks_frame.iloc[idx, 0]+'_Surface.ply').replace('\\\\','/')\n",
    "        img = np.genfromtxt(img_name, skip_header = 7, skip_footer = 1)\n",
    "        img = np.concatenate((img, np.zeros([58014-img.shape[0],3])), axis=0)\n",
    "        landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
    "        landmarks = np.array([landmarks])\n",
    "        landmarks = landmarks.astype('float').reshape(-1, D_out)\n",
    "        sample = {'image': img, 'landmarks': landmarks}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data_dir,os.path.join(data_dir,'/sh_paramters.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(os.path.isfile('tmean.csv'),os.path.isfile(os.path.join(data_dir,'/sh_paramters.csv')))\n",
    "#print(not(os.path.isfile('tmean.csv') and os.path.isfile(mainpath+'/sh_paramters.csv')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmpCrop(object):\n",
    "    \"\"\"Crop the label, spherical harmonics amplitude.\"\"\"\n",
    "    def __init__(self, ampl):\n",
    "        self.ampl = ampl\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "        return {'image': image,\n",
    "                'landmarks': landmarks[:,:self.ampl]}\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "    def __init__(self, device):\n",
    "#        assert isinstance(device, str)\n",
    "        self.device = device\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "\n",
    "        landmarks = np.squeeze(landmarks)\n",
    "        return {'image': torch.Tensor(image).to(self.device),\n",
    "                'landmarks': torch.Tensor(landmarks).to(self.device)}\n",
    "class Minmax3Dimage(object):\n",
    "    \"\"\"Normalize 3D input data to be laying in [0,1]\"\"\"\n",
    "    def __init__(self,minmax):\n",
    "        minf = minmax[0]\n",
    "        maxf = minmax[1]\n",
    "        self.minf = minf\n",
    "        self.maxf = maxf\n",
    "#        assert isinstance(device, str)\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "        image = (image-self.minf)/(self.maxf-self.minf)\n",
    "        #         for i in range(3):\n",
    "#             image[:,i] = (image[:,i]-np.min(image,axis=0)[i])/\\\n",
    "#             (np.max(image,axis=0)[i]-np.min(image,axis=0)[i])\n",
    "        return {'image': image,\n",
    "                'landmarks': landmarks}\n",
    "\n",
    "class Downsample(object):\n",
    "    \"\"\"Downsample the input ply file.\"\"\"\n",
    "    def __init__(self, ds):\n",
    "        #assert isinstance(output_size, (int, tuple))\n",
    "        self.ds = ds\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "        ds_image = image[::self.ds,:]\n",
    "        return {'image': ds_image,\n",
    "                'landmarks': landmarks}\n",
    "class Shuffleinput(object):\n",
    "    \"\"\"Shuffle the rows of input ply file.\"\"\"\n",
    "    def __init__(self, shuffle_seed):\n",
    "        #assert isinstance(output_size, (int, tuple))\n",
    "        self.shuffle_seed = shuffle_seed\n",
    "    def __call__(self, sample):\n",
    "        np.random.seed(self.shuffle_seed)\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "        np.random.shuffle(image) \n",
    "        return {'image': image,\n",
    "                'landmarks': landmarks}\n",
    "class Minmax(object):\n",
    "    \"\"\"Normalize the input data to lay in [0,1].\"\"\"\n",
    "    def __init__(self, tmean):\n",
    "#        assert isinstance(tmean, numpy.ndarray)\n",
    "        self.tmean = tmean\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "#        print(type(landmarks))\n",
    "#        print(type(self.tmean[3]))\n",
    "#        landmarks = (landmarks - self.tmean[2])/(self.tmean[3]-self.tmean[2])\n",
    "        landmarks = (landmarks - np.min(self.tmean[2]))/(np.max(self.tmean[3])-np.min(self.tmean[2]))\n",
    "        \n",
    "        return {'image': image,\n",
    "                'landmarks': landmarks}\n",
    "class Reshape(object):\n",
    "    \"\"\"Normalize the input data to lay in [0,1].\"\"\"\n",
    "    def __init__(self, input_layer):\n",
    "#        assert isinstance(tmean, numpy.ndarray)\n",
    "        self.input_layer = input_layer\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "#        print(type(landmarks))\n",
    "#        print(type(self.tmean[3]))\n",
    "        padval = self.input_layer**2-image.shape[0]\n",
    "        if padval >= 0:\n",
    "            image = np.pad(image, ((0,padval),(0,0)), mode='constant')\n",
    "        else: \n",
    "            image = image[:self.input_layer**2]\n",
    "        image = np.reshape(image, [3,self.input_layer,self.input_layer])\n",
    "#        landmarks = (landmarks - self.tmean[2])/(self.tmean[3]-self.tmean[2]) \n",
    "        return {'image': image,\n",
    "                'landmarks': landmarks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-11610.34032578, -11610.34032578, -11610.34032578, -11610.34032578,\n",
       "       -11610.34032578])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(5)+np.min(tmean[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=np.zeros([50000,3])\n",
    "# a = np.pad(a, ((0,229**2-50000),(0,0)), mode='constant')\n",
    "# a = np.reshape(a, [229,229,3])\n",
    "# print(a.shape)\n",
    "# #a.reshape([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getsize(new_hi,ker,srd): \n",
    "    pad = (0,0)\n",
    "    dil = np.asarray((1,1))\n",
    "    return(tuple((np.squeeze((np.asarray(new_hi)+\\\n",
    "    2*np.asarray(pad)-dil*[np.asarray(ker)-1]+1)/\\\n",
    "                             np.asarray(srd))).astype(int)))\n",
    "class TNet(nn.Module):\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "#         new_h, ampl, m_kernel, m_stride, hidden_dim, C_in, ratio = \\\n",
    "#         tuple(tupa[i] for i in list(range(6, 10))+[11,16,17])\n",
    "        new_h = 58014\n",
    "        ampl = 9\n",
    "        m_kernel = 1\n",
    "        m_stride = 1\n",
    "        hidden_dim = 20\n",
    "        C_in = 1\n",
    "        new_w = 3\n",
    "        self.pool = nn.MaxPool2d(m_kernel,m_stride)\n",
    "        idt = tuple(map(lambda x: x, \\\n",
    "                getsize((new_h,new_w,m_kernel,m_stride))))\n",
    "        input_dim = idt[0]*idt[1]*C_in\n",
    "#        print(idt)\n",
    "        current_dim = input_dim\n",
    "        self.layers = nn.ModuleList()\n",
    "        for hdim in hidden_dim:\n",
    "            self.layers.append(nn.Linear(current_dim, hdim))\n",
    "            current_dim = hdim\n",
    "        self.layers.append(nn.Linear(current_dim, ampl))\n",
    "        \n",
    "    def forward(self, x):\n",
    "#        print('start',x.shape)\n",
    "        x = self.pool(x)\n",
    "#        print('view',x.shape)\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        for layer in self.layers[:-1]:\n",
    "#            print(x.shape)\n",
    "            x = F.leaky_relu(layer(x))\n",
    "        out = F.softmax(self.layers[-1](x))\n",
    "        return out  \n",
    "nnarchitectures = {'TNet':TNet}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_h = 58014\n",
    "new_w = 3\n",
    "ampl = 9\n",
    "m_kernel = 1\n",
    "m_stride = 2\n",
    "hidden_dim = 20\n",
    "C_in = 1\n",
    "\n",
    "#self.pool = nn.MaxPool2d(m_kernel,m_stride)\n",
    "idt = tuple(map(lambda x: x, getsize((new_h,new_w),m_kernel,m_stride)))\n",
    "idt = tuple(map(lambda x: x, getsize(idt,2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14503, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29006"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int((new_h-m_kernel)/m_stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNet, self).__init__()\n",
    "#         C_in = 1\n",
    "#         D_out = 9\n",
    "        self.conv0 = nn.Conv2d(1, 16, 1)\n",
    "#         self.conv1 = nn.Conv2d(6, 16, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.linear0 = torch.nn.Linear(464112, 9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "        x = self.pool(F.relu(self.conv0(x)))\n",
    "#         print(t.mean(x))\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "#         x = x.view(-1, x.size(1)*x.size(2)*x.size(3))\n",
    "#         print(x.shape)\n",
    "#     16, 464112\n",
    "        x = F.relu(self.linear0(x))\n",
    "#         x = F.relu(self.linear1(x))\n",
    "        #x = self.linear1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "ampl = 16\n",
    "batch_size = 16\n",
    "data_dir = 'D:/seva/598test'\n",
    "model_name = \"densenet\"\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "            AmpCrop(ampl),\\\n",
    "#             Minmax(tmean[:,:ampl]),\\\n",
    "            Minmax3Dimage((0.60117054538415,110.972068294924)),\\\n",
    "#            Downsample(ds),\\\n",
    "#           Shuffleinput(0),\\\n",
    "            Reshape(224),\\\n",
    "            ToTensor(device)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "            AmpCrop(ampl),\\\n",
    "#             Minmax(tmean[:,:ampl]),\\\n",
    "            Minmax3Dimage((0.60117054538415,110.972068294924)),\\\n",
    "#            Downsample(ds),\\\n",
    "#           Shuffleinput(0),\\\n",
    "            Reshape(224),\\\n",
    "            ToTensor(device)\n",
    "    ]),\n",
    "}\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: Seed3D_Dataset(root_dir=os.path.join(data_dir,x), \\\n",
    "                                    transform=data_transforms[x]) \n",
    "                  for x in ['train', 'val']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=False, num_workers=0) for x in ['train', 'val']}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# dataset = Seed3D_Dataset(csv_file=mainpath+'/sh_paramters.csv', root_dir=data_dir, transform=data_transform)\n",
    "# dataloader = DataLoader(dataset, bs,\n",
    "#                         shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phase = 'train'\n",
    "# bn = 100\n",
    "# for i_batch, sample_batched in enumerate(dataloaders_dict[phase]):\n",
    "#     print(i_batch)\n",
    "#     if i_batch == bn:\n",
    "#         break\n",
    "#     inputs = sample_batched['image']\n",
    "#     labels = sample_batched['landmarks']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Optimizer\n",
    "--------------------\n",
    "\n",
    "Now that the model structure is correct, the final step for finetuning\n",
    "and feature extracting is to create an optimizer that only updates the\n",
    "desired parameters. Recall that after loading the pretrained model, but\n",
    "before reshaping, if ``feature_extract=True`` we manually set all of the\n",
    "parameterâ€™s ``.requires_grad`` attributes to False. Then the\n",
    "reinitialized layerâ€™s parameters have ``.requires_grad=True`` by\n",
    "default. So now we know that *all parameters that have\n",
    ".requires_grad=True should be optimized.* Next, we make a list of such\n",
    "parameters and input this list to the SGD algorithm constructor.\n",
    "\n",
    "To verify this, check out the printed parameters to learn. When\n",
    "finetuning, this list should be long and include all of the model\n",
    "parameters. However, when feature extracting this list should be short\n",
    "and only include the weights and biases of the reshaped layers.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t classifier.weight\n",
      "\t classifier.bias\n"
     ]
    }
   ],
   "source": [
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are \n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Training and Validation Step\n",
    "--------------------------------\n",
    "\n",
    "Finally, the last step is to setup the loss for the model, then run the\n",
    "training and validation function for the set number of epochs. Notice,\n",
    "depending on the number of epochs this step may take a while on a CPU.\n",
    "Also, the default learning rate is not optimal for all of the models, so\n",
    "to achieve maximum accuracy it would be necessary to tune for each model\n",
    "separately.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.00 GiB total capacity; 6.02 GiB already allocated; 10.72 MiB free; 6.16 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-120-e868bfdab1a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0meps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-08\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             amsgrad=False)\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlossar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloaders_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscratch_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_inception\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"inception\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-37-d04823cca60d>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloaders, criterion, optimizer, num_epochs, is_inception)\u001b[0m\n\u001b[0;32m     47\u001b[0m                         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.4\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloss2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m                         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m                         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                         \u001b[1;31m#loss = my_loss(outputs,labels)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\sh\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\sh\\lib\\site-packages\\torchvision\\models\\densenet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madaptive_avg_pool2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\sh\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\sh\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\sh\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\sh\\lib\\site-packages\\torchvision\\models\\densenet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, init_features)\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0minit_features\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[0mnew_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\sh\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\sh\\lib\\site-packages\\torchvision\\models\\densenet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[0mbottleneck_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_checkpoint_bottleneck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprev_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m             \u001b[0mbottleneck_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprev_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mnew_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbottleneck_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\sh\\lib\\site-packages\\torchvision\\models\\densenet.py\u001b[0m in \u001b[0;36mbn_function\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;31m# type: (List[Tensor]) -> Tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mconcated_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mbottleneck_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcated_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# noqa: T484\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbottleneck_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\sh\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\sh\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\sh\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m    417\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 419\u001b[1;33m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0m\u001b[0;32m    420\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.00 GiB total capacity; 6.02 GiB already allocated; 10.72 MiB free; 6.16 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# Setup the loss fxn\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "bn = 128\n",
    "num_epochs = 10\n",
    "model_name = 'densenet'\n",
    "lossar = np.zeros([2,num_epochs])\n",
    "#scratch_optimizer = optim.SGD(scratch_model.parameters(), lr=0.001, momentum=0.9)\n",
    "#for i,lr in enumerate([0.0001, 0.0005, 0.001, 0.005]):\n",
    "num_classes = 16\n",
    "ampl = num_classes\n",
    "smodel, input_size = initialize_model(model_name, num_classes, feature_extract=False, use_pretrained=False)\n",
    "# Train and evaluate\n",
    "# smodel = CNet()\n",
    "smodel.to(device)\n",
    "scratch_optimizer = torch.optim.Adam(smodel.parameters(), \\\n",
    "            lr=5e-4, betas=(0.9, 0.999),\\\n",
    "            eps=1e-08, weight_decay=0, \\\n",
    "            amsgrad=False)\n",
    "model, lossar = train_model(smodel, dataloaders_dict, criterion, scratch_optimizer, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'set_parameter_requires_grad' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-a3df0a848aa4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#scratch_optimizer = optim.SGD(scratch_model.parameters(), lr=0.001, momentum=0.9)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mscratch_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitialize_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_extract\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_pretrained\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Train and evaluate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mscratch_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-79642852059d>\u001b[0m in \u001b[0;36minitialize_model\u001b[1;34m(model_name, num_classes, feature_extract, use_pretrained)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \"\"\"\n\u001b[0;32m     46\u001b[0m         \u001b[0mmodel_ft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdensenet121\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_pretrained\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mset_parameter_requires_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_extract\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0mnum_ftrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_ft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mmodel_ft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_ftrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'set_parameter_requires_grad' is not defined"
     ]
    }
   ],
   "source": [
    "# Setup the loss fxn\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "bn = 4\n",
    "num_epochs = 16\n",
    "model_name = 'densenet'\n",
    "lrl = [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]\n",
    "lossar = np.zeros([len(lrl),2,num_epochs])\n",
    "#scratch_optimizer = optim.SGD(scratch_model.parameters(), lr=0.001, momentum=0.9)\n",
    "for i,lr in enumerate(lrl):\n",
    "    scratch_model, input_size = initialize_model(model_name, num_classes, feature_extract=False, use_pretrained=False)\n",
    "    # Train and evaluate\n",
    "    scratch_model.to(device)\n",
    "    scratch_optimizer = torch.optim.Adam(scratch_model.parameters(), \\\n",
    "                lr, betas=(0.9, 0.999),\\\n",
    "                eps=1e-08, weight_decay=0, \\\n",
    "                amsgrad=False)\n",
    "    model, lossar[i] = train_model(scratch_model, dataloaders_dict, criterion, scratch_optimizer, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrl = [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]\n",
    "len(lrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "axes = plt.gca()\n",
    "ymin, ymax = 100, 2500\n",
    "axes.set_ylim([ymin,ymax])\n",
    "for i in range(2):\n",
    "    plt.plot(np.arange(lossar.shape[1]),lossar[i,:])\n",
    "plt.savefig('plot output/'+\\\n",
    "                'num_ampl='+str(ampl)+\\\n",
    "                'bn'+str(bn)+\\\n",
    "                'bs'+str(batch_size)+\\\n",
    "                'epochs'+str(num_epochs)+\\\n",
    "                'net_name=Densenet'+\\\n",
    "                'lr'+str(0.01)+\\\n",
    "                'optimizer adam'+\\\n",
    "                'time'+str(time.time())+\\\n",
    "            '.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lossar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    fig = plt.figure()\n",
    "    axes = plt.gca()\n",
    "    ymin, ymax = -5, 1\n",
    "    axes.set_ylim([ymin,ymax])\n",
    "    for j in range(lossar.shape[0]):\n",
    "        plt.plot(np.arange(lossar.shape[2]),np.log10(lossar[j,i,:]))\n",
    "    plt.savefig('plot output/'+\\\n",
    "                    'num_ampl='+str(ampl)+\\\n",
    "                    'bn'+str(bn)+\\\n",
    "                    'bs'+str(batch_size)+\\\n",
    "                    'epochs'+str(num_epochs)+\\\n",
    "                    'net_name=Densenet'+\\\n",
    "                    'lr'+str(0.01)+\\\n",
    "                    'optimizer adam'+\\\n",
    "                    'time'+str(time.time())+\\\n",
    "                '.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linestyles = ['-', '--']\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink']\n",
    "fig, axs = plt.subplots(2, 4, figsize=(15, 6), sharex=True, sharey=True)\n",
    "# fig = plt.figure()\n",
    "# axes = plt.gca()\n",
    "axs = axs.ravel()\n",
    "# ymin, ymax = -5, 1\n",
    "# axs.set_ylim([ymin,ymax])\n",
    "for j in range(lossar.shape[0]):\n",
    "    for i in range(2):\n",
    "        axs[j].plot(np.arange(lossar.shape[2]),np.log10(lossar[j,i,:]),linestyle=linestyles[i],color=colors[j])\n",
    "    plt.savefig('plot output/'+\\\n",
    "                    'num_ampl='+str(ampl)+\\\n",
    "                    'bn'+str(bn)+\\\n",
    "                    'bs'+str(batch_size)+\\\n",
    "                    'epochs'+str(num_epochs)+\\\n",
    "                    'net_name=Densenet'+\\\n",
    "                    'lr'+str(0.01)+\\\n",
    "                    'optimizer adam'+\\\n",
    "                    'time'+str(time.time())+\\\n",
    "                '.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create some toy data:\n",
    "x = np.linspace(0, 2*np.pi, 400)\n",
    "y = np.sin(x**2)\n",
    "\n",
    "# Create just a figure and only one subplot\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y)\n",
    "ax.set_title('Simple plot')\n",
    "\n",
    "# Create two subplots and unpack the output array immediately\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "ax1.plot(x, y)\n",
    "ax1.set_title('Sharing Y axis')\n",
    "ax2.scatter(x, y)\n",
    "\n",
    "# Create four polar axes and access them through the returned array\n",
    "fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n",
    "axs[0, 0].plot(x, y)\n",
    "axs[1, 1].scatter(x, y)\n",
    "\n",
    "# Share a X axis with each column of subplots\n",
    "plt.subplots(2, 2, sharex='col')\n",
    "\n",
    "# Share a Y axis with each row of subplots\n",
    "plt.subplots(2, 2, sharey='row')\n",
    "\n",
    "# Share both X and Y axes with all subplots\n",
    "plt.subplots(2, 2, sharex='all', sharey='all')\n",
    "\n",
    "# Note that this is the same as\n",
    "plt.subplots(2, 2, sharex=True, sharey=True)\n",
    "\n",
    "# Create figure number 10 with a single subplot\n",
    "# and clears it if it already exists.\n",
    "fig, ax = plt.subplots(num=10, clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linestyles = ['-', '--']\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink']\n",
    "fig = plt.figure()\n",
    "axes = plt.gca()\n",
    "ymin, ymax = -5, 1\n",
    "axes.set_ylim([ymin,ymax])\n",
    "for j in range(lossar.shape[0]):\n",
    "    for i in range(2):\n",
    "        plt.plot(np.arange(lossar.shape[2]),np.log10(lossar[j,i,:]),linestyle=linestyles[i],color=colors[j])\n",
    "        plt.savefig('plot output/'+\\\n",
    "                        'num_ampl='+str(ampl)+\\\n",
    "                        'bn'+str(bn)+\\\n",
    "                        'bs'+str(batch_size)+\\\n",
    "                        'epochs'+str(num_epochs)+\\\n",
    "                        'net_name=Densenet'+\\\n",
    "                        'lr'+str(0.01)+\\\n",
    "                        'optimizer adam'+\\\n",
    "                        'time'+str(time.time())+\\\n",
    "                    '.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return SH coefficient vector from the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tmean[:,:ampl].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = 'val'\n",
    "bn = 1\n",
    "ampl = 9\n",
    "dataloaders = dataloaders_dict\n",
    "for i_batch, sample_batched in enumerate(dataloaders[phase]):\n",
    "    if i_batch == bn:\n",
    "        break\n",
    "    inputs = sample_batched['image']\n",
    "    gt = sample_batched['landmarks']\n",
    "    o = model(inputs)\n",
    "#    print(model(inputs).shape)\n",
    "#    output = np.multiply(model(inputs).detach().cpu().numpy(),tmean[3,:ampl]-tmean[2,:ampl])+tmean[2,:ampl]\n",
    "#    real_output = np.multiply(gt.detach().cpu().numpy(),tmean[3,:ampl]-tmean[2,:ampl])+tmean[2,:ampl]\n",
    "#     print(output[0])\n",
    "#     print(real_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0864e+02,  1.4140e+00, -2.5845e-01, -4.5770e-02,  6.0368e-01,\n",
       "         6.0523e-01,  2.6567e+01, -3.7471e-01, -7.3613e+00, -3.3841e-02,\n",
       "        -1.6963e+00, -8.3987e-01,  2.4902e-01,  2.0087e-01,  7.6525e-01,\n",
       "         3.7502e-01], device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seed before and after supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison with Model Trained from Scratch\n",
    "------------------------------------------\n",
    "\n",
    "Just for fun, lets see how the model learns if we do not use transfer\n",
    "learning. The performance of finetuning vs.Â feature extracting depends\n",
    "largely on the dataset but in general both transfer learning methods\n",
    "produce favorable results in terms of training time and overall accuracy\n",
    "versus a model trained from scratch.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the non-pretrained version of the model used for this run\n",
    "model_names = [\"resnet\", \"alexnet\", \"vgg\", \"squeezenet\", \"densenet\"]\n",
    "scratch_hist = np.zeros([len(model_names),2,num_epochs])\n",
    "for i, model_name in  enumerate(model_names):\n",
    "    scratch_model,_ = initialize_model(model_name, num_classes, feature_extract=False, use_pretrained=False)\n",
    "    scratch_model = scratch_model.to(device)\n",
    "    scratch_optimizer = optim.SGD(scratch_model.parameters(), lr=0.001, momentum=0.9)\n",
    "    scratch_criterion = nn.MSELoss('mean')\n",
    "    #scratch_criterion = nn.CrossEntropyLoss()\n",
    "    _,scratch_hist[i] = train_model(scratch_model, dataloaders_dict, scratch_criterion, \\\n",
    "                                 scratch_optimizer, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch_hist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linestyles = ['-','--']\n",
    "for i in range(2):\n",
    "    plt.plot(np.arange(num_epochs),scratch_hist[i,:],color = 'tab:blue',linestyle = linestyles[i],linewidth = 3)\n",
    "plt.savefig('plot output/'+\\\n",
    "                'num_ampl='+str(ampl)+\\\n",
    "                'bn'+str(bn)+\\\n",
    "                'bs'+str(batch_size)+\\\n",
    "                'epochs'+str(num_epochs)+\\\n",
    "                'net_names'+str(net_names)+\\\n",
    "                'lr'+str(0.001)+\\\n",
    "                'momentum'+str(0.9)+\\\n",
    "                'optimizer SGD'+\\\n",
    "                'scratch'+str(1)+\\\n",
    "                'time'+str(time.time())+\\\n",
    "            '.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training curves of validation accuracy vs. number \n",
    "#  of training epochs for the transfer learning method and\n",
    "#  the model trained from scratch\n",
    "ohist = []\n",
    "shist = []\n",
    "\n",
    "ohist = [h.cpu().numpy() for h in hist]\n",
    "shist = [h.cpu().numpy() for h in scratch_hist]\n",
    "\n",
    "plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
    "plt.plot(range(1,num_epochs+1),shist,label=\"Scratch\")\n",
    "plt.ylim((0,1.))\n",
    "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Thoughts and Where to Go Next\n",
    "-----------------------------------\n",
    "\n",
    "Try running some of the other models and see how good the accuracy gets.\n",
    "Also, notice that feature extracting takes less time because in the\n",
    "backward pass we do not have to calculate most of the gradients. There\n",
    "are many places to go from here. You could:\n",
    "\n",
    "-  Run this code with a harder dataset and see some more benefits of\n",
    "   transfer learning\n",
    "-  Using the methods described here, use transfer learning to update a\n",
    "   different model, perhaps in a new domain (i.e.Â NLP, audio, etc.)\n",
    "-  Once you are happy with a model, you can export it as an ONNX model,\n",
    "   or trace it using the hybrid frontend for more speed and optimization\n",
    "   opportunities.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## search for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Setup the loss fxn\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "bn = 1\n",
    "#model_ft = None\n",
    "num_epochs = 10\n",
    "net_names = [\"resnet\", \"alexnet\", \"vgg\", \"squeezenet\", \"densenet\"]\n",
    "#[\"vgg\", \"squeezenet\"]\n",
    "lossa = np.zeros([len(net_names),2,num_epochs])\n",
    "#[\"resnet\", \"alexnet\", \"vgg\", \"squeezenet\", \"densenet\"]\n",
    "for i, model_name in enumerate(net_names):\n",
    "# Train and evaluate\n",
    "    model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "    print(input_size)\n",
    "    model_ft.to(device)\n",
    "    model_ft, lossar = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))\n",
    "    lossa[i] = lossar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scratch_hist.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "axes = plt.gca()\n",
    "ymin, ymax = 0, 10\n",
    "axes.set_ylim([ymin,ymax])\n",
    "linestyles = ['-', '--', '-.', ':', '-', '--',\\\n",
    "                  '-.', ':', '-', '--', '-.', ':']\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple']\n",
    "lossa = scratch_hist\n",
    "for i in range(lossa.shape[0]):\n",
    "    for j in range(lossa.shape[1]):\n",
    "        plt.plot(np.arange(num_epochs), \\\n",
    "                 lossa[i,j,:], \\\n",
    "                 color = colors[i], linestyle=linestyles[j], \\\n",
    "                 linewidth=3)\n",
    "plt.savefig('plot output/'+\\\n",
    "                'num_ampl='+str(ampl)+\\\n",
    "                'bn'+str(bn)+\\\n",
    "                'bs'+str(batch_size)+\\\n",
    "                'epochs'+str(num_epochs)+\\\n",
    "                'net_names'+str(net_names)+\\\n",
    "                'lr'+str(0.001)+\\\n",
    "                'momentum'+str(0.9)+\\\n",
    "                'optimizer SGD'+\\\n",
    "                'time'+str(time.time())+\\\n",
    "            '.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linestyles = ['-', '--', '-.', ':', '-', '--',\\\n",
    "                  '-.', ':', '-', '--', '-.', ':']\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple']\n",
    "lossa = scratch_hist\n",
    "for j in range(lossa.shape[1]):\n",
    "    fig = plt.figure()\n",
    "    axes = plt.gca()\n",
    "    ymin, ymax = 0, 10\n",
    "    axes.set_ylim([ymin,ymax])\n",
    "    for i in range(lossa.shape[0]):\n",
    "        plt.plot(np.arange(num_epochs), \\\n",
    "                 lossa[i,j,:], \\\n",
    "                 color = colors[i], linestyle=linestyles[j], \\\n",
    "                 linewidth=3)\n",
    "    plt.savefig('plot output/'+\\\n",
    "                    'num_ampl='+str(ampl)+\\\n",
    "                    'bn'+str(bn)+\\\n",
    "                    'bs'+str(batch_size)+\\\n",
    "                    'epochs'+str(num_epochs)+\\\n",
    "                    'net_names'+str(net_names)+\\\n",
    "                    'lr'+str(0.001)+\\\n",
    "                    'momentum'+str(0.9)+\\\n",
    "                    'optimizer SGD'+\\\n",
    "                    'phase'+str(j)+\\\n",
    "                    'time'+str(time.time())+\\\n",
    "                '.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## search for maximum size of the ply file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cip = []\n",
    "for root, directories, filenames in os.walk(data_dir): \n",
    "    for filename in filenames:\n",
    "        if filename[-4:] == '.ply':\n",
    "            cip.append(os.path.join(root,filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cip)\n",
    "cip[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxsize = 0\n",
    "sizear = []\n",
    "i = 0\n",
    "sumav = 0\n",
    "for path in cip:\n",
    "    img = np.genfromtxt(path, skip_header = 7, skip_footer = 1)\n",
    "    sumav += os.stat(path).st_size/img.shape[0]\n",
    "\n",
    "    i+=1\n",
    "    if i == 50:\n",
    "        break\n",
    "    \n",
    "sumav/50\n",
    "\n",
    "#     sizear.append(img.shape[0])\n",
    "#     if img.shape[0]>maxsize:\n",
    "#         maxsize = img.shape[0]\n",
    "# print(maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "32.761725985875024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxsize = 0\n",
    "sizear = np.zeros(len(cip))\n",
    "i = 0\n",
    "sumav = 0\n",
    "for i, path in enumerate(cip):\n",
    "#    img = np.genfromtxt(path, skip_header = 7, skip_footer = 1)\n",
    "    sizear[i] = os.stat(path).st_size/32.761725985875024\n",
    "    \n",
    "#     sizear.append(img.shape[0])\n",
    "#     if img.shape[0]>maxsize:\n",
    "#         maxsize = img.shape[0]\n",
    "# print(maxsize)\n",
    "plt.plot(np.sort(sizear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(sizear))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdl_venv",
   "language": "python",
   "name": "sdl_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
