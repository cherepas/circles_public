The activation script must be sourced, otherwise the virtual environment will not work.
Setting vars
iscuda= True
iscuda= True
iscuda= True
iscuda= True
PyTorch Version:  1.8.1
Torchvision Version:  0.9.0a0
opt:
 Namespace(ampl=441, aug_gt='orient', batch_output=2, bs=15, cencrop=700, center=False, chidden_dim=[96, 128, 256, 256, 256], classicnorm=False, cmscrop=0, conTrain='', criterion='L2', csvname='598csv9', datapath='/p/project/delia-mp/cherepashkin1/phenoseed/', dfname='598frame', downsample=1, epoch=300, expdescr='', expnum='e072', feature_extract=False, framelim=6000, gradient_predivide_factor=1.0, gttype='single_file', haf=True, hidden_dim=[64, 9], inputt='img', jobdir='/var/spool/parastation/jobs', jobname='10162423', kernel_sizes=[7, 3, 3, 3, 3, 3], lb='orient', loadh5=False, localexp='', lr=5e-05, machine='jureca', maintain=False, maintain_line=False, man_dist=False, measure_time=False, merging='batch', minmax=False, minmax3dimage=False, minmax_f=True, minmax_fn='', model_name='', netname=['cnet'], ngpu=4, noise_input=False, noise_output=False, normalize=False, num_input_images=3, num_sam_points=500, num_workers=0, outputt='orient', parallel='horovod', pin_memory=False, print_minibatch=10, pscale=100, rand_angle=False, rescale=500, rot_dirs=False, rotate_output=False, save_output=True, single_folder=False, specie='598', standardize=255, steplr=[1000.0, 1.0], transappendix='_image', ufmodel=100000, updateFraction=0.25, use_adasum=False, use_cuda=True, use_existing_csv=True, use_pretrained=False, use_sep_csv=True, view_sep=False, wandb='', weight_decay=0, zero_angle=True)
sys.argv:
 ['../../main.py', '-datapath', '/p/project/delia-mp/cherepashkin1/phenoseed/', '-jobname', '10162423', '-jobdir', '/var/spool/parastation/jobs', '-epoch', '300', '-bs', '15', '-num_input_images', '3', '-framelim', '6000', '-criterion', 'L2', '-localexp', '', '-lr', '5e-5', '-expnum', 'e072', '-hidden_dim', '64', '9', '-inputt', 'img', '-outputt', 'orient', '-lb', 'orient', '-no_loadh5', '-minmax_fn', '', '-parallel', 'horovod', '-machine', 'jureca', '-merging', 'batch', '-aug_gt', 'orient', '-updateFraction', '0.25', '-steplr', '1000', '1', '-print_minibatch', '10', '-dfname', '598frame']
seed =  0
path were main.py is located= ../../
opt.wandb =  
93
opt.wandb =  
93
opt.wandb =  
93
file to frame csv ../../csv/598frame.csv
file to frame csv ../../csv/598frame.csv
file to frame csv ../../csv/598frame.csv
opt.wandb =  
93
file to frame csv ../../csv/598frame.csv
lframe's length after laoding =  5283
lframe's length after laoding =  5283
lframe's length after laoding =  5283
lframe's length after laoding =  5283
lframe len after excluding all exceptions= 5200
lframe len after excluding all exceptions= 5200
lframe len after excluding all exceptions= 5200
lframe len after excluding all exceptions= 5200
len train =  4160
len train =  4160
len train =  4160
len train =  4160
train consists of 277 full batches with 15 tensors with 3 views
the last batch has size of 5 tensors with 3 views
val consists of 69 full batches with 15 tensors with 3 views
the last batch has size of 5 tensors with 3 views
Epoch 0/299
----------
Mon Jan 31 17:46:01 2022
batch 0, train loss = 0.37, mean loss = 0.37
Mon Jan 31 17:46:17 2022
batch 10, train loss = 26.28, mean loss = 62.11
Mon Jan 31 17:47:28 2022
batch 20, train loss = 27.70, mean loss = 49.09
Mon Jan 31 17:48:40 2022
batch 30, train loss = 14.80, mean loss = 35.59
Mon Jan 31 17:49:50 2022
batch 40, train loss = 3.44, mean loss = 29.04
Mon Jan 31 17:51:00 2022
batch 50, train loss = 7.90, mean loss = 24.40
Mon Jan 31 17:52:11 2022
batch 60, train loss = 0.80, mean loss = 20.89
Mon Jan 31 17:53:21 2022
train Loss: 18.67

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 17:54:27 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 17:55:38 2022
val Loss: 0.03

epoch 0 was done for 625.409999 seconds
Epoch 1/299
----------
Mon Jan 31 17:56:27 2022
batch 0, train loss = 3.28, mean loss = 3.28
Mon Jan 31 17:56:33 2022
batch 10, train loss = 1.66, mean loss = 1.53
Mon Jan 31 17:57:31 2022
batch 20, train loss = 2.07, mean loss = 1.85
Mon Jan 31 17:58:31 2022
batch 30, train loss = 1.26, mean loss = 1.58
Mon Jan 31 17:59:30 2022
batch 40, train loss = 1.52, mean loss = 1.57
Mon Jan 31 18:00:28 2022
batch 50, train loss = 0.78, mean loss = 1.44
Mon Jan 31 18:01:24 2022
batch 60, train loss = 1.25, mean loss = 1.40
Mon Jan 31 18:02:21 2022
train Loss: 1.32

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 18:03:14 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 18:04:09 2022
val Loss: 0.02

epoch 1 was done for 499.826986 seconds
Epoch 2/299
----------
Mon Jan 31 18:04:47 2022
batch 0, train loss = 0.55, mean loss = 0.55
Mon Jan 31 18:04:52 2022
batch 10, train loss = 0.85, mean loss = 0.86
Mon Jan 31 18:05:47 2022
batch 20, train loss = 0.55, mean loss = 0.74
Mon Jan 31 18:06:43 2022
batch 30, train loss = 0.78, mean loss = 0.77
Mon Jan 31 18:07:38 2022
batch 40, train loss = 0.41, mean loss = 0.73
Mon Jan 31 18:08:35 2022
batch 50, train loss = 0.56, mean loss = 0.73
Mon Jan 31 18:09:34 2022
batch 60, train loss = 0.53, mean loss = 0.70
Mon Jan 31 18:10:36 2022
train Loss: 0.69

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 18:11:35 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 18:12:40 2022
val Loss: 0.02

epoch 2 was done for 517.555299 seconds
Epoch 3/299
----------
Mon Jan 31 18:13:24 2022
batch 0, train loss = 0.50, mean loss = 0.50
Mon Jan 31 18:13:31 2022
batch 10, train loss = 0.60, mean loss = 0.49
Mon Jan 31 18:14:35 2022
batch 20, train loss = 0.49, mean loss = 0.50
Mon Jan 31 18:15:40 2022
batch 30, train loss = 0.54, mean loss = 0.49
Mon Jan 31 18:16:45 2022
batch 40, train loss = 0.35, mean loss = 0.49
Mon Jan 31 18:17:49 2022
batch 50, train loss = 0.42, mean loss = 0.48
Mon Jan 31 18:18:53 2022
batch 60, train loss = 0.37, mean loss = 0.48
Mon Jan 31 18:19:55 2022
train Loss: 0.47

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 18:20:52 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 18:21:53 2022
val Loss: 0.02

epoch 3 was done for 548.803981 seconds
Epoch 4/299
----------
Mon Jan 31 18:22:33 2022
batch 0, train loss = 0.39, mean loss = 0.39
Mon Jan 31 18:22:39 2022
batch 10, train loss = 0.43, mean loss = 0.40
Mon Jan 31 18:23:36 2022
batch 20, train loss = 0.40, mean loss = 0.40
Mon Jan 31 18:24:32 2022
batch 30, train loss = 0.42, mean loss = 0.40
Mon Jan 31 18:25:27 2022
batch 40, train loss = 0.31, mean loss = 0.39
Mon Jan 31 18:26:22 2022
batch 50, train loss = 0.33, mean loss = 0.39
Mon Jan 31 18:27:17 2022
batch 60, train loss = 0.30, mean loss = 0.39
Mon Jan 31 18:28:12 2022
train Loss: 0.39

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 18:29:02 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 18:29:57 2022
val Loss: 0.02

epoch 4 was done for 481.952423 seconds
Epoch 5/299
----------
Mon Jan 31 18:30:35 2022
batch 0, train loss = 0.30, mean loss = 0.30
Mon Jan 31 18:30:41 2022
batch 10, train loss = 0.37, mean loss = 0.35
Mon Jan 31 18:31:35 2022
batch 20, train loss = 0.34, mean loss = 0.35
Mon Jan 31 18:32:29 2022
batch 30, train loss = 0.38, mean loss = 0.35
Mon Jan 31 18:33:24 2022
batch 40, train loss = 0.27, mean loss = 0.35
Mon Jan 31 18:34:18 2022
batch 50, train loss = 0.30, mean loss = 0.35
Mon Jan 31 18:35:12 2022
batch 60, train loss = 0.28, mean loss = 0.35
Mon Jan 31 18:36:07 2022
train Loss: 0.35

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 18:36:57 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 18:37:52 2022
val Loss: 0.02

epoch 5 was done for 475.152122 seconds
Epoch 6/299
----------
Mon Jan 31 18:38:30 2022
batch 0, train loss = 0.26, mean loss = 0.26
Mon Jan 31 18:38:36 2022
batch 10, train loss = 0.36, mean loss = 0.33
Mon Jan 31 18:39:29 2022
batch 20, train loss = 0.31, mean loss = 0.33
Mon Jan 31 18:40:23 2022
batch 30, train loss = 0.34, mean loss = 0.33
Mon Jan 31 18:41:17 2022
batch 40, train loss = 0.25, mean loss = 0.33
Mon Jan 31 18:42:11 2022
batch 50, train loss = 0.29, mean loss = 0.33
Mon Jan 31 18:43:04 2022
batch 60, train loss = 0.27, mean loss = 0.33
Mon Jan 31 18:43:58 2022
train Loss: 0.33

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 18:44:48 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 18:45:43 2022
val Loss: 0.02

epoch 6 was done for 470.039352 seconds
Epoch 7/299
----------
Mon Jan 31 18:46:20 2022
batch 0, train loss = 0.24, mean loss = 0.24
Mon Jan 31 18:46:26 2022
batch 10, train loss = 0.33, mean loss = 0.32
Mon Jan 31 18:47:19 2022
batch 20, train loss = 0.30, mean loss = 0.32
Mon Jan 31 18:48:13 2022
batch 30, train loss = 0.32, mean loss = 0.33
Mon Jan 31 18:49:07 2022
batch 40, train loss = 0.24, mean loss = 0.32
Mon Jan 31 18:50:01 2022
batch 50, train loss = 0.29, mean loss = 0.32
Mon Jan 31 18:50:55 2022
batch 60, train loss = 0.26, mean loss = 0.32
Mon Jan 31 18:51:49 2022
train Loss: 0.32

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 18:52:39 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 18:53:34 2022
val Loss: 0.02

epoch 7 was done for 471.355473 seconds
Epoch 8/299
----------
Mon Jan 31 18:54:11 2022
batch 0, train loss = 0.23, mean loss = 0.23
Mon Jan 31 18:54:17 2022
batch 10, train loss = 0.32, mean loss = 0.32
Mon Jan 31 18:55:10 2022
batch 20, train loss = 0.29, mean loss = 0.32
Mon Jan 31 18:56:03 2022
batch 30, train loss = 0.31, mean loss = 0.32
Mon Jan 31 18:56:56 2022
batch 40, train loss = 0.24, mean loss = 0.32
Mon Jan 31 18:57:49 2022
batch 50, train loss = 0.29, mean loss = 0.32
Mon Jan 31 18:58:42 2022
batch 60, train loss = 0.25, mean loss = 0.32
Mon Jan 31 18:59:36 2022
train Loss: 0.32

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 19:00:26 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 19:01:21 2022
val Loss: 0.02

epoch 8 was done for 467.162795 seconds
Epoch 9/299
----------
Mon Jan 31 19:01:59 2022
batch 0, train loss = 0.23, mean loss = 0.23
Mon Jan 31 19:02:04 2022
batch 10, train loss = 0.32, mean loss = 0.31
Mon Jan 31 19:02:58 2022
batch 20, train loss = 0.28, mean loss = 0.31
Mon Jan 31 19:03:52 2022
batch 30, train loss = 0.31, mean loss = 0.32
Mon Jan 31 19:04:46 2022
batch 40, train loss = 0.23, mean loss = 0.31
Mon Jan 31 19:05:39 2022
batch 50, train loss = 0.29, mean loss = 0.32
Mon Jan 31 19:06:33 2022
batch 60, train loss = 0.24, mean loss = 0.32
Mon Jan 31 19:07:27 2022
train Loss: 0.32

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 19:08:18 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 19:09:12 2022
val Loss: 0.02

epoch 9 was done for 471.158492 seconds
Epoch 10/299
----------
Mon Jan 31 19:09:50 2022
batch 0, train loss = 0.22, mean loss = 0.22
Mon Jan 31 19:09:55 2022
batch 10, train loss = 0.32, mean loss = 0.31
Mon Jan 31 19:10:49 2022
batch 20, train loss = 0.28, mean loss = 0.31
Mon Jan 31 19:11:43 2022
batch 30, train loss = 0.31, mean loss = 0.32
Mon Jan 31 19:12:37 2022
batch 40, train loss = 0.23, mean loss = 0.31
Mon Jan 31 19:13:31 2022
batch 50, train loss = 0.29, mean loss = 0.32
Mon Jan 31 19:14:25 2022
batch 60, train loss = 0.23, mean loss = 0.31
Mon Jan 31 19:15:19 2022
train Loss: 0.32

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 19:16:10 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 19:17:04 2022
val Loss: 0.02

epoch 10 was done for 472.299391 seconds
Epoch 11/299
----------
Mon Jan 31 19:17:42 2022
batch 0, train loss = 0.22, mean loss = 0.22
Mon Jan 31 19:17:47 2022
batch 10, train loss = 0.32, mean loss = 0.31
Mon Jan 31 19:18:41 2022
batch 20, train loss = 0.27, mean loss = 0.31
Mon Jan 31 19:19:34 2022
batch 30, train loss = 0.31, mean loss = 0.32
Mon Jan 31 19:20:27 2022
batch 40, train loss = 0.22, mean loss = 0.31
Mon Jan 31 19:21:21 2022
batch 50, train loss = 0.29, mean loss = 0.31
Mon Jan 31 19:22:14 2022
batch 60, train loss = 0.22, mean loss = 0.31
Mon Jan 31 19:23:08 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 19:23:58 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 19:24:52 2022
val Loss: 0.03

epoch 11 was done for 467.026563 seconds
Epoch 12/299
----------
Mon Jan 31 19:25:29 2022
batch 0, train loss = 0.21, mean loss = 0.21
Mon Jan 31 19:25:35 2022
batch 10, train loss = 0.31, mean loss = 0.31
Mon Jan 31 19:26:28 2022
batch 20, train loss = 0.27, mean loss = 0.31
Mon Jan 31 19:27:23 2022
batch 30, train loss = 0.31, mean loss = 0.32
Mon Jan 31 19:28:17 2022
batch 40, train loss = 0.21, mean loss = 0.31
Mon Jan 31 19:29:11 2022
batch 50, train loss = 0.29, mean loss = 0.31
Mon Jan 31 19:30:05 2022
batch 60, train loss = 0.21, mean loss = 0.31
Mon Jan 31 19:30:59 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 19:31:50 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 19:32:45 2022
val Loss: 0.03

epoch 12 was done for 473.730467 seconds
Epoch 13/299
----------
Mon Jan 31 19:33:23 2022
batch 0, train loss = 0.21, mean loss = 0.21
Mon Jan 31 19:33:28 2022
batch 10, train loss = 0.31, mean loss = 0.31
Mon Jan 31 19:34:23 2022
batch 20, train loss = 0.26, mean loss = 0.31
Mon Jan 31 19:35:17 2022
batch 30, train loss = 0.31, mean loss = 0.31
Mon Jan 31 19:36:12 2022
batch 40, train loss = 0.20, mean loss = 0.31
Mon Jan 31 19:37:06 2022
batch 50, train loss = 0.29, mean loss = 0.31
Mon Jan 31 19:38:00 2022
batch 60, train loss = 0.21, mean loss = 0.31
Mon Jan 31 19:38:54 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 19:39:45 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 19:40:40 2022
val Loss: 0.03

epoch 13 was done for 474.432985 seconds
Epoch 14/299
----------
Mon Jan 31 19:41:17 2022
batch 0, train loss = 0.20, mean loss = 0.20
Mon Jan 31 19:41:23 2022
batch 10, train loss = 0.31, mean loss = 0.31
Mon Jan 31 19:42:16 2022
batch 20, train loss = 0.25, mean loss = 0.31
Mon Jan 31 19:43:10 2022
batch 30, train loss = 0.31, mean loss = 0.31
Mon Jan 31 19:44:04 2022
batch 40, train loss = 0.19, mean loss = 0.31
Mon Jan 31 19:44:58 2022
batch 50, train loss = 0.29, mean loss = 0.31
Mon Jan 31 19:45:52 2022
batch 60, train loss = 0.20, mean loss = 0.31
Mon Jan 31 19:46:46 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 19:47:37 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 19:48:32 2022
val Loss: 0.03

epoch 14 was done for 472.196223 seconds
Epoch 15/299
----------
Mon Jan 31 19:49:10 2022
batch 0, train loss = 0.20, mean loss = 0.20
Mon Jan 31 19:49:15 2022
batch 10, train loss = 0.31, mean loss = 0.31
Mon Jan 31 19:50:09 2022
batch 20, train loss = 0.24, mean loss = 0.31
Mon Jan 31 19:51:04 2022
batch 30, train loss = 0.32, mean loss = 0.31
Mon Jan 31 19:51:58 2022
batch 40, train loss = 0.19, mean loss = 0.31
Mon Jan 31 19:52:52 2022
batch 50, train loss = 0.30, mean loss = 0.31
Mon Jan 31 19:53:46 2022
batch 60, train loss = 0.19, mean loss = 0.31
Mon Jan 31 19:54:40 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 19:55:31 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 19:56:26 2022
val Loss: 0.03

epoch 15 was done for 473.767969 seconds
Epoch 16/299
----------
Mon Jan 31 19:57:03 2022
batch 0, train loss = 0.19, mean loss = 0.19
Mon Jan 31 19:57:09 2022
batch 10, train loss = 0.31, mean loss = 0.30
Mon Jan 31 19:58:03 2022
batch 20, train loss = 0.23, mean loss = 0.31
Mon Jan 31 19:58:58 2022
batch 30, train loss = 0.32, mean loss = 0.31
Mon Jan 31 19:59:53 2022
batch 40, train loss = 0.18, mean loss = 0.31
Mon Jan 31 20:00:47 2022
batch 50, train loss = 0.30, mean loss = 0.31
Mon Jan 31 20:01:42 2022
batch 60, train loss = 0.18, mean loss = 0.31
Mon Jan 31 20:02:36 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 20:03:27 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 20:04:23 2022
val Loss: 0.03

epoch 16 was done for 477.208375 seconds
Epoch 17/299
----------
Mon Jan 31 20:05:00 2022
batch 0, train loss = 0.19, mean loss = 0.19
Mon Jan 31 20:05:06 2022
batch 10, train loss = 0.31, mean loss = 0.30
Mon Jan 31 20:06:00 2022
batch 20, train loss = 0.23, mean loss = 0.31
Mon Jan 31 20:06:53 2022
batch 30, train loss = 0.32, mean loss = 0.31
Mon Jan 31 20:07:47 2022
batch 40, train loss = 0.17, mean loss = 0.31
Mon Jan 31 20:08:41 2022
batch 50, train loss = 0.30, mean loss = 0.31
Mon Jan 31 20:09:35 2022
batch 60, train loss = 0.18, mean loss = 0.31
Mon Jan 31 20:10:29 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 20:11:19 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 20:12:13 2022
val Loss: 0.03

epoch 17 was done for 470.515114 seconds
Epoch 18/299
----------
Mon Jan 31 20:12:51 2022
batch 0, train loss = 0.18, mean loss = 0.18
Mon Jan 31 20:12:56 2022
batch 10, train loss = 0.30, mean loss = 0.30
Mon Jan 31 20:13:50 2022
batch 20, train loss = 0.22, mean loss = 0.30
Mon Jan 31 20:14:44 2022
batch 30, train loss = 0.32, mean loss = 0.31
Mon Jan 31 20:15:38 2022
batch 40, train loss = 0.17, mean loss = 0.31
Mon Jan 31 20:16:32 2022
batch 50, train loss = 0.30, mean loss = 0.31
Mon Jan 31 20:17:26 2022
batch 60, train loss = 0.17, mean loss = 0.31
Mon Jan 31 20:18:20 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 20:19:10 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 20:20:05 2022
val Loss: 0.03

epoch 18 was done for 471.430222 seconds
Epoch 19/299
----------
Mon Jan 31 20:20:42 2022
batch 0, train loss = 0.18, mean loss = 0.18
Mon Jan 31 20:20:48 2022
batch 10, train loss = 0.30, mean loss = 0.30
Mon Jan 31 20:21:42 2022
batch 20, train loss = 0.21, mean loss = 0.30
Mon Jan 31 20:22:37 2022
batch 30, train loss = 0.33, mean loss = 0.31
Mon Jan 31 20:23:32 2022
batch 40, train loss = 0.16, mean loss = 0.31
Mon Jan 31 20:24:26 2022
batch 50, train loss = 0.30, mean loss = 0.31
Mon Jan 31 20:25:21 2022
batch 60, train loss = 0.16, mean loss = 0.31
Mon Jan 31 20:26:15 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 20:27:06 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 20:28:01 2022
val Loss: 0.03

epoch 19 was done for 476.753246 seconds
Epoch 20/299
----------
Mon Jan 31 20:28:39 2022
batch 0, train loss = 0.17, mean loss = 0.17
Mon Jan 31 20:28:45 2022
batch 10, train loss = 0.31, mean loss = 0.30
Mon Jan 31 20:29:38 2022
batch 20, train loss = 0.20, mean loss = 0.30
Mon Jan 31 20:30:31 2022
batch 30, train loss = 0.33, mean loss = 0.31
Mon Jan 31 20:31:25 2022
batch 40, train loss = 0.16, mean loss = 0.31
Mon Jan 31 20:32:18 2022
batch 50, train loss = 0.31, mean loss = 0.31
Mon Jan 31 20:33:12 2022
batch 60, train loss = 0.16, mean loss = 0.31
Mon Jan 31 20:34:05 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 20:34:55 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 20:35:50 2022
val Loss: 0.03

epoch 20 was done for 467.739125 seconds
Epoch 21/299
----------
Mon Jan 31 20:36:27 2022
batch 0, train loss = 0.17, mean loss = 0.17
Mon Jan 31 20:36:32 2022
batch 10, train loss = 0.31, mean loss = 0.30
Mon Jan 31 20:37:26 2022
batch 20, train loss = 0.19, mean loss = 0.30
Mon Jan 31 20:38:20 2022
batch 30, train loss = 0.34, mean loss = 0.31
Mon Jan 31 20:39:14 2022
batch 40, train loss = 0.15, mean loss = 0.31
Mon Jan 31 20:40:08 2022
batch 50, train loss = 0.31, mean loss = 0.31
Mon Jan 31 20:41:02 2022
batch 60, train loss = 0.15, mean loss = 0.31
Mon Jan 31 20:41:56 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 20:42:46 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 20:43:41 2022
val Loss: 0.03

epoch 21 was done for 471.452009 seconds
Epoch 22/299
----------
Mon Jan 31 20:44:18 2022
batch 0, train loss = 0.17, mean loss = 0.17
Mon Jan 31 20:44:24 2022
batch 10, train loss = 0.31, mean loss = 0.30
Mon Jan 31 20:45:17 2022
batch 20, train loss = 0.19, mean loss = 0.30
Mon Jan 31 20:46:10 2022
batch 30, train loss = 0.34, mean loss = 0.31
Mon Jan 31 20:47:03 2022
batch 40, train loss = 0.14, mean loss = 0.31
Mon Jan 31 20:47:56 2022
batch 50, train loss = 0.31, mean loss = 0.31
Mon Jan 31 20:48:48 2022
batch 60, train loss = 0.14, mean loss = 0.31
Mon Jan 31 20:49:42 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 20:50:31 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 20:51:25 2022
val Loss: 0.03

epoch 22 was done for 464.257242 seconds
Epoch 23/299
----------
Mon Jan 31 20:52:03 2022
batch 0, train loss = 0.16, mean loss = 0.16
Mon Jan 31 20:52:08 2022
batch 10, train loss = 0.31, mean loss = 0.30
Mon Jan 31 20:53:01 2022
batch 20, train loss = 0.18, mean loss = 0.31
Mon Jan 31 20:53:55 2022
batch 30, train loss = 0.35, mean loss = 0.31
Mon Jan 31 20:54:49 2022
batch 40, train loss = 0.13, mean loss = 0.31
Mon Jan 31 20:55:43 2022
batch 50, train loss = 0.31, mean loss = 0.31
Mon Jan 31 20:56:36 2022
batch 60, train loss = 0.14, mean loss = 0.31
Mon Jan 31 20:57:30 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 20:58:21 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 20:59:15 2022
val Loss: 0.03

epoch 23 was done for 470.287459 seconds
Epoch 24/299
----------
Mon Jan 31 20:59:53 2022
batch 0, train loss = 0.16, mean loss = 0.16
Mon Jan 31 20:59:58 2022
batch 10, train loss = 0.32, mean loss = 0.30
Mon Jan 31 21:01:05 2022
batch 20, train loss = 0.17, mean loss = 0.31
Mon Jan 31 21:02:00 2022
batch 30, train loss = 0.35, mean loss = 0.32
Mon Jan 31 21:02:56 2022
batch 40, train loss = 0.13, mean loss = 0.31
Mon Jan 31 21:03:50 2022
batch 50, train loss = 0.32, mean loss = 0.32
Mon Jan 31 21:04:45 2022
batch 60, train loss = 0.13, mean loss = 0.31
Mon Jan 31 21:05:39 2022
train Loss: 0.32

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 21:06:30 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 21:07:25 2022
val Loss: 0.03

epoch 24 was done for 490.536592 seconds
Epoch 25/299
----------
Mon Jan 31 21:08:03 2022
batch 0, train loss = 0.15, mean loss = 0.15
Mon Jan 31 21:08:09 2022
batch 10, train loss = 0.32, mean loss = 0.31
Mon Jan 31 21:09:02 2022
batch 20, train loss = 0.16, mean loss = 0.31
Mon Jan 31 21:09:56 2022
batch 30, train loss = 0.36, mean loss = 0.32
Mon Jan 31 21:10:50 2022
batch 40, train loss = 0.12, mean loss = 0.31
Mon Jan 31 21:11:44 2022
batch 50, train loss = 0.33, mean loss = 0.32
Mon Jan 31 21:12:37 2022
batch 60, train loss = 0.13, mean loss = 0.31
Mon Jan 31 21:13:31 2022
train Loss: 0.33

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 21:14:21 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 21:15:16 2022
val Loss: 0.03

epoch 25 was done for 469.807136 seconds
Epoch 26/299
----------
Mon Jan 31 21:15:53 2022
batch 0, train loss = 0.28, mean loss = 0.28
Mon Jan 31 21:15:59 2022
batch 10, train loss = 0.40, mean loss = 0.36
Mon Jan 31 21:16:53 2022
batch 20, train loss = 0.21, mean loss = 0.38
Mon Jan 31 21:17:47 2022
batch 30, train loss = 0.36, mean loss = 0.37
Mon Jan 31 21:18:40 2022
batch 40, train loss = 0.25, mean loss = 0.38
Mon Jan 31 21:19:34 2022
batch 50, train loss = 0.33, mean loss = 0.39
Mon Jan 31 21:20:28 2022
batch 60, train loss = 0.15, mean loss = 0.38
Mon Jan 31 21:21:22 2022
train Loss: 0.38

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 21:22:13 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 21:23:07 2022
val Loss: 0.02

epoch 26 was done for 471.885405 seconds
Epoch 27/299
----------
Mon Jan 31 21:23:45 2022
batch 0, train loss = 0.23, mean loss = 0.23
Mon Jan 31 21:23:51 2022
batch 10, train loss = 0.75, mean loss = 0.88
Mon Jan 31 21:24:45 2022
batch 20, train loss = 0.23, mean loss = 0.69
Mon Jan 31 21:25:39 2022
batch 30, train loss = 0.39, mean loss = 0.59
Mon Jan 31 21:26:33 2022
batch 40, train loss = 0.17, mean loss = 0.54
Mon Jan 31 21:27:28 2022
batch 50, train loss = 0.98, mean loss = 0.58
Mon Jan 31 21:28:22 2022
batch 60, train loss = 0.53, mean loss = 0.59
Mon Jan 31 21:29:17 2022
train Loss: 0.59

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 21:30:08 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 21:31:04 2022
val Loss: 0.02

epoch 27 was done for 476.360869 seconds
Epoch 28/299
----------
Mon Jan 31 21:31:42 2022
batch 0, train loss = 0.28, mean loss = 0.28
Mon Jan 31 21:31:47 2022
batch 10, train loss = 0.54, mean loss = 0.43
Mon Jan 31 21:32:41 2022
batch 20, train loss = 0.27, mean loss = 0.44
Mon Jan 31 21:33:35 2022
batch 30, train loss = 0.50, mean loss = 0.50
Mon Jan 31 21:34:29 2022
batch 40, train loss = 0.36, mean loss = 0.51
Mon Jan 31 21:35:23 2022
batch 50, train loss = 0.58, mean loss = 0.51
Mon Jan 31 21:36:18 2022
batch 60, train loss = 0.21, mean loss = 0.52
Mon Jan 31 21:37:12 2022
train Loss: 0.55

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 21:38:03 2022
batch 10, val loss = 0.03, mean loss = 0.02
Mon Jan 31 21:38:58 2022
val Loss: 0.02

epoch 28 was done for 474.394462 seconds
Epoch 29/299
----------
Mon Jan 31 21:39:36 2022
batch 0, train loss = 0.20, mean loss = 0.20
Mon Jan 31 21:39:41 2022
batch 10, train loss = 1.05, mean loss = 0.78
Mon Jan 31 21:40:36 2022
batch 20, train loss = 0.72, mean loss = 0.84
Mon Jan 31 21:41:30 2022
batch 30, train loss = 1.75, mean loss = 0.94
Mon Jan 31 21:42:25 2022
batch 40, train loss = 1.20, mean loss = 0.97
Mon Jan 31 21:43:19 2022
batch 50, train loss = 1.23, mean loss = 0.98
Mon Jan 31 21:44:14 2022
batch 60, train loss = 0.59, mean loss = 0.97
Mon Jan 31 21:45:09 2022
train Loss: 0.98

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 21:46:00 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 21:46:56 2022
val Loss: 0.03

epoch 29 was done for 478.322212 seconds
Epoch 30/299
----------
Mon Jan 31 21:47:34 2022
batch 0, train loss = 0.96, mean loss = 0.96
Mon Jan 31 21:47:40 2022
batch 10, train loss = 1.43, mean loss = 1.15
Mon Jan 31 21:48:35 2022
batch 20, train loss = 1.30, mean loss = 1.22
Mon Jan 31 21:49:30 2022
batch 30, train loss = 1.35, mean loss = 1.31
Mon Jan 31 21:50:27 2022
batch 40, train loss = 1.54, mean loss = 1.38
Mon Jan 31 21:51:23 2022
batch 50, train loss = 1.13, mean loss = 1.37
Mon Jan 31 21:52:18 2022
batch 60, train loss = 1.02, mean loss = 1.33
Mon Jan 31 21:53:14 2022
train Loss: 1.28

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 21:54:05 2022
batch 10, val loss = 0.03, mean loss = 0.02
Mon Jan 31 21:55:02 2022
val Loss: 0.02

epoch 30 was done for 485.674269 seconds
Epoch 31/299
----------
Mon Jan 31 21:55:40 2022
batch 0, train loss = 0.60, mean loss = 0.60
Mon Jan 31 21:55:45 2022
batch 10, train loss = 0.81, mean loss = 0.77
Mon Jan 31 21:56:39 2022
batch 20, train loss = 0.52, mean loss = 0.69
Mon Jan 31 21:57:33 2022
batch 30, train loss = 0.46, mean loss = 0.62
Mon Jan 31 21:58:27 2022
batch 40, train loss = 0.25, mean loss = 0.59
Mon Jan 31 21:59:23 2022
batch 50, train loss = 0.37, mean loss = 0.56
Mon Jan 31 22:00:17 2022
batch 60, train loss = 0.18, mean loss = 0.53
Mon Jan 31 22:01:12 2022
train Loss: 0.51

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 22:02:03 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 22:02:59 2022
val Loss: 0.02

epoch 31 was done for 477.323024 seconds
Epoch 32/299
----------
Mon Jan 31 22:03:37 2022
batch 0, train loss = 0.27, mean loss = 0.27
Mon Jan 31 22:03:43 2022
batch 10, train loss = 0.46, mean loss = 0.40
Mon Jan 31 22:04:38 2022
batch 20, train loss = 0.38, mean loss = 0.41
Mon Jan 31 22:05:35 2022
batch 30, train loss = 0.54, mean loss = 0.43
Mon Jan 31 22:06:31 2022
batch 40, train loss = 0.39, mean loss = 0.44
Mon Jan 31 22:07:26 2022
batch 50, train loss = 0.45, mean loss = 0.45
Mon Jan 31 22:08:22 2022
batch 60, train loss = 0.27, mean loss = 0.45
Mon Jan 31 22:09:17 2022
train Loss: 0.45

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 22:10:08 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 22:11:04 2022
val Loss: 0.03

epoch 32 was done for 484.637404 seconds
Epoch 33/299
----------
Mon Jan 31 22:11:42 2022
batch 0, train loss = 0.26, mean loss = 0.26
Mon Jan 31 22:11:47 2022
batch 10, train loss = 0.49, mean loss = 0.44
Mon Jan 31 22:12:42 2022
batch 20, train loss = 0.34, mean loss = 0.45
Mon Jan 31 22:13:37 2022
batch 30, train loss = 0.59, mean loss = 0.47
Mon Jan 31 22:14:31 2022
batch 40, train loss = 0.46, mean loss = 0.48
Mon Jan 31 22:15:26 2022
batch 50, train loss = 0.53, mean loss = 0.49
Mon Jan 31 22:16:20 2022
batch 60, train loss = 0.41, mean loss = 0.49
Mon Jan 31 22:17:15 2022
train Loss: 0.50

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 22:18:06 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 22:19:01 2022
val Loss: 0.02

epoch 33 was done for 477.487347 seconds
Epoch 34/299
----------
Mon Jan 31 22:19:39 2022
batch 0, train loss = 0.19, mean loss = 0.19
Mon Jan 31 22:19:45 2022
batch 10, train loss = 0.49, mean loss = 0.51
Mon Jan 31 22:20:41 2022
batch 20, train loss = 0.20, mean loss = 0.49
Mon Jan 31 22:21:35 2022
batch 30, train loss = 0.65, mean loss = 0.50
Mon Jan 31 22:22:28 2022
batch 40, train loss = 0.25, mean loss = 0.50
Mon Jan 31 22:23:22 2022
batch 50, train loss = 0.57, mean loss = 0.51
Mon Jan 31 22:24:16 2022
batch 60, train loss = 0.33, mean loss = 0.50
Mon Jan 31 22:25:09 2022
train Loss: 0.50

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 22:26:00 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 22:26:56 2022
val Loss: 0.02

epoch 34 was done for 474.503150 seconds
Epoch 35/299
----------
Mon Jan 31 22:27:34 2022
batch 0, train loss = 0.43, mean loss = 0.43
Mon Jan 31 22:27:39 2022
batch 10, train loss = 0.51, mean loss = 0.48
Mon Jan 31 22:28:33 2022
batch 20, train loss = 0.53, mean loss = 0.50
Mon Jan 31 22:29:27 2022
batch 30, train loss = 0.44, mean loss = 0.51
Mon Jan 31 22:30:21 2022
batch 40, train loss = 0.40, mean loss = 0.52
Mon Jan 31 22:31:15 2022
batch 50, train loss = 0.44, mean loss = 0.53
Mon Jan 31 22:32:09 2022
batch 60, train loss = 0.29, mean loss = 0.53
Mon Jan 31 22:33:03 2022
train Loss: 0.53

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 22:33:53 2022
batch 10, val loss = 0.03, mean loss = 0.02
Mon Jan 31 22:34:48 2022
val Loss: 0.02

epoch 35 was done for 472.025995 seconds
Epoch 36/299
----------
Mon Jan 31 22:35:26 2022
batch 0, train loss = 0.30, mean loss = 0.30
Mon Jan 31 22:35:31 2022
batch 10, train loss = 0.79, mean loss = 0.53
Mon Jan 31 22:36:25 2022
batch 20, train loss = 0.37, mean loss = 0.53
Mon Jan 31 22:37:18 2022
batch 30, train loss = 0.59, mean loss = 0.52
Mon Jan 31 22:38:14 2022
batch 40, train loss = 0.39, mean loss = 0.51
Mon Jan 31 22:39:09 2022
batch 50, train loss = 0.52, mean loss = 0.51
Mon Jan 31 22:40:04 2022
batch 60, train loss = 0.46, mean loss = 0.51
Mon Jan 31 22:41:00 2022
train Loss: 0.52

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 22:41:52 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 22:42:48 2022
val Loss: 0.03

epoch 36 was done for 481.594244 seconds
Epoch 37/299
----------
Mon Jan 31 22:43:27 2022
batch 0, train loss = 0.24, mean loss = 0.24
Mon Jan 31 22:43:33 2022
batch 10, train loss = 0.48, mean loss = 0.54
Mon Jan 31 22:44:28 2022
batch 20, train loss = 0.24, mean loss = 0.53
Mon Jan 31 22:45:24 2022
batch 30, train loss = 0.46, mean loss = 0.54
Mon Jan 31 22:46:19 2022
batch 40, train loss = 0.24, mean loss = 0.53
Mon Jan 31 22:47:13 2022
batch 50, train loss = 0.42, mean loss = 0.53
Mon Jan 31 22:48:07 2022
batch 60, train loss = 0.16, mean loss = 0.51
Mon Jan 31 22:49:00 2022
train Loss: 0.51

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 22:49:51 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 22:50:47 2022
val Loss: 0.02

epoch 37 was done for 477.278538 seconds
Epoch 38/299
----------
Mon Jan 31 22:51:25 2022
batch 0, train loss = 0.24, mean loss = 0.24
Mon Jan 31 22:51:30 2022
batch 10, train loss = 0.52, mean loss = 0.45
Mon Jan 31 22:52:24 2022
batch 20, train loss = 0.39, mean loss = 0.46
Mon Jan 31 22:53:18 2022
batch 30, train loss = 0.72, mean loss = 0.48
Mon Jan 31 22:54:12 2022
batch 40, train loss = 0.52, mean loss = 0.50
Mon Jan 31 22:55:06 2022
batch 50, train loss = 0.61, mean loss = 0.51
Mon Jan 31 22:56:00 2022
batch 60, train loss = 0.34, mean loss = 0.50
Mon Jan 31 22:56:56 2022
train Loss: 0.50

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 22:57:47 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 22:58:41 2022
val Loss: 0.02

epoch 38 was done for 474.315999 seconds
Epoch 39/299
----------
Mon Jan 31 22:59:19 2022
batch 0, train loss = 0.31, mean loss = 0.31
Mon Jan 31 22:59:24 2022
batch 10, train loss = 0.48, mean loss = 0.49
Mon Jan 31 23:00:17 2022
batch 20, train loss = 0.36, mean loss = 0.47
Mon Jan 31 23:01:11 2022
batch 30, train loss = 0.50, mean loss = 0.47
Mon Jan 31 23:02:04 2022
batch 40, train loss = 0.19, mean loss = 0.46
Mon Jan 31 23:02:57 2022
batch 50, train loss = 0.38, mean loss = 0.46
Mon Jan 31 23:03:50 2022
batch 60, train loss = 0.21, mean loss = 0.46
Mon Jan 31 23:04:43 2022
train Loss: 0.47

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 23:05:33 2022
batch 10, val loss = 0.03, mean loss = 0.02
Mon Jan 31 23:06:27 2022
val Loss: 0.02

epoch 39 was done for 464.677931 seconds
Epoch 40/299
----------
Mon Jan 31 23:07:04 2022
batch 0, train loss = 0.22, mean loss = 0.22
Mon Jan 31 23:07:09 2022
batch 10, train loss = 0.67, mean loss = 0.54
Mon Jan 31 23:08:05 2022
batch 20, train loss = 0.24, mean loss = 0.51
Mon Jan 31 23:08:58 2022
batch 30, train loss = 0.52, mean loss = 0.51
Mon Jan 31 23:09:52 2022
batch 40, train loss = 0.22, mean loss = 0.51
Mon Jan 31 23:10:45 2022
batch 50, train loss = 0.47, mean loss = 0.51
Mon Jan 31 23:11:38 2022
batch 60, train loss = 0.32, mean loss = 0.50
Mon Jan 31 23:12:32 2022
train Loss: 0.50

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 23:13:22 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 23:14:16 2022
val Loss: 0.02

epoch 40 was done for 470.016907 seconds
Epoch 41/299
----------
Mon Jan 31 23:14:54 2022
batch 0, train loss = 0.30, mean loss = 0.30
Mon Jan 31 23:14:59 2022
batch 10, train loss = 0.57, mean loss = 0.51
Mon Jan 31 23:15:53 2022
batch 20, train loss = 0.28, mean loss = 0.50
Mon Jan 31 23:16:49 2022
batch 30, train loss = 0.57, mean loss = 0.54
Mon Jan 31 23:17:43 2022
batch 40, train loss = 0.38, mean loss = 0.55
Mon Jan 31 23:18:38 2022
batch 50, train loss = 0.72, mean loss = 0.56
Mon Jan 31 23:19:31 2022
batch 60, train loss = 0.37, mean loss = 0.56
Mon Jan 31 23:20:26 2022
train Loss: 0.56

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 23:21:16 2022
batch 10, val loss = 0.03, mean loss = 0.02
Mon Jan 31 23:22:11 2022
val Loss: 0.02

epoch 41 was done for 474.920989 seconds
Epoch 42/299
----------
Mon Jan 31 23:22:49 2022
batch 0, train loss = 0.52, mean loss = 0.52
Mon Jan 31 23:22:54 2022
batch 10, train loss = 0.53, mean loss = 0.60
Mon Jan 31 23:23:47 2022
batch 20, train loss = 0.50, mean loss = 0.62
Mon Jan 31 23:24:42 2022
batch 30, train loss = 0.62, mean loss = 0.63
Mon Jan 31 23:25:35 2022
batch 40, train loss = 0.53, mean loss = 0.64
Mon Jan 31 23:26:31 2022
batch 50, train loss = 0.59, mean loss = 0.64
Mon Jan 31 23:27:25 2022
batch 60, train loss = 0.27, mean loss = 0.64
Mon Jan 31 23:28:22 2022
train Loss: 0.65

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 23:29:13 2022
batch 10, val loss = 0.03, mean loss = 0.02
Mon Jan 31 23:30:08 2022
val Loss: 0.02

epoch 42 was done for 477.029847 seconds
Epoch 43/299
----------
Mon Jan 31 23:30:46 2022
batch 0, train loss = 0.21, mean loss = 0.21
Mon Jan 31 23:30:51 2022
batch 10, train loss = 0.75, mean loss = 0.74
Mon Jan 31 23:31:45 2022
batch 20, train loss = 0.39, mean loss = 0.73
Mon Jan 31 23:32:40 2022
batch 30, train loss = 1.10, mean loss = 0.73
Mon Jan 31 23:33:33 2022
batch 40, train loss = 0.38, mean loss = 0.71
Mon Jan 31 23:34:28 2022
batch 50, train loss = 0.75, mean loss = 0.71
Mon Jan 31 23:35:23 2022
batch 60, train loss = 0.30, mean loss = 0.68
Mon Jan 31 23:36:17 2022
train Loss: 0.68

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 23:37:10 2022
batch 10, val loss = 0.03, mean loss = 0.02
Mon Jan 31 23:38:04 2022
val Loss: 0.02

epoch 43 was done for 475.890694 seconds
Epoch 44/299
----------
Mon Jan 31 23:38:42 2022
batch 0, train loss = 0.33, mean loss = 0.33
Mon Jan 31 23:38:47 2022
batch 10, train loss = 0.49, mean loss = 0.57
Mon Jan 31 23:39:41 2022
batch 20, train loss = 0.49, mean loss = 0.55
Mon Jan 31 23:40:35 2022
batch 30, train loss = 0.78, mean loss = 0.54
Mon Jan 31 23:41:29 2022
batch 40, train loss = 0.26, mean loss = 0.54
Mon Jan 31 23:42:23 2022
batch 50, train loss = 0.44, mean loss = 0.56
Mon Jan 31 23:43:17 2022
batch 60, train loss = 0.43, mean loss = 0.56
Mon Jan 31 23:44:12 2022
train Loss: 0.57

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 23:45:02 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 23:45:57 2022
val Loss: 0.03

epoch 44 was done for 473.213717 seconds
Epoch 45/299
----------
Mon Jan 31 23:46:35 2022
batch 0, train loss = 0.53, mean loss = 0.53
Mon Jan 31 23:46:40 2022
batch 10, train loss = 0.78, mean loss = 0.62
Mon Jan 31 23:47:36 2022
batch 20, train loss = 0.40, mean loss = 0.61
Mon Jan 31 23:48:30 2022
batch 30, train loss = 0.71, mean loss = 0.62
Mon Jan 31 23:49:23 2022
batch 40, train loss = 0.37, mean loss = 0.63
Mon Jan 31 23:50:17 2022
batch 50, train loss = 0.58, mean loss = 0.65
Mon Jan 31 23:51:10 2022
batch 60, train loss = 0.58, mean loss = 0.65
Mon Jan 31 23:52:03 2022
train Loss: 0.65

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 23:52:53 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 23:53:48 2022
val Loss: 0.03

epoch 45 was done for 470.216982 seconds
Epoch 46/299
----------
Mon Jan 31 23:54:25 2022
batch 0, train loss = 0.63, mean loss = 0.63
Mon Jan 31 23:54:30 2022
batch 10, train loss = 0.83, mean loss = 0.66
Mon Jan 31 23:55:24 2022
batch 20, train loss = 0.40, mean loss = 0.68
Mon Jan 31 23:56:17 2022
batch 30, train loss = 0.73, mean loss = 0.70
Mon Jan 31 23:57:11 2022
batch 40, train loss = 0.58, mean loss = 0.69
Mon Jan 31 23:58:07 2022
batch 50, train loss = 0.64, mean loss = 0.69
Mon Jan 31 23:59:01 2022
batch 60, train loss = 0.44, mean loss = 0.69
Mon Jan 31 23:59:55 2022
train Loss: 0.69

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 00:00:46 2022
batch 10, val loss = 0.02, mean loss = 0.03
Tue Feb  1 00:01:41 2022
val Loss: 0.02

epoch 46 was done for 474.846366 seconds
Epoch 47/299
----------
Tue Feb  1 00:02:20 2022
batch 0, train loss = 0.52, mean loss = 0.52
Tue Feb  1 00:02:26 2022
batch 10, train loss = 0.90, mean loss = 0.64
Tue Feb  1 00:03:21 2022
batch 20, train loss = 0.48, mean loss = 0.63
Tue Feb  1 00:04:16 2022
batch 30, train loss = 0.63, mean loss = 0.66
Tue Feb  1 00:05:12 2022
batch 40, train loss = 0.48, mean loss = 0.68
Tue Feb  1 00:06:07 2022
batch 50, train loss = 0.76, mean loss = 0.68
Tue Feb  1 00:07:04 2022
batch 60, train loss = 0.56, mean loss = 0.67
Tue Feb  1 00:08:00 2022
train Loss: 0.67

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 00:08:51 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 00:09:46 2022
val Loss: 0.02

epoch 47 was done for 484.201891 seconds
Epoch 48/299
----------
Tue Feb  1 00:10:24 2022
batch 0, train loss = 0.50, mean loss = 0.50
Tue Feb  1 00:10:29 2022
batch 10, train loss = 0.64, mean loss = 0.63
Tue Feb  1 00:11:23 2022
batch 20, train loss = 0.45, mean loss = 0.68
Tue Feb  1 00:12:16 2022
batch 30, train loss = 0.74, mean loss = 0.70
Tue Feb  1 00:13:10 2022
batch 40, train loss = 0.63, mean loss = 0.69
Tue Feb  1 00:14:03 2022
batch 50, train loss = 0.72, mean loss = 0.68
Tue Feb  1 00:14:56 2022
batch 60, train loss = 0.29, mean loss = 0.67
Tue Feb  1 00:15:50 2022
train Loss: 0.68

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 00:16:40 2022
batch 10, val loss = 0.03, mean loss = 0.03
Tue Feb  1 00:17:38 2022
val Loss: 0.03

epoch 48 was done for 472.030207 seconds
Epoch 49/299
----------
Tue Feb  1 00:18:16 2022
batch 0, train loss = 0.33, mean loss = 0.33
Tue Feb  1 00:18:21 2022
batch 10, train loss = 0.56, mean loss = 0.63
Tue Feb  1 00:19:15 2022
batch 20, train loss = 0.70, mean loss = 0.64
Tue Feb  1 00:20:09 2022
batch 30, train loss = 0.84, mean loss = 0.63
Tue Feb  1 00:21:02 2022
batch 40, train loss = 0.34, mean loss = 0.65
Tue Feb  1 00:21:56 2022
batch 50, train loss = 0.65, mean loss = 0.66
Tue Feb  1 00:22:49 2022
batch 60, train loss = 0.53, mean loss = 0.64
Tue Feb  1 00:23:42 2022
train Loss: 0.63

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 00:24:32 2022
batch 10, val loss = 0.02, mean loss = 0.03
Tue Feb  1 00:25:26 2022
val Loss: 0.03

epoch 49 was done for 467.440015 seconds
Epoch 50/299
----------
Tue Feb  1 00:26:04 2022
batch 0, train loss = 0.37, mean loss = 0.37
Tue Feb  1 00:26:09 2022
batch 10, train loss = 0.39, mean loss = 0.51
Tue Feb  1 00:27:02 2022
batch 20, train loss = 0.31, mean loss = 0.51
Tue Feb  1 00:27:57 2022
batch 30, train loss = 0.52, mean loss = 0.49
Tue Feb  1 00:28:50 2022
batch 40, train loss = 0.32, mean loss = 0.48
Tue Feb  1 00:29:43 2022
batch 50, train loss = 0.42, mean loss = 0.47
Tue Feb  1 00:30:36 2022
batch 60, train loss = 0.19, mean loss = 0.46
Tue Feb  1 00:31:29 2022
train Loss: 0.47

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 00:32:18 2022
batch 10, val loss = 0.03, mean loss = 0.03
Tue Feb  1 00:33:13 2022
val Loss: 0.03

epoch 50 was done for 467.496044 seconds
Epoch 51/299
----------
Tue Feb  1 00:33:51 2022
batch 0, train loss = 0.21, mean loss = 0.21
Tue Feb  1 00:33:56 2022
batch 10, train loss = 0.43, mean loss = 0.42
Tue Feb  1 00:34:49 2022
batch 20, train loss = 0.36, mean loss = 0.41
Tue Feb  1 00:35:43 2022
batch 30, train loss = 0.52, mean loss = 0.41
Tue Feb  1 00:36:36 2022
batch 40, train loss = 0.13, mean loss = 0.41
Tue Feb  1 00:37:29 2022
batch 50, train loss = 0.34, mean loss = 0.42
Tue Feb  1 00:38:25 2022
batch 60, train loss = 0.29, mean loss = 0.41
Tue Feb  1 00:39:18 2022
train Loss: 0.41

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 00:40:08 2022
batch 10, val loss = 0.03, mean loss = 0.03
Tue Feb  1 00:41:02 2022
val Loss: 0.03

epoch 51 was done for 468.160850 seconds
Epoch 52/299
----------
Tue Feb  1 00:41:39 2022
batch 0, train loss = 0.30, mean loss = 0.30
Tue Feb  1 00:41:45 2022
batch 10, train loss = 0.47, mean loss = 0.37
Tue Feb  1 00:42:38 2022
batch 20, train loss = 0.16, mean loss = 0.37
Tue Feb  1 00:43:32 2022
batch 30, train loss = 0.34, mean loss = 0.37
Tue Feb  1 00:44:26 2022
batch 40, train loss = 0.19, mean loss = 0.37
Tue Feb  1 00:45:19 2022
batch 50, train loss = 0.38, mean loss = 0.37
Tue Feb  1 00:46:13 2022
batch 60, train loss = 0.15, mean loss = 0.36
Tue Feb  1 00:47:10 2022
train Loss: 0.36

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 00:48:00 2022
batch 10, val loss = 0.03, mean loss = 0.03
Tue Feb  1 00:48:55 2022
val Loss: 0.03

epoch 52 was done for 472.833453 seconds
Epoch 53/299
----------
Tue Feb  1 00:49:32 2022
batch 0, train loss = 0.15, mean loss = 0.15
Tue Feb  1 00:49:37 2022
batch 10, train loss = 0.37, mean loss = 0.33
Tue Feb  1 00:50:30 2022
batch 20, train loss = 0.16, mean loss = 0.34
Tue Feb  1 00:51:24 2022
batch 30, train loss = 0.39, mean loss = 0.34
Tue Feb  1 00:52:17 2022
batch 40, train loss = 0.12, mean loss = 0.34
Tue Feb  1 00:53:10 2022
batch 50, train loss = 0.33, mean loss = 0.34
Tue Feb  1 00:54:03 2022
batch 60, train loss = 0.12, mean loss = 0.34
Tue Feb  1 00:54:56 2022
train Loss: 0.34

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 00:55:46 2022
batch 10, val loss = 0.03, mean loss = 0.02
Tue Feb  1 00:56:40 2022
val Loss: 0.02

epoch 53 was done for 465.406824 seconds
Epoch 54/299
----------
Tue Feb  1 00:57:17 2022
batch 0, train loss = 0.13, mean loss = 0.13
Tue Feb  1 00:57:23 2022
batch 10, train loss = 0.37, mean loss = 0.33
Tue Feb  1 00:58:19 2022
batch 20, train loss = 0.15, mean loss = 0.32
Tue Feb  1 00:59:13 2022
batch 30, train loss = 0.39, mean loss = 0.33
Tue Feb  1 01:00:07 2022
batch 40, train loss = 0.10, mean loss = 0.33
Tue Feb  1 01:01:01 2022
batch 50, train loss = 0.33, mean loss = 0.34
Tue Feb  1 01:01:55 2022
batch 60, train loss = 0.13, mean loss = 0.34
Tue Feb  1 01:02:49 2022
train Loss: 0.34

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 01:03:39 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 01:04:34 2022
val Loss: 0.02

epoch 54 was done for 474.701313 seconds
Epoch 55/299
----------
Tue Feb  1 01:05:12 2022
batch 0, train loss = 0.14, mean loss = 0.14
Tue Feb  1 01:05:18 2022
batch 10, train loss = 0.41, mean loss = 0.33
Tue Feb  1 01:06:12 2022
batch 20, train loss = 0.14, mean loss = 0.33
Tue Feb  1 01:07:06 2022
batch 30, train loss = 0.39, mean loss = 0.34
Tue Feb  1 01:08:01 2022
batch 40, train loss = 0.10, mean loss = 0.34
Tue Feb  1 01:08:58 2022
batch 50, train loss = 0.36, mean loss = 0.35
Tue Feb  1 01:09:53 2022
batch 60, train loss = 0.14, mean loss = 0.34
Tue Feb  1 01:10:47 2022
train Loss: 0.35

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 01:11:39 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 01:12:33 2022
val Loss: 0.02

epoch 55 was done for 479.402442 seconds
Epoch 56/299
----------
Tue Feb  1 01:13:12 2022
batch 0, train loss = 0.15, mean loss = 0.15
Tue Feb  1 01:13:17 2022
batch 10, train loss = 0.45, mean loss = 0.35
Tue Feb  1 01:14:11 2022
batch 20, train loss = 0.13, mean loss = 0.34
Tue Feb  1 01:15:05 2022
batch 30, train loss = 0.39, mean loss = 0.35
Tue Feb  1 01:16:00 2022
batch 40, train loss = 0.12, mean loss = 0.35
Tue Feb  1 01:16:54 2022
batch 50, train loss = 0.40, mean loss = 0.36
Tue Feb  1 01:17:51 2022
batch 60, train loss = 0.14, mean loss = 0.36
Tue Feb  1 01:18:45 2022
train Loss: 0.36

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 01:19:35 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 01:20:29 2022
val Loss: 0.02

epoch 56 was done for 475.552708 seconds
Epoch 57/299
----------
Tue Feb  1 01:21:07 2022
batch 0, train loss = 0.16, mean loss = 0.16
Tue Feb  1 01:21:12 2022
batch 10, train loss = 0.49, mean loss = 0.38
Tue Feb  1 01:22:06 2022
batch 20, train loss = 0.14, mean loss = 0.37
Tue Feb  1 01:23:00 2022
batch 30, train loss = 0.43, mean loss = 0.38
Tue Feb  1 01:23:53 2022
batch 40, train loss = 0.14, mean loss = 0.38
Tue Feb  1 01:24:47 2022
batch 50, train loss = 0.44, mean loss = 0.39
Tue Feb  1 01:25:41 2022
batch 60, train loss = 0.16, mean loss = 0.39
Tue Feb  1 01:26:35 2022
train Loss: 0.39

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 01:27:26 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 01:28:24 2022
val Loss: 0.02

epoch 57 was done for 474.422467 seconds
Epoch 58/299
----------
Tue Feb  1 01:29:02 2022
batch 0, train loss = 0.19, mean loss = 0.19
Tue Feb  1 01:29:07 2022
batch 10, train loss = 0.55, mean loss = 0.42
Tue Feb  1 01:30:02 2022
batch 20, train loss = 0.17, mean loss = 0.42
Tue Feb  1 01:30:58 2022
batch 30, train loss = 0.50, mean loss = 0.43
Tue Feb  1 01:31:54 2022
batch 40, train loss = 0.18, mean loss = 0.43
Tue Feb  1 01:32:49 2022
batch 50, train loss = 0.50, mean loss = 0.44
Tue Feb  1 01:33:44 2022
batch 60, train loss = 0.20, mean loss = 0.43
Tue Feb  1 01:34:39 2022
train Loss: 0.44

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 01:35:31 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 01:36:27 2022
val Loss: 0.02

epoch 58 was done for 486.681282 seconds
Epoch 59/299
----------
Tue Feb  1 01:37:08 2022
batch 0, train loss = 0.24, mean loss = 0.24
Tue Feb  1 01:37:14 2022
batch 10, train loss = 0.62, mean loss = 0.49
Tue Feb  1 01:38:08 2022
batch 20, train loss = 0.24, mean loss = 0.50
Tue Feb  1 01:39:02 2022
batch 30, train loss = 0.65, mean loss = 0.52
Tue Feb  1 01:39:56 2022
batch 40, train loss = 0.29, mean loss = 0.51
Tue Feb  1 01:40:51 2022
batch 50, train loss = 0.63, mean loss = 0.52
Tue Feb  1 01:41:45 2022
batch 60, train loss = 0.21, mean loss = 0.51
Tue Feb  1 01:42:39 2022
train Loss: 0.52

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 01:43:30 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 01:44:25 2022
val Loss: 0.02

epoch 59 was done for 474.909349 seconds
Epoch 60/299
----------
Tue Feb  1 01:45:03 2022
batch 0, train loss = 0.26, mean loss = 0.26
Tue Feb  1 01:45:09 2022
batch 10, train loss = 0.60, mean loss = 0.57
Tue Feb  1 01:46:02 2022
batch 20, train loss = 0.35, mean loss = 0.60
Tue Feb  1 01:46:57 2022
batch 30, train loss = 0.82, mean loss = 0.61
Tue Feb  1 01:47:51 2022
batch 40, train loss = 0.34, mean loss = 0.59
Tue Feb  1 01:48:45 2022
batch 50, train loss = 0.68, mean loss = 0.59
Tue Feb  1 01:49:38 2022
batch 60, train loss = 0.21, mean loss = 0.58
Tue Feb  1 01:50:32 2022
train Loss: 0.60

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 01:51:23 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 01:52:18 2022
val Loss: 0.02

epoch 60 was done for 471.954852 seconds
Epoch 61/299
----------
Tue Feb  1 01:52:55 2022
batch 0, train loss = 0.22, mean loss = 0.22
Tue Feb  1 01:53:01 2022
batch 10, train loss = 0.52, mean loss = 0.62
Tue Feb  1 01:53:55 2022
batch 20, train loss = 0.63, mean loss = 0.66
Tue Feb  1 01:54:50 2022
batch 30, train loss = 1.27, mean loss = 0.67
Tue Feb  1 01:55:44 2022
batch 40, train loss = 0.22, mean loss = 0.67
Tue Feb  1 01:56:39 2022
batch 50, train loss = 0.45, mean loss = 0.70
Tue Feb  1 01:57:33 2022
batch 60, train loss = 0.64, mean loss = 0.70
Tue Feb  1 01:58:28 2022
train Loss: 0.69

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 01:59:19 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 02:00:14 2022
val Loss: 0.02

epoch 61 was done for 477.674561 seconds
Epoch 62/299
----------
Tue Feb  1 02:00:53 2022
batch 0, train loss = 0.57, mean loss = 0.57
Tue Feb  1 02:00:58 2022
batch 10, train loss = 0.74, mean loss = 0.59
Tue Feb  1 02:01:52 2022
batch 20, train loss = 0.44, mean loss = 0.60
Tue Feb  1 02:02:46 2022
batch 30, train loss = 0.67, mean loss = 0.64
Tue Feb  1 02:03:39 2022
batch 40, train loss = 0.69, mean loss = 0.69
Tue Feb  1 02:04:33 2022
batch 50, train loss = 0.90, mean loss = 0.69
Tue Feb  1 02:05:26 2022
batch 60, train loss = 0.33, mean loss = 0.67
Tue Feb  1 02:06:21 2022
train Loss: 0.67

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 02:07:11 2022
batch 10, val loss = 0.03, mean loss = 0.03
Tue Feb  1 02:08:06 2022
val Loss: 0.03

epoch 62 was done for 470.811572 seconds
Epoch 63/299
----------
Tue Feb  1 02:08:44 2022
batch 0, train loss = 0.21, mean loss = 0.21
Tue Feb  1 02:08:49 2022
batch 10, train loss = 0.40, mean loss = 0.54
Tue Feb  1 02:09:43 2022
batch 20, train loss = 0.44, mean loss = 0.52
Tue Feb  1 02:10:37 2022
batch 30, train loss = 0.56, mean loss = 0.49
Tue Feb  1 02:11:31 2022
batch 40, train loss = 0.15, mean loss = 0.48
Tue Feb  1 02:12:25 2022
batch 50, train loss = 0.37, mean loss = 0.48
Tue Feb  1 02:13:19 2022
batch 60, train loss = 0.26, mean loss = 0.46
Tue Feb  1 02:14:13 2022
train Loss: 0.45

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 02:15:04 2022
batch 10, val loss = 0.03, mean loss = 0.02
Tue Feb  1 02:15:59 2022
val Loss: 0.02

epoch 63 was done for 472.855843 seconds
Epoch 64/299
----------
Tue Feb  1 02:16:36 2022
batch 0, train loss = 0.15, mean loss = 0.15
Tue Feb  1 02:16:42 2022
batch 10, train loss = 0.34, mean loss = 0.35
Tue Feb  1 02:17:37 2022
batch 20, train loss = 0.15, mean loss = 0.33
Tue Feb  1 02:18:34 2022
batch 30, train loss = 0.39, mean loss = 0.34
Tue Feb  1 02:19:29 2022
batch 40, train loss = 0.17, mean loss = 0.34
Tue Feb  1 02:20:24 2022
batch 50, train loss = 0.37, mean loss = 0.35
Tue Feb  1 02:21:17 2022
batch 60, train loss = 0.11, mean loss = 0.35
Tue Feb  1 02:22:11 2022
train Loss: 0.35

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 02:23:01 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 02:23:55 2022
val Loss: 0.02

epoch 64 was done for 476.680059 seconds
Epoch 65/299
----------
Tue Feb  1 02:24:33 2022
batch 0, train loss = 0.10, mean loss = 0.10
Tue Feb  1 02:24:38 2022
batch 10, train loss = 0.38, mean loss = 0.33
Tue Feb  1 02:25:32 2022
batch 20, train loss = 0.14, mean loss = 0.33
Tue Feb  1 02:26:26 2022
batch 30, train loss = 0.42, mean loss = 0.34
Tue Feb  1 02:27:21 2022
batch 40, train loss = 0.12, mean loss = 0.34
Tue Feb  1 02:28:14 2022
batch 50, train loss = 0.35, mean loss = 0.35
Tue Feb  1 02:29:08 2022
batch 60, train loss = 0.12, mean loss = 0.35
Tue Feb  1 02:30:01 2022
train Loss: 0.35

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 02:30:51 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 02:31:45 2022
val Loss: 0.02

epoch 65 was done for 469.821701 seconds
Epoch 66/299
----------
Tue Feb  1 02:32:23 2022
batch 0, train loss = 0.11, mean loss = 0.11
Tue Feb  1 02:32:28 2022
batch 10, train loss = 0.39, mean loss = 0.33
Tue Feb  1 02:33:22 2022
batch 20, train loss = 0.14, mean loss = 0.33
Tue Feb  1 02:34:17 2022
batch 30, train loss = 0.46, mean loss = 0.35
Tue Feb  1 02:35:11 2022
batch 40, train loss = 0.11, mean loss = 0.35
Tue Feb  1 02:36:05 2022
batch 50, train loss = 0.36, mean loss = 0.36
Tue Feb  1 02:36:58 2022
batch 60, train loss = 0.12, mean loss = 0.36
Tue Feb  1 02:37:52 2022
train Loss: 0.36

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 02:38:42 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 02:39:36 2022
val Loss: 0.02

epoch 66 was done for 471.093003 seconds
Epoch 67/299
----------
Tue Feb  1 02:40:14 2022
batch 0, train loss = 0.11, mean loss = 0.11
Tue Feb  1 02:40:19 2022
batch 10, train loss = 0.41, mean loss = 0.33
Tue Feb  1 02:41:13 2022
batch 20, train loss = 0.11, mean loss = 0.33
Tue Feb  1 02:42:07 2022
batch 30, train loss = 0.44, mean loss = 0.34
Tue Feb  1 02:43:00 2022
batch 40, train loss = 0.10, mean loss = 0.35
Tue Feb  1 02:43:53 2022
batch 50, train loss = 0.38, mean loss = 0.36
Tue Feb  1 02:44:47 2022
batch 60, train loss = 0.13, mean loss = 0.36
Tue Feb  1 02:45:41 2022
train Loss: 0.36

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 02:46:30 2022
batch 10, val loss = 0.02, mean loss = 0.03
Tue Feb  1 02:47:24 2022
val Loss: 0.03

epoch 67 was done for 467.025379 seconds
Epoch 68/299
----------
Tue Feb  1 02:48:01 2022
batch 0, train loss = 0.12, mean loss = 0.12
Tue Feb  1 02:48:06 2022
batch 10, train loss = 0.44, mean loss = 0.34
Tue Feb  1 02:49:00 2022
batch 20, train loss = 0.09, mean loss = 0.34
Tue Feb  1 02:49:54 2022
batch 30, train loss = 0.45, mean loss = 0.35
Tue Feb  1 02:50:48 2022
batch 40, train loss = 0.10, mean loss = 0.36
Tue Feb  1 02:51:41 2022
batch 50, train loss = 0.40, mean loss = 0.37
Tue Feb  1 02:52:35 2022
batch 60, train loss = 0.12, mean loss = 0.36
Tue Feb  1 02:53:28 2022
train Loss: 0.37

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 02:54:18 2022
batch 10, val loss = 0.03, mean loss = 0.03
Tue Feb  1 02:55:12 2022
val Loss: 0.03

epoch 68 was done for 469.059654 seconds
Epoch 69/299
----------
Tue Feb  1 02:55:50 2022
batch 0, train loss = 0.11, mean loss = 0.11
Tue Feb  1 02:55:56 2022
batch 10, train loss = 0.45, mean loss = 0.35
Tue Feb  1 02:56:49 2022
batch 20, train loss = 0.08, mean loss = 0.35
Tue Feb  1 02:57:44 2022
batch 30, train loss = 0.47, mean loss = 0.37
Tue Feb  1 02:58:37 2022
batch 40, train loss = 0.11, mean loss = 0.37
Tue Feb  1 02:59:32 2022
batch 50, train loss = 0.43, mean loss = 0.38
Tue Feb  1 03:00:26 2022
batch 60, train loss = 0.11, mean loss = 0.38
Tue Feb  1 03:01:20 2022
train Loss: 0.38

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 03:02:11 2022
batch 10, val loss = 0.03, mean loss = 0.03
Tue Feb  1 03:03:06 2022
val Loss: 0.03

epoch 69 was done for 474.252014 seconds
Epoch 70/299
----------
Tue Feb  1 03:03:44 2022
batch 0, train loss = 0.11, mean loss = 0.11
Tue Feb  1 03:03:50 2022
batch 10, train loss = 0.46, mean loss = 0.37
Tue Feb  1 03:04:43 2022
batch 20, train loss = 0.10, mean loss = 0.37
Tue Feb  1 03:05:36 2022
batch 30, train loss = 0.51, mean loss = 0.39
Tue Feb  1 03:06:30 2022
batch 40, train loss = 0.12, mean loss = 0.39
Tue Feb  1 03:07:23 2022
batch 50, train loss = 0.46, mean loss = 0.40
Tue Feb  1 03:08:15 2022
batch 60, train loss = 0.09, mean loss = 0.39
Tue Feb  1 03:09:09 2022
train Loss: 0.40

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 03:09:59 2022
batch 10, val loss = 0.03, mean loss = 0.02
Tue Feb  1 03:10:54 2022
val Loss: 0.02

epoch 70 was done for 467.640670 seconds
Epoch 71/299
----------
Tue Feb  1 03:11:32 2022
batch 0, train loss = 0.10, mean loss = 0.10
Tue Feb  1 03:11:37 2022
batch 10, train loss = 0.45, mean loss = 0.39
Tue Feb  1 03:12:31 2022
batch 20, train loss = 0.13, mean loss = 0.39
Tue Feb  1 03:13:27 2022
batch 30, train loss = 0.58, mean loss = 0.41
Tue Feb  1 03:14:21 2022
batch 40, train loss = 0.12, mean loss = 0.41
Tue Feb  1 03:15:14 2022
batch 50, train loss = 0.49, mean loss = 0.42
Tue Feb  1 03:16:07 2022
batch 60, train loss = 0.09, mean loss = 0.41
Tue Feb  1 03:17:01 2022
train Loss: 0.43

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 03:17:51 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 03:18:45 2022
val Loss: 0.02

epoch 71 was done for 470.870717 seconds
Epoch 72/299
----------
Tue Feb  1 03:19:23 2022
batch 0, train loss = 0.11, mean loss = 0.11
Tue Feb  1 03:19:28 2022
batch 10, train loss = 0.48, mean loss = 0.43
Tue Feb  1 03:20:22 2022
batch 20, train loss = 0.19, mean loss = 0.43
Tue Feb  1 03:21:17 2022
batch 30, train loss = 0.68, mean loss = 0.44
Tue Feb  1 03:22:11 2022
batch 40, train loss = 0.11, mean loss = 0.45
Tue Feb  1 03:23:05 2022
batch 50, train loss = 0.50, mean loss = 0.46
Tue Feb  1 03:23:59 2022
batch 60, train loss = 0.13, mean loss = 0.46
Tue Feb  1 03:24:53 2022
train Loss: 0.47

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 03:25:43 2022
batch 10, val loss = 0.02, mean loss = 0.03
Tue Feb  1 03:26:38 2022
val Loss: 0.03

epoch 72 was done for 472.660263 seconds
Epoch 73/299
----------
Tue Feb  1 03:27:16 2022
batch 0, train loss = 0.18, mean loss = 0.18
Tue Feb  1 03:27:21 2022
batch 10, train loss = 0.58, mean loss = 0.48
Tue Feb  1 03:28:14 2022
batch 20, train loss = 0.24, mean loss = 0.48
Tue Feb  1 03:29:07 2022
batch 30, train loss = 0.74, mean loss = 0.49
Tue Feb  1 03:30:00 2022
batch 40, train loss = 0.11, mean loss = 0.49
Tue Feb  1 03:30:53 2022
batch 50, train loss = 0.48, mean loss = 0.51
Tue Feb  1 03:31:46 2022
batch 60, train loss = 0.23, mean loss = 0.50
Tue Feb  1 03:32:39 2022
train Loss: 0.51

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 03:33:29 2022
batch 10, val loss = 0.02, mean loss = 0.03
Tue Feb  1 03:34:22 2022
val Loss: 0.03

epoch 73 was done for 464.004477 seconds
Epoch 74/299
----------
Tue Feb  1 03:35:00 2022
batch 0, train loss = 0.33, mean loss = 0.33
Tue Feb  1 03:35:05 2022
batch 10, train loss = 0.75, mean loss = 0.52
Tue Feb  1 03:35:58 2022
batch 20, train loss = 0.21, mean loss = 0.52
Tue Feb  1 03:36:51 2022
batch 30, train loss = 0.64, mean loss = 0.54
Tue Feb  1 03:37:45 2022
batch 40, train loss = 0.26, mean loss = 0.56
Tue Feb  1 03:38:38 2022
batch 50, train loss = 0.58, mean loss = 0.57
Tue Feb  1 03:39:31 2022
batch 60, train loss = 0.40, mean loss = 0.57
Tue Feb  1 03:40:24 2022
train Loss: 0.57

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 03:41:14 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 03:42:08 2022
val Loss: 0.03

epoch 74 was done for 465.780287 seconds
Epoch 75/299
----------
Tue Feb  1 03:42:45 2022
batch 0, train loss = 0.52, mean loss = 0.52
Tue Feb  1 03:42:51 2022
batch 10, train loss = 0.90, mean loss = 0.55
Tue Feb  1 03:43:45 2022
batch 20, train loss = 0.13, mean loss = 0.54
Tue Feb  1 03:44:39 2022
batch 30, train loss = 0.55, mean loss = 0.58
Tue Feb  1 03:45:33 2022
batch 40, train loss = 0.48, mean loss = 0.60
Tue Feb  1 03:46:27 2022
batch 50, train loss = 0.71, mean loss = 0.60
Tue Feb  1 03:47:21 2022
batch 60, train loss = 0.30, mean loss = 0.59
Tue Feb  1 03:48:16 2022
train Loss: 0.59

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 03:49:07 2022
batch 10, val loss = 0.02, mean loss = 0.03
Tue Feb  1 03:50:01 2022
val Loss: 0.03

epoch 75 was done for 473.819407 seconds
Epoch 76/299
----------
Tue Feb  1 03:50:39 2022
batch 0, train loss = 0.34, mean loss = 0.34
Tue Feb  1 03:50:44 2022
batch 10, train loss = 0.62, mean loss = 0.60
Tue Feb  1 03:51:38 2022
batch 20, train loss = 0.47, mean loss = 0.68
Tue Feb  1 03:52:31 2022
batch 30, train loss = 0.97, mean loss = 0.69
Tue Feb  1 03:53:24 2022
batch 40, train loss = 0.47, mean loss = 0.68
Tue Feb  1 03:54:17 2022
batch 50, train loss = 0.55, mean loss = 0.68
Tue Feb  1 03:55:11 2022
batch 60, train loss = 0.39, mean loss = 0.69
Tue Feb  1 03:56:04 2022
train Loss: 0.68

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 03:56:54 2022
batch 10, val loss = 0.03, mean loss = 0.03
Tue Feb  1 03:57:48 2022
val Loss: 0.03

epoch 76 was done for 466.101418 seconds
Epoch 77/299
----------
Tue Feb  1 03:58:25 2022
batch 0, train loss = 0.47, mean loss = 0.47
Tue Feb  1 03:58:31 2022
batch 10, train loss = 0.65, mean loss = 0.58
Tue Feb  1 03:59:24 2022
batch 20, train loss = 0.57, mean loss = 0.60
Tue Feb  1 04:00:20 2022
batch 30, train loss = 0.70, mean loss = 0.59
Tue Feb  1 04:01:13 2022
batch 40, train loss = 0.43, mean loss = 0.62
Tue Feb  1 04:02:06 2022
batch 50, train loss = 0.70, mean loss = 0.62
Tue Feb  1 04:02:59 2022
batch 60, train loss = 0.42, mean loss = 0.61
Tue Feb  1 04:03:53 2022
train Loss: 0.60

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 04:04:42 2022
batch 10, val loss = 0.02, mean loss = 0.03
Tue Feb  1 04:05:36 2022
val Loss: 0.03

epoch 77 was done for 468.098430 seconds
Epoch 78/299
----------
Tue Feb  1 04:06:13 2022
batch 0, train loss = 0.45, mean loss = 0.45
Tue Feb  1 04:06:19 2022
batch 10, train loss = 0.51, mean loss = 0.48
Tue Feb  1 04:07:12 2022
batch 20, train loss = 0.30, mean loss = 0.50
Tue Feb  1 04:08:05 2022
batch 30, train loss = 0.48, mean loss = 0.47
Tue Feb  1 04:08:58 2022
batch 40, train loss = 0.39, mean loss = 0.49
Tue Feb  1 04:09:50 2022
batch 50, train loss = 0.49, mean loss = 0.48
Tue Feb  1 04:10:43 2022
batch 60, train loss = 0.30, mean loss = 0.48
Tue Feb  1 04:11:36 2022
train Loss: 0.47

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 04:12:25 2022
batch 10, val loss = 0.02, mean loss = 0.03
Tue Feb  1 04:13:20 2022
val Loss: 0.03

epoch 78 was done for 464.942599 seconds
Epoch 79/299
----------
Tue Feb  1 04:13:58 2022
batch 0, train loss = 0.23, mean loss = 0.23
Tue Feb  1 04:14:04 2022
batch 10, train loss = 0.40, mean loss = 0.54
Tue Feb  1 04:14:57 2022
batch 20, train loss = 0.33, mean loss = 0.51
Tue Feb  1 04:15:51 2022
batch 30, train loss = 0.39, mean loss = 0.54
Tue Feb  1 04:16:44 2022
batch 40, train loss = 0.41, mean loss = 0.53
Tue Feb  1 04:17:37 2022
batch 50, train loss = 0.40, mean loss = 0.54
Tue Feb  1 04:18:30 2022
batch 60, train loss = 0.46, mean loss = 0.54
Tue Feb  1 04:19:24 2022
train Loss: 0.53

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 04:20:14 2022
batch 10, val loss = 0.03, mean loss = 0.03
Tue Feb  1 04:21:08 2022
val Loss: 0.02

epoch 79 was done for 466.653240 seconds
Epoch 80/299
----------
Tue Feb  1 04:21:45 2022
batch 0, train loss = 0.16, mean loss = 0.16
Tue Feb  1 04:21:50 2022
batch 10, train loss = 0.42, mean loss = 0.50
Tue Feb  1 04:22:44 2022
batch 20, train loss = 0.24, mean loss = 0.46
Tue Feb  1 04:23:38 2022
batch 30, train loss = 0.38, mean loss = 0.49
Tue Feb  1 04:24:32 2022
batch 40, train loss = 0.35, mean loss = 0.49
Tue Feb  1 04:25:26 2022
batch 50, train loss = 0.41, mean loss = 0.51
Tue Feb  1 04:26:20 2022
batch 60, train loss = 0.39, mean loss = 0.50
Tue Feb  1 04:27:14 2022
train Loss: 0.51

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 04:28:05 2022
batch 10, val loss = 0.02, mean loss = 0.03
Tue Feb  1 04:28:59 2022
val Loss: 0.03

epoch 80 was done for 472.220366 seconds
Epoch 81/299
----------
Tue Feb  1 04:29:37 2022
batch 0, train loss = 0.13, mean loss = 0.13
Tue Feb  1 04:29:43 2022
batch 10, train loss = 0.52, mean loss = 0.55
Tue Feb  1 04:30:35 2022
batch 20, train loss = 0.15, mean loss = 0.49
Tue Feb  1 04:31:28 2022
batch 30, train loss = 0.53, mean loss = 0.52
Tue Feb  1 04:32:21 2022
batch 40, train loss = 0.17, mean loss = 0.50
Tue Feb  1 04:33:14 2022
batch 50, train loss = 0.61, mean loss = 0.53
Tue Feb  1 04:34:06 2022
batch 60, train loss = 0.18, mean loss = 0.51
Tue Feb  1 04:34:59 2022
train Loss: 0.53

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 04:35:49 2022
batch 10, val loss = 0.03, mean loss = 0.03
Tue Feb  1 04:36:42 2022
val Loss: 0.03

epoch 81 was done for 461.816890 seconds
Epoch 82/299
----------
Tue Feb  1 04:37:19 2022
batch 0, train loss = 0.31, mean loss = 0.31
Tue Feb  1 04:37:24 2022
batch 10, train loss = 0.58, mean loss = 0.42
Tue Feb  1 04:38:18 2022
batch 20, train loss = 0.24, mean loss = 0.47
Tue Feb  1 04:39:11 2022
batch 30, train loss = 0.54, mean loss = 0.45
Tue Feb  1 04:40:04 2022
batch 40, train loss = 0.19, mean loss = 0.45
Tue Feb  1 04:40:58 2022
batch 50, train loss = 0.65, mean loss = 0.46
Tue Feb  1 04:41:51 2022
batch 60, train loss = 0.15, mean loss = 0.46
Tue Feb  1 04:42:45 2022
train Loss: 0.46

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 04:43:34 2022
batch 10, val loss = 0.02, mean loss = 0.03
Tue Feb  1 04:44:28 2022
val Loss: 0.03

epoch 82 was done for 466.874968 seconds
Epoch 83/299
----------
Tue Feb  1 04:45:06 2022
batch 0, train loss = 0.37, mean loss = 0.37
Tue Feb  1 04:45:11 2022
batch 10, train loss = 0.44, mean loss = 0.41
Tue Feb  1 04:46:05 2022
batch 20, train loss = 0.31, mean loss = 0.44
Tue Feb  1 04:46:59 2022
batch 30, train loss = 0.45, mean loss = 0.43
Tue Feb  1 04:47:53 2022
batch 40, train loss = 0.24, mean loss = 0.43
Tue Feb  1 04:48:48 2022
batch 50, train loss = 0.56, mean loss = 0.43
Tue Feb  1 04:49:42 2022
batch 60, train loss = 0.20, mean loss = 0.43
Tue Feb  1 04:50:36 2022
train Loss: 0.43

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 04:51:27 2022
batch 10, val loss = 0.03, mean loss = 0.03
Tue Feb  1 04:52:21 2022
val Loss: 0.03

epoch 83 was done for 473.436567 seconds
Epoch 84/299
----------
Tue Feb  1 04:52:59 2022
batch 0, train loss = 0.28, mean loss = 0.28
Tue Feb  1 04:53:05 2022
batch 10, train loss = 0.45, mean loss = 0.46
Tue Feb  1 04:53:58 2022
batch 20, train loss = 0.35, mean loss = 0.48
Tue Feb  1 04:54:51 2022
batch 30, train loss = 0.50, mean loss = 0.47
Tue Feb  1 04:55:44 2022
batch 40, train loss = 0.31, mean loss = 0.48
Tue Feb  1 04:56:37 2022
batch 50, train loss = 0.54, mean loss = 0.47
Tue Feb  1 04:57:30 2022
batch 60, train loss = 0.29, mean loss = 0.47
Tue Feb  1 04:58:23 2022
train Loss: 0.47

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 04:59:12 2022
batch 10, val loss = 0.03, mean loss = 0.03
Tue Feb  1 05:00:06 2022
val Loss: 0.03

epoch 84 was done for 463.912451 seconds
Epoch 85/299
----------
Tue Feb  1 05:00:43 2022
batch 0, train loss = 0.21, mean loss = 0.21
Tue Feb  1 05:00:49 2022
batch 10, train loss = 0.50, mean loss = 0.49
Tue Feb  1 05:01:42 2022
batch 20, train loss = 0.28, mean loss = 0.47
Tue Feb  1 05:02:35 2022
batch 30, train loss = 0.52, mean loss = 0.49
Tue Feb  1 05:03:28 2022
batch 40, train loss = 0.26, mean loss = 0.48
Tue Feb  1 05:04:21 2022
batch 50, train loss = 0.52, mean loss = 0.49
Tue Feb  1 05:05:14 2022
batch 60, train loss = 0.26, mean loss = 0.48
Tue Feb  1 05:06:08 2022
train Loss: 0.48

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 05:06:57 2022
batch 10, val loss = 0.03, mean loss = 0.02
Tue Feb  1 05:07:51 2022
val Loss: 0.02

epoch 85 was done for 465.415192 seconds
Epoch 86/299
----------
Tue Feb  1 05:08:29 2022
batch 0, train loss = 0.22, mean loss = 0.22
Tue Feb  1 05:08:34 2022
batch 10, train loss = 0.55, mean loss = 0.47
Tue Feb  1 05:09:28 2022
batch 20, train loss = 0.26, mean loss = 0.47
Tue Feb  1 05:10:23 2022
batch 30, train loss = 0.60, mean loss = 0.48
Tue Feb  1 05:11:17 2022
batch 40, train loss = 0.24, mean loss = 0.48
Tue Feb  1 05:12:11 2022
batch 50, train loss = 0.58, mean loss = 0.48
Tue Feb  1 05:13:08 2022
batch 60, train loss = 0.23, mean loss = 0.48
Tue Feb  1 05:14:03 2022
train Loss: 0.47

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 05:14:53 2022
batch 10, val loss = 0.03, mean loss = 0.03
Tue Feb  1 05:15:48 2022
val Loss: 0.03

epoch 86 was done for 477.455036 seconds
Epoch 87/299
----------
Tue Feb  1 05:16:26 2022
batch 0, train loss = 0.31, mean loss = 0.31
Tue Feb  1 05:16:31 2022
batch 10, train loss = 0.45, mean loss = 0.47
Tue Feb  1 05:17:25 2022
batch 20, train loss = 0.33, mean loss = 0.47
Tue Feb  1 05:18:18 2022
batch 30, train loss = 0.48, mean loss = 0.47
Tue Feb  1 05:19:11 2022
batch 40, train loss = 0.30, mean loss = 0.48
Tue Feb  1 05:20:05 2022
batch 50, train loss = 0.55, mean loss = 0.48
Tue Feb  1 05:20:58 2022
batch 60, train loss = 0.30, mean loss = 0.48
Tue Feb  1 05:21:51 2022
train Loss: 0.48

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 05:22:41 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 05:23:35 2022
val Loss: 0.02

epoch 87 was done for 465.865685 seconds
Epoch 88/299
----------
Tue Feb  1 05:24:12 2022
batch 0, train loss = 0.28, mean loss = 0.28
Tue Feb  1 05:24:17 2022
batch 10, train loss = 0.47, mean loss = 0.49
Tue Feb  1 05:25:10 2022
batch 20, train loss = 0.32, mean loss = 0.49
Tue Feb  1 05:26:04 2022
batch 30, train loss = 0.50, mean loss = 0.49
Tue Feb  1 05:26:57 2022
batch 40, train loss = 0.29, mean loss = 0.49
Tue Feb  1 05:27:50 2022
batch 50, train loss = 0.62, mean loss = 0.50
Tue Feb  1 05:28:44 2022
batch 60, train loss = 0.31, mean loss = 0.50
Tue Feb  1 05:29:37 2022
train Loss: 0.50

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 05:30:27 2022
batch 10, val loss = 0.03, mean loss = 0.02
Tue Feb  1 05:31:21 2022
val Loss: 0.02

epoch 88 was done for 466.432881 seconds
Epoch 89/299
----------
Tue Feb  1 05:31:58 2022
batch 0, train loss = 0.34, mean loss = 0.34
Tue Feb  1 05:32:04 2022
batch 10, train loss = 0.46, mean loss = 0.51
Tue Feb  1 05:32:58 2022
batch 20, train loss = 0.37, mean loss = 0.50
Tue Feb  1 05:33:52 2022
batch 30, train loss = 0.48, mean loss = 0.50
Tue Feb  1 05:34:47 2022
batch 40, train loss = 0.32, mean loss = 0.50
Tue Feb  1 05:35:41 2022
batch 50, train loss = 0.52, mean loss = 0.50
Tue Feb  1 05:36:35 2022
batch 60, train loss = 0.32, mean loss = 0.50
Tue Feb  1 05:37:30 2022
train Loss: 0.49

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 05:38:21 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 05:39:15 2022
val Loss: 0.02

epoch 89 was done for 475.063246 seconds
Epoch 90/299
----------
Tue Feb  1 05:39:53 2022
batch 0, train loss = 0.28, mean loss = 0.28
Tue Feb  1 05:39:59 2022
batch 10, train loss = 0.47, mean loss = 0.48
Tue Feb  1 05:40:52 2022
batch 20, train loss = 0.29, mean loss = 0.45
Tue Feb  1 05:41:45 2022
batch 30, train loss = 0.46, mean loss = 0.47
Tue Feb  1 05:42:38 2022
batch 40, train loss = 0.22, mean loss = 0.45
Tue Feb  1 05:43:31 2022
batch 50, train loss = 0.38, mean loss = 0.46
Tue Feb  1 05:44:24 2022
batch 60, train loss = 0.25, mean loss = 0.45
Tue Feb  1 05:45:18 2022
train Loss: 0.45

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 05:46:07 2022
batch 10, val loss = 0.03, mean loss = 0.02
Tue Feb  1 05:47:01 2022
val Loss: 0.02

epoch 90 was done for 464.836664 seconds
Epoch 91/299
----------
Tue Feb  1 05:47:38 2022
batch 0, train loss = 0.17, mean loss = 0.17
Tue Feb  1 05:47:44 2022
batch 10, train loss = 0.51, mean loss = 0.41
Tue Feb  1 05:48:37 2022
batch 20, train loss = 0.16, mean loss = 0.38
Tue Feb  1 05:49:30 2022
batch 30, train loss = 0.48, mean loss = 0.41
Tue Feb  1 05:50:24 2022
batch 40, train loss = 0.12, mean loss = 0.39
Tue Feb  1 05:51:17 2022
batch 50, train loss = 0.38, mean loss = 0.41
Tue Feb  1 05:52:10 2022
batch 60, train loss = 0.15, mean loss = 0.40
Tue Feb  1 05:53:04 2022
train Loss: 0.40

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 05:53:54 2022
batch 10, val loss = 0.03, mean loss = 0.03
Tue Feb  1 05:54:48 2022
val Loss: 0.02

epoch 91 was done for 467.071333 seconds
Epoch 92/299
----------
Tue Feb  1 05:55:25 2022
batch 0, train loss = 0.16, mean loss = 0.16
Tue Feb  1 05:55:31 2022
batch 10, train loss = 0.50, mean loss = 0.37
Tue Feb  1 05:56:23 2022
batch 20, train loss = 0.13, mean loss = 0.36
Tue Feb  1 05:57:16 2022
batch 30, train loss = 0.48, mean loss = 0.38
Tue Feb  1 05:58:09 2022
batch 40, train loss = 0.09, mean loss = 0.37
Tue Feb  1 05:59:02 2022
batch 50, train loss = 0.39, mean loss = 0.39
Tue Feb  1 05:59:54 2022
batch 60, train loss = 0.12, mean loss = 0.38
Tue Feb  1 06:00:47 2022
train Loss: 0.38

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 06:01:37 2022
batch 10, val loss = 0.03, mean loss = 0.03
Tue Feb  1 06:02:31 2022
val Loss: 0.03

epoch 92 was done for 463.665232 seconds
Epoch 93/299
----------
Tue Feb  1 06:03:09 2022
batch 0, train loss = 0.16, mean loss = 0.16
Tue Feb  1 06:03:14 2022
batch 10, train loss = 0.53, mean loss = 0.37
Tue Feb  1 06:04:07 2022
batch 20, train loss = 0.12, mean loss = 0.36
Tue Feb  1 06:05:01 2022
batch 30, train loss = 0.53, mean loss = 0.37
Tue Feb  1 06:05:54 2022
batch 40, train loss = 0.08, mean loss = 0.37
Tue Feb  1 06:06:47 2022
batch 50, train loss = 0.47, mean loss = 0.39
Tue Feb  1 06:07:39 2022
batch 60, train loss = 0.08, mean loss = 0.38
Tue Feb  1 06:08:32 2022
train Loss: 0.39

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 06:09:22 2022
batch 10, val loss = 0.03, mean loss = 0.03
Tue Feb  1 06:10:15 2022
val Loss: 0.03

epoch 93 was done for 463.538939 seconds
Epoch 94/299
----------
Tue Feb  1 06:10:53 2022
batch 0, train loss = 0.21, mean loss = 0.21
Tue Feb  1 06:10:58 2022
batch 10, train loss = 0.53, mean loss = 0.36
Tue Feb  1 06:11:52 2022
batch 20, train loss = 0.18, mean loss = 0.38
Tue Feb  1 06:12:46 2022
batch 30, train loss = 0.54, mean loss = 0.38
Tue Feb  1 06:13:40 2022
batch 40, train loss = 0.14, mean loss = 0.38
Tue Feb  1 06:14:35 2022
batch 50, train loss = 0.54, mean loss = 0.39
Tue Feb  1 06:15:29 2022
batch 60, train loss = 0.09, mean loss = 0.39
Tue Feb  1 06:16:23 2022
train Loss: 0.40

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 06:17:14 2022
batch 10, val loss = 0.03, mean loss = 0.03
Tue Feb  1 06:18:09 2022
val Loss: 0.03

epoch 94 was done for 474.656903 seconds
Epoch 95/299
----------
Tue Feb  1 06:18:47 2022
batch 0, train loss = 0.27, mean loss = 0.27
Tue Feb  1 06:18:53 2022
batch 10, train loss = 0.46, mean loss = 0.36
Tue Feb  1 06:19:47 2022
batch 20, train loss = 0.23, mean loss = 0.38
Tue Feb  1 06:20:41 2022
batch 30, train loss = 0.50, mean loss = 0.38
Tue Feb  1 06:21:35 2022
batch 40, train loss = 0.21, mean loss = 0.39
Tue Feb  1 06:22:29 2022
batch 50, train loss = 0.45, mean loss = 0.40
Tue Feb  1 06:23:23 2022
batch 60, train loss = 0.15, mean loss = 0.39
Tue Feb  1 06:24:17 2022
train Loss: 0.39

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 06:25:08 2022
batch 10, val loss = 0.03, mean loss = 0.03
Tue Feb  1 06:26:03 2022
val Loss: 0.03

epoch 95 was done for 473.242390 seconds
Epoch 96/299
----------
Tue Feb  1 06:26:40 2022
batch 0, train loss = 0.22, mean loss = 0.22
Tue Feb  1 06:26:46 2022
batch 10, train loss = 0.43, mean loss = 0.37
Tue Feb  1 06:27:39 2022
batch 20, train loss = 0.20, mean loss = 0.37
Tue Feb  1 06:28:33 2022
batch 30, train loss = 0.50, mean loss = 0.38
Tue Feb  1 06:29:26 2022
batch 40, train loss = 0.22, mean loss = 0.39
Tue Feb  1 06:30:20 2022
batch 50, train loss = 0.46, mean loss = 0.40
Tue Feb  1 06:31:13 2022
batch 60, train loss = 0.14, mean loss = 0.40
Tue Feb  1 06:32:07 2022
train Loss: 0.39

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 06:32:57 2022
batch 10, val loss = 0.03, mean loss = 0.03
Tue Feb  1 06:33:51 2022
val Loss: 0.03

epoch 96 was done for 468.068842 seconds
Epoch 97/299
----------
Tue Feb  1 06:34:29 2022
batch 0, train loss = 0.20, mean loss = 0.20
Tue Feb  1 06:34:34 2022
batch 10, train loss = 0.44, mean loss = 0.37
Tue Feb  1 06:35:29 2022
batch 20, train loss = 0.17, mean loss = 0.36
Tue Feb  1 06:36:24 2022
batch 30, train loss = 0.50, mean loss = 0.37
Tue Feb  1 06:37:18 2022
batch 40, train loss = 0.17, mean loss = 0.37
Tue Feb  1 06:38:13 2022
batch 50, train loss = 0.48, mean loss = 0.39
Tue Feb  1 06:39:08 2022
batch 60, train loss = 0.12, mean loss = 0.39
Tue Feb  1 06:40:03 2022
train Loss: 0.39

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 06:40:54 2022
batch 10, val loss = 0.03, mean loss = 0.03
Tue Feb  1 06:41:50 2022
val Loss: 0.03

epoch 97 was done for 479.296456 seconds
Epoch 98/299
----------
Tue Feb  1 06:42:28 2022
batch 0, train loss = 0.19, mean loss = 0.19
Tue Feb  1 06:42:33 2022
batch 10, train loss = 0.44, mean loss = 0.35
Tue Feb  1 06:43:27 2022
batch 20, train loss = 0.18, mean loss = 0.36
Tue Feb  1 06:44:20 2022
batch 30, train loss = 0.47, mean loss = 0.36
Tue Feb  1 06:45:14 2022
batch 40, train loss = 0.17, mean loss = 0.37
Tue Feb  1 06:46:07 2022
batch 50, train loss = 0.44, mean loss = 0.37
Tue Feb  1 06:47:01 2022
batch 60, train loss = 0.15, mean loss = 0.37
Tue Feb  1 06:47:54 2022
train Loss: 0.37

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 06:48:47 2022
batch 10, val loss = 0.03, mean loss = 0.03
Tue Feb  1 06:49:41 2022
val Loss: 0.03

epoch 98 was done for 470.682878 seconds
Epoch 99/299
----------
Tue Feb  1 06:50:18 2022
batch 0, train loss = 0.22, mean loss = 0.22
Tue Feb  1 06:50:24 2022
batch 10, train loss = 0.40, mean loss = 0.38
Tue Feb  1 06:51:17 2022
batch 20, train loss = 0.21, mean loss = 0.39
Tue Feb  1 06:52:11 2022
batch 30, train loss = 0.41, mean loss = 0.38
Tue Feb  1 06:53:05 2022
batch 40, train loss = 0.24, mean loss = 0.39
Tue Feb  1 06:53:58 2022
batch 50, train loss = 0.40, mean loss = 0.38
Tue Feb  1 06:54:52 2022
batch 60, train loss = 0.21, mean loss = 0.39
Tue Feb  1 06:55:45 2022
train Loss: 0.38

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 06:56:38 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 06:57:32 2022
val Loss: 0.02

epoch 99 was done for 471.405429 seconds
Epoch 100/299
----------
Tue Feb  1 06:58:10 2022
batch 0, train loss = 0.16, mean loss = 0.16
Tue Feb  1 06:58:15 2022
batch 10, train loss = 0.37, mean loss = 0.42
Tue Feb  1 06:59:10 2022
batch 20, train loss = 0.18, mean loss = 0.39
Tue Feb  1 07:00:05 2022
batch 30, train loss = 0.36, mean loss = 0.40
Tue Feb  1 07:00:59 2022
batch 40, train loss = 0.22, mean loss = 0.40
Tue Feb  1 07:01:54 2022
batch 50, train loss = 0.35, mean loss = 0.40
Tue Feb  1 07:02:49 2022
batch 60, train loss = 0.22, mean loss = 0.39
Tue Feb  1 07:03:44 2022
train Loss: 0.39

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 07:04:35 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 07:05:30 2022
val Loss: 0.02

epoch 100 was done for 478.408445 seconds
Epoch 101/299
----------
Tue Feb  1 07:06:08 2022
batch 0, train loss = 0.08, mean loss = 0.08
Tue Feb  1 07:06:14 2022
batch 10, train loss = 0.45, mean loss = 0.41
Tue Feb  1 07:07:07 2022
batch 20, train loss = 0.08, mean loss = 0.37
Tue Feb  1 07:08:01 2022
batch 30, train loss = 0.41, mean loss = 0.39
Tue Feb  1 07:08:54 2022
batch 40, train loss = 0.08, mean loss = 0.37
Tue Feb  1 07:09:48 2022
batch 50, train loss = 0.42, mean loss = 0.39
Tue Feb  1 07:10:41 2022
batch 60, train loss = 0.10, mean loss = 0.37
Tue Feb  1 07:11:34 2022
train Loss: 0.38

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 07:12:24 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 07:13:18 2022
val Loss: 0.02

epoch 101 was done for 466.898945 seconds
Epoch 102/299
----------
Tue Feb  1 07:13:55 2022
batch 0, train loss = 0.12, mean loss = 0.12
Tue Feb  1 07:14:01 2022
batch 10, train loss = 0.45, mean loss = 0.34
Tue Feb  1 07:14:54 2022
batch 20, train loss = 0.10, mean loss = 0.35
Tue Feb  1 07:15:48 2022
batch 30, train loss = 0.44, mean loss = 0.36
Tue Feb  1 07:16:41 2022
batch 40, train loss = 0.07, mean loss = 0.35
Tue Feb  1 07:17:35 2022
batch 50, train loss = 0.47, mean loss = 0.36
Tue Feb  1 07:18:28 2022
batch 60, train loss = 0.06, mean loss = 0.35
Tue Feb  1 07:19:22 2022
train Loss: 0.35

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 07:20:11 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 07:21:06 2022
val Loss: 0.02

epoch 102 was done for 467.646894 seconds
Epoch 103/299
----------
Tue Feb  1 07:21:43 2022
batch 0, train loss = 0.15, mean loss = 0.15
Tue Feb  1 07:21:48 2022
batch 10, train loss = 0.39, mean loss = 0.32
Tue Feb  1 07:22:43 2022
batch 20, train loss = 0.13, mean loss = 0.34
Tue Feb  1 07:23:37 2022
batch 30, train loss = 0.41, mean loss = 0.34
Tue Feb  1 07:24:31 2022
batch 40, train loss = 0.11, mean loss = 0.34
Tue Feb  1 07:25:25 2022
batch 50, train loss = 0.46, mean loss = 0.35
Tue Feb  1 07:26:20 2022
batch 60, train loss = 0.08, mean loss = 0.34
Tue Feb  1 07:27:14 2022
train Loss: 0.35

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 07:28:05 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 07:29:00 2022
val Loss: 0.02

epoch 103 was done for 475.085349 seconds
Epoch 104/299
----------
Tue Feb  1 07:29:38 2022
batch 0, train loss = 0.14, mean loss = 0.14
Tue Feb  1 07:29:43 2022
batch 10, train loss = 0.38, mean loss = 0.33
Tue Feb  1 07:30:37 2022
batch 20, train loss = 0.13, mean loss = 0.34
Tue Feb  1 07:31:30 2022
batch 30, train loss = 0.41, mean loss = 0.35
Tue Feb  1 07:32:23 2022
batch 40, train loss = 0.11, mean loss = 0.35
Tue Feb  1 07:33:17 2022
batch 50, train loss = 0.46, mean loss = 0.35
Tue Feb  1 07:34:10 2022
batch 60, train loss = 0.08, mean loss = 0.35
Tue Feb  1 07:35:04 2022
train Loss: 0.35

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 07:35:53 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 07:36:47 2022
val Loss: 0.02

epoch 104 was done for 466.635370 seconds
Epoch 105/299
----------
Tue Feb  1 07:37:25 2022
batch 0, train loss = 0.14, mean loss = 0.14
Tue Feb  1 07:37:30 2022
batch 10, train loss = 0.37, mean loss = 0.33
Tue Feb  1 07:38:24 2022
batch 20, train loss = 0.14, mean loss = 0.34
Tue Feb  1 07:39:17 2022
batch 30, train loss = 0.40, mean loss = 0.35
Tue Feb  1 07:40:11 2022
batch 40, train loss = 0.12, mean loss = 0.35
Tue Feb  1 07:41:05 2022
batch 50, train loss = 0.44, mean loss = 0.35
Tue Feb  1 07:41:58 2022
batch 60, train loss = 0.11, mean loss = 0.35
Tue Feb  1 07:42:52 2022
train Loss: 0.35

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 07:43:43 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 07:44:37 2022
val Loss: 0.02

epoch 105 was done for 469.986585 seconds
Epoch 106/299
----------
Tue Feb  1 07:45:15 2022
batch 0, train loss = 0.13, mean loss = 0.13
Tue Feb  1 07:45:20 2022
batch 10, train loss = 0.35, mean loss = 0.35
Tue Feb  1 07:46:13 2022
batch 20, train loss = 0.15, mean loss = 0.35
Tue Feb  1 07:47:06 2022
batch 30, train loss = 0.39, mean loss = 0.36
Tue Feb  1 07:47:59 2022
batch 40, train loss = 0.14, mean loss = 0.36
Tue Feb  1 07:48:52 2022
batch 50, train loss = 0.42, mean loss = 0.36
Tue Feb  1 07:49:44 2022
batch 60, train loss = 0.14, mean loss = 0.36
Tue Feb  1 07:50:37 2022
train Loss: 0.36

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 07:51:27 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 07:52:22 2022
val Loss: 0.02

epoch 106 was done for 465.189383 seconds
Epoch 107/299
----------
Tue Feb  1 07:53:00 2022
batch 0, train loss = 0.10, mean loss = 0.10
Tue Feb  1 07:53:05 2022
batch 10, train loss = 0.36, mean loss = 0.37
Tue Feb  1 07:53:59 2022
batch 20, train loss = 0.13, mean loss = 0.35
Tue Feb  1 07:54:52 2022
batch 30, train loss = 0.39, mean loss = 0.37
Tue Feb  1 07:55:46 2022
batch 40, train loss = 0.13, mean loss = 0.37
Tue Feb  1 07:56:49 2022
batch 50, train loss = 0.38, mean loss = 0.37
Tue Feb  1 07:57:43 2022
batch 60, train loss = 0.16, mean loss = 0.37
Tue Feb  1 07:58:36 2022
train Loss: 0.37

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 07:59:26 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 08:00:20 2022
val Loss: 0.02

epoch 107 was done for 478.012342 seconds
Epoch 108/299
----------
Tue Feb  1 08:00:58 2022
batch 0, train loss = 0.06, mean loss = 0.06
Tue Feb  1 08:01:03 2022
batch 10, train loss = 0.42, mean loss = 0.40
Tue Feb  1 08:02:03 2022
batch 20, train loss = 0.08, mean loss = 0.36
Tue Feb  1 08:03:04 2022
batch 30, train loss = 0.44, mean loss = 0.39
Tue Feb  1 08:04:11 2022
batch 40, train loss = 0.09, mean loss = 0.38
Tue Feb  1 08:05:26 2022
batch 50, train loss = 0.39, mean loss = 0.39
Tue Feb  1 08:06:23 2022
batch 60, train loss = 0.16, mean loss = 0.39
Tue Feb  1 08:07:18 2022
train Loss: 0.39

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 08:08:09 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 08:09:10 2022
val Loss: 0.02

epoch 108 was done for 535.453068 seconds
Epoch 109/299
----------
Tue Feb  1 08:09:53 2022
batch 0, train loss = 0.06, mean loss = 0.06
Tue Feb  1 08:10:00 2022
batch 10, train loss = 0.48, mean loss = 0.39
Tue Feb  1 08:11:11 2022
batch 20, train loss = 0.07, mean loss = 0.36
Tue Feb  1 08:12:26 2022
batch 30, train loss = 0.48, mean loss = 0.38
Tue Feb  1 08:13:41 2022
batch 40, train loss = 0.06, mean loss = 0.37
Tue Feb  1 08:14:50 2022
batch 50, train loss = 0.42, mean loss = 0.39
Tue Feb  1 08:15:58 2022
batch 60, train loss = 0.12, mean loss = 0.38
Tue Feb  1 08:17:08 2022
train Loss: 0.39

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 08:18:15 2022
batch 10, val loss = 0.03, mean loss = 0.03
Tue Feb  1 08:19:26 2022
val Loss: 0.03

epoch 109 was done for 619.306633 seconds
Epoch 110/299
----------
Tue Feb  1 08:20:13 2022
batch 0, train loss = 0.07, mean loss = 0.07
Tue Feb  1 08:20:20 2022
batch 10, train loss = 0.49, mean loss = 0.37
Tue Feb  1 08:21:33 2022
batch 20, train loss = 0.07, mean loss = 0.35
Tue Feb  1 08:22:44 2022
batch 30, train loss = 0.46, mean loss = 0.37
Tue Feb  1 08:23:55 2022
batch 40, train loss = 0.07, mean loss = 0.36
Tue Feb  1 08:25:06 2022
batch 50, train loss = 0.42, mean loss = 0.37
Tue Feb  1 08:26:16 2022
batch 60, train loss = 0.07, mean loss = 0.37
Tue Feb  1 08:27:24 2022
train Loss: 0.38

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 08:28:27 2022
batch 10, val loss = 0.03, mean loss = 0.03
Tue Feb  1 08:29:34 2022
val Loss: 0.03

epoch 110 was done for 606.228865 seconds
Epoch 111/299
----------
Tue Feb  1 08:30:19 2022
batch 0, train loss = 0.12, mean loss = 0.12
Tue Feb  1 08:30:25 2022
batch 10, train loss = 0.54, mean loss = 0.38
Tue Feb  1 08:31:28 2022
batch 20, train loss = 0.10, mean loss = 0.38
Tue Feb  1 08:32:27 2022
batch 30, train loss = 0.57, mean loss = 0.39
Tue Feb  1 08:33:27 2022
batch 40, train loss = 0.11, mean loss = 0.39
Tue Feb  1 08:34:33 2022
batch 50, train loss = 0.50, mean loss = 0.40
Tue Feb  1 08:35:40 2022
batch 60, train loss = 0.09, mean loss = 0.39
Tue Feb  1 08:36:49 2022
train Loss: 0.40

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 08:37:54 2022
batch 10, val loss = 0.03, mean loss = 0.03
Tue Feb  1 08:39:05 2022
val Loss: 0.03

epoch 111 was done for 574.373445 seconds
Epoch 112/299
----------
Tue Feb  1 08:39:53 2022
batch 0, train loss = 0.26, mean loss = 0.26
Tue Feb  1 08:40:00 2022
batch 10, train loss = 0.46, mean loss = 0.39
Tue Feb  1 08:41:11 2022
batch 20, train loss = 0.24, mean loss = 0.42
Tue Feb  1 08:42:23 2022
batch 30, train loss = 0.50, mean loss = 0.41
Tue Feb  1 08:43:34 2022
batch 40, train loss = 0.27, mean loss = 0.42
Tue Feb  1 08:44:45 2022
batch 50, train loss = 0.47, mean loss = 0.42
Tue Feb  1 08:45:56 2022
batch 60, train loss = 0.19, mean loss = 0.42
Tue Feb  1 08:47:06 2022
train Loss: 0.41

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 08:48:14 2022
batch 10, val loss = 0.03, mean loss = 0.02
Tue Feb  1 08:49:28 2022
val Loss: 0.02

epoch 112 was done for 623.544062 seconds
Epoch 113/299
----------
Tue Feb  1 08:50:17 2022
batch 0, train loss = 0.21, mean loss = 0.21
Tue Feb  1 08:50:24 2022
batch 10, train loss = 0.41, mean loss = 0.41
Tue Feb  1 08:51:36 2022
batch 20, train loss = 0.21, mean loss = 0.40
Tue Feb  1 08:52:48 2022
batch 30, train loss = 0.42, mean loss = 0.41
Tue Feb  1 08:53:59 2022
batch 40, train loss = 0.23, mean loss = 0.41
Tue Feb  1 08:55:10 2022
batch 50, train loss = 0.42, mean loss = 0.41
Tue Feb  1 08:56:20 2022
batch 60, train loss = 0.18, mean loss = 0.40
Tue Feb  1 08:57:32 2022
train Loss: 0.40

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 08:58:40 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 08:59:53 2022
val Loss: 0.02

epoch 113 was done for 625.288780 seconds
Epoch 114/299
----------
Tue Feb  1 09:00:42 2022
batch 0, train loss = 0.13, mean loss = 0.13
Tue Feb  1 09:00:49 2022
batch 10, train loss = 0.39, mean loss = 0.38
Tue Feb  1 09:02:02 2022
batch 20, train loss = 0.15, mean loss = 0.36
Tue Feb  1 09:03:14 2022
batch 30, train loss = 0.40, mean loss = 0.38
Tue Feb  1 09:04:20 2022
batch 40, train loss = 0.13, mean loss = 0.37
Tue Feb  1 09:05:25 2022
batch 50, train loss = 0.39, mean loss = 0.38
Tue Feb  1 09:06:22 2022
batch 60, train loss = 0.12, mean loss = 0.37
Tue Feb  1 09:07:20 2022
train Loss: 0.37

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 09:08:13 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 09:09:09 2022
val Loss: 0.02

epoch 114 was done for 545.943258 seconds
Epoch 115/299
----------
Tue Feb  1 09:09:48 2022
batch 0, train loss = 0.10, mean loss = 0.10
Tue Feb  1 09:09:54 2022
batch 10, train loss = 0.46, mean loss = 0.35
Tue Feb  1 09:10:50 2022
batch 20, train loss = 0.09, mean loss = 0.35
Tue Feb  1 09:11:44 2022
batch 30, train loss = 0.43, mean loss = 0.36
Tue Feb  1 09:12:38 2022
batch 40, train loss = 0.07, mean loss = 0.35
Tue Feb  1 09:13:32 2022
batch 50, train loss = 0.42, mean loss = 0.36
Tue Feb  1 09:14:26 2022
batch 60, train loss = 0.09, mean loss = 0.35
Tue Feb  1 09:15:21 2022
train Loss: 0.36

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 09:16:13 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 09:17:08 2022
val Loss: 0.02

epoch 115 was done for 478.180756 seconds
Epoch 116/299
----------
Tue Feb  1 09:17:46 2022
batch 0, train loss = 0.12, mean loss = 0.12
Tue Feb  1 09:17:52 2022
batch 10, train loss = 0.44, mean loss = 0.35
Tue Feb  1 09:18:46 2022
batch 20, train loss = 0.12, mean loss = 0.36
Tue Feb  1 09:19:45 2022
batch 30, train loss = 0.45, mean loss = 0.36
Tue Feb  1 09:20:39 2022
batch 40, train loss = 0.12, mean loss = 0.37
Tue Feb  1 09:21:34 2022
batch 50, train loss = 0.44, mean loss = 0.37
Tue Feb  1 09:22:43 2022
batch 60, train loss = 0.12, mean loss = 0.37
Tue Feb  1 09:23:39 2022
train Loss: 0.37

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 09:24:29 2022
batch 10, val loss = 0.02, mean loss = 0.03
Tue Feb  1 09:25:24 2022
val Loss: 0.03

epoch 116 was done for 495.690439 seconds
Epoch 117/299
----------
Tue Feb  1 09:26:02 2022
batch 0, train loss = 0.14, mean loss = 0.14
Tue Feb  1 09:26:07 2022
batch 10, train loss = 0.47, mean loss = 0.38
Tue Feb  1 09:27:02 2022
batch 20, train loss = 0.13, mean loss = 0.37
Tue Feb  1 09:27:57 2022
batch 30, train loss = 0.48, mean loss = 0.38
Tue Feb  1 09:28:53 2022
batch 40, train loss = 0.11, mean loss = 0.38
Tue Feb  1 09:29:50 2022
batch 50, train loss = 0.47, mean loss = 0.39
Tue Feb  1 09:30:46 2022
batch 60, train loss = 0.08, mean loss = 0.38
Tue Feb  1 09:31:43 2022
train Loss: 0.39

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 09:32:36 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 09:33:32 2022
val Loss: 0.03

epoch 117 was done for 488.639545 seconds
Epoch 118/299
----------
Tue Feb  1 09:34:10 2022
batch 0, train loss = 0.19, mean loss = 0.19
Tue Feb  1 09:34:16 2022
batch 10, train loss = 0.50, mean loss = 0.35
Tue Feb  1 09:35:09 2022
batch 20, train loss = 0.16, mean loss = 0.38
Tue Feb  1 09:36:03 2022
batch 30, train loss = 0.49, mean loss = 0.37
Tue Feb  1 09:36:57 2022
batch 40, train loss = 0.15, mean loss = 0.38
Tue Feb  1 09:37:53 2022
batch 50, train loss = 0.46, mean loss = 0.38
Tue Feb  1 09:38:50 2022
batch 60, train loss = 0.10, mean loss = 0.38
Tue Feb  1 09:39:49 2022
train Loss: 0.38

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 09:40:42 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 09:41:37 2022
val Loss: 0.02

epoch 118 was done for 484.649256 seconds
Epoch 119/299
----------
Tue Feb  1 09:42:15 2022
batch 0, train loss = 0.20, mean loss = 0.20
Tue Feb  1 09:42:21 2022
batch 10, train loss = 0.46, mean loss = 0.37
Tue Feb  1 09:43:14 2022
batch 20, train loss = 0.19, mean loss = 0.40
Tue Feb  1 09:44:08 2022
batch 30, train loss = 0.48, mean loss = 0.39
Tue Feb  1 09:45:02 2022
batch 40, train loss = 0.16, mean loss = 0.39
Tue Feb  1 09:45:56 2022
batch 50, train loss = 0.44, mean loss = 0.39
Tue Feb  1 09:46:49 2022
batch 60, train loss = 0.14, mean loss = 0.38
Tue Feb  1 09:47:44 2022
train Loss: 0.38

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 09:48:34 2022
batch 10, val loss = 0.02, mean loss = 0.03
Tue Feb  1 09:49:29 2022
val Loss: 0.03

epoch 119 was done for 478.039934 seconds
Epoch 120/299
----------
Tue Feb  1 09:50:13 2022
batch 0, train loss = 0.17, mean loss = 0.17
Tue Feb  1 09:50:20 2022
batch 10, train loss = 0.38, mean loss = 0.38
Tue Feb  1 09:51:29 2022
batch 20, train loss = 0.20, mean loss = 0.38
Tue Feb  1 09:52:50 2022
batch 30, train loss = 0.40, mean loss = 0.39
Tue Feb  1 09:54:18 2022
batch 40, train loss = 0.21, mean loss = 0.39
Tue Feb  1 09:55:44 2022
batch 50, train loss = 0.40, mean loss = 0.40
Tue Feb  1 09:57:15 2022
batch 60, train loss = 0.20, mean loss = 0.39
Tue Feb  1 09:58:46 2022
train Loss: 0.39

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 09:59:55 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 10:01:08 2022
val Loss: 0.03

epoch 120 was done for 706.243073 seconds
Epoch 121/299
----------
Tue Feb  1 10:01:59 2022
batch 0, train loss = 0.11, mean loss = 0.11
Tue Feb  1 10:02:06 2022
batch 10, train loss = 0.39, mean loss = 0.39
Tue Feb  1 10:03:27 2022
batch 20, train loss = 0.11, mean loss = 0.37
Tue Feb  1 10:04:51 2022
batch 30, train loss = 0.42, mean loss = 0.38
Tue Feb  1 10:06:32 2022
batch 40, train loss = 0.13, mean loss = 0.38
Tue Feb  1 10:08:11 2022
batch 50, train loss = 0.44, mean loss = 0.38
Tue Feb  1 10:09:42 2022
batch 60, train loss = 0.14, mean loss = 0.38
Tue Feb  1 10:11:07 2022
train Loss: 0.38

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 10:12:35 2022
batch 10, val loss = 0.03, mean loss = 0.03
Tue Feb  1 10:14:06 2022
val Loss: 0.03

epoch 121 was done for 795.196015 seconds
Epoch 122/299
----------
Tue Feb  1 10:15:15 2022
batch 0, train loss = 0.12, mean loss = 0.12
Tue Feb  1 10:15:22 2022
batch 10, train loss = 0.38, mean loss = 0.37
Tue Feb  1 10:16:57 2022
batch 20, train loss = 0.12, mean loss = 0.35
Tue Feb  1 10:18:28 2022
batch 30, train loss = 0.39, mean loss = 0.37
Tue Feb  1 10:19:48 2022
batch 40, train loss = 0.14, mean loss = 0.36
Tue Feb  1 10:21:02 2022
batch 50, train loss = 0.37, mean loss = 0.37
Tue Feb  1 10:22:18 2022
batch 60, train loss = 0.13, mean loss = 0.36
Tue Feb  1 10:23:41 2022
train Loss: 0.36

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 10:25:02 2022
batch 10, val loss = 0.03, mean loss = 0.02
Tue Feb  1 10:26:21 2022
val Loss: 0.02

epoch 122 was done for 724.931550 seconds
Epoch 123/299
----------
Tue Feb  1 10:27:19 2022
batch 0, train loss = 0.08, mean loss = 0.08
Tue Feb  1 10:27:27 2022
batch 10, train loss = 0.44, mean loss = 0.37
Tue Feb  1 10:28:48 2022
batch 20, train loss = 0.07, mean loss = 0.35
Tue Feb  1 10:30:12 2022
batch 30, train loss = 0.43, mean loss = 0.37
Tue Feb  1 10:31:48 2022
batch 40, train loss = 0.08, mean loss = 0.35
Tue Feb  1 10:33:25 2022
batch 50, train loss = 0.38, mean loss = 0.37
Tue Feb  1 10:35:05 2022
batch 60, train loss = 0.09, mean loss = 0.35
Tue Feb  1 10:36:45 2022
train Loss: 0.36

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 10:38:12 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 10:39:25 2022
val Loss: 0.02

epoch 123 was done for 779.301418 seconds
Epoch 124/299
----------
Tue Feb  1 10:40:19 2022
batch 0, train loss = 0.09, mean loss = 0.09
Tue Feb  1 10:40:27 2022
batch 10, train loss = 0.43, mean loss = 0.33
Tue Feb  1 10:41:44 2022
batch 20, train loss = 0.06, mean loss = 0.33
Tue Feb  1 10:43:08 2022
batch 30, train loss = 0.48, mean loss = 0.35
Tue Feb  1 10:44:28 2022
batch 40, train loss = 0.05, mean loss = 0.34
Tue Feb  1 10:45:47 2022
batch 50, train loss = 0.42, mean loss = 0.35
Tue Feb  1 10:47:10 2022
batch 60, train loss = 0.05, mean loss = 0.34
Tue Feb  1 10:48:24 2022
train Loss: 0.34

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 10:49:43 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 10:51:02 2022
val Loss: 0.02

epoch 124 was done for 699.110795 seconds
Epoch 125/299
----------
Tue Feb  1 10:51:58 2022
batch 0, train loss = 0.11, mean loss = 0.11
Tue Feb  1 10:52:05 2022
batch 10, train loss = 0.40, mean loss = 0.31
Tue Feb  1 10:53:30 2022
batch 20, train loss = 0.08, mean loss = 0.32
Tue Feb  1 10:54:55 2022
batch 30, train loss = 0.44, mean loss = 0.33
Tue Feb  1 10:56:18 2022
batch 40, train loss = 0.07, mean loss = 0.33
Tue Feb  1 10:57:34 2022
batch 50, train loss = 0.45, mean loss = 0.34
Tue Feb  1 10:58:48 2022
batch 60, train loss = 0.05, mean loss = 0.33
Tue Feb  1 11:00:04 2022
train Loss: 0.33

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 11:01:20 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 11:02:40 2022
val Loss: 0.02

epoch 125 was done for 699.023747 seconds
Epoch 126/299
----------
Tue Feb  1 11:03:37 2022
batch 0, train loss = 0.12, mean loss = 0.12
Tue Feb  1 11:03:44 2022
batch 10, train loss = 0.36, mean loss = 0.32
Tue Feb  1 11:05:02 2022
batch 20, train loss = 0.10, mean loss = 0.32
Tue Feb  1 11:06:17 2022
batch 30, train loss = 0.37, mean loss = 0.32
Tue Feb  1 11:07:36 2022
batch 40, train loss = 0.09, mean loss = 0.32
Tue Feb  1 11:08:55 2022
batch 50, train loss = 0.41, mean loss = 0.33
Tue Feb  1 11:10:13 2022
batch 60, train loss = 0.07, mean loss = 0.33
Tue Feb  1 11:11:38 2022
train Loss: 0.33

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 11:12:58 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 11:14:21 2022
val Loss: 0.02

epoch 126 was done for 694.207203 seconds
Epoch 127/299
----------
Tue Feb  1 11:15:11 2022
batch 0, train loss = 0.08, mean loss = 0.08
Tue Feb  1 11:15:19 2022
batch 10, train loss = 0.35, mean loss = 0.32
Tue Feb  1 11:16:34 2022
batch 20, train loss = 0.07, mean loss = 0.32
Tue Feb  1 11:17:46 2022
batch 30, train loss = 0.37, mean loss = 0.32
Tue Feb  1 11:19:12 2022
batch 40, train loss = 0.06, mean loss = 0.32
Tue Feb  1 11:20:32 2022
batch 50, train loss = 0.40, mean loss = 0.32
Tue Feb  1 11:21:56 2022
batch 60, train loss = 0.05, mean loss = 0.32
Tue Feb  1 11:23:14 2022
train Loss: 0.32

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 11:24:20 2022
batch 10, val loss = 0.03, mean loss = 0.02
Tue Feb  1 11:25:31 2022
val Loss: 0.02

epoch 127 was done for 667.834753 seconds
Epoch 128/299
----------
Tue Feb  1 11:26:19 2022
batch 0, train loss = 0.06, mean loss = 0.06
Tue Feb  1 11:26:26 2022
batch 10, train loss = 0.36, mean loss = 0.31
Tue Feb  1 11:27:38 2022
batch 20, train loss = 0.07, mean loss = 0.31
Tue Feb  1 11:28:49 2022
batch 30, train loss = 0.38, mean loss = 0.32
Tue Feb  1 11:30:00 2022
batch 40, train loss = 0.07, mean loss = 0.32
Tue Feb  1 11:31:11 2022
batch 50, train loss = 0.38, mean loss = 0.32
Tue Feb  1 11:32:21 2022
batch 60, train loss = 0.06, mean loss = 0.32
Tue Feb  1 11:33:33 2022
train Loss: 0.32

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 11:34:43 2022
batch 10, val loss = 0.03, mean loss = 0.02
Tue Feb  1 11:36:17 2022
val Loss: 0.02

epoch 128 was done for 660.912784 seconds
Epoch 129/299
----------
Tue Feb  1 11:37:20 2022
batch 0, train loss = 0.05, mean loss = 0.05
Tue Feb  1 11:37:27 2022
batch 10, train loss = 0.37, mean loss = 0.32
Tue Feb  1 11:39:01 2022
batch 20, train loss = 0.06, mean loss = 0.31
Tue Feb  1 11:40:36 2022
batch 30, train loss = 0.39, mean loss = 0.32
Tue Feb  1 11:42:07 2022
batch 40, train loss = 0.06, mean loss = 0.32
Tue Feb  1 11:43:25 2022
batch 50, train loss = 0.36, mean loss = 0.32
Tue Feb  1 11:44:38 2022
batch 60, train loss = 0.05, mean loss = 0.32
Tue Feb  1 11:46:04 2022
train Loss: 0.32

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 11:47:27 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 11:48:47 2022
val Loss: 0.02

epoch 129 was done for 738.802658 seconds
Epoch 130/299
----------
Tue Feb  1 11:49:39 2022
batch 0, train loss = 0.03, mean loss = 0.03
Tue Feb  1 11:49:46 2022
batch 10, train loss = 0.39, mean loss = 0.31
Tue Feb  1 11:50:58 2022
batch 20, train loss = 0.04, mean loss = 0.30
Tue Feb  1 11:52:08 2022
batch 30, train loss = 0.41, mean loss = 0.32
Tue Feb  1 11:53:20 2022
batch 40, train loss = 0.04, mean loss = 0.31
Tue Feb  1 11:54:31 2022
batch 50, train loss = 0.36, mean loss = 0.32
Tue Feb  1 11:55:41 2022
batch 60, train loss = 0.04, mean loss = 0.31
Tue Feb  1 11:56:52 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 11:57:57 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 11:59:08 2022
val Loss: 0.02

epoch 130 was done for 617.648018 seconds
Epoch 131/299
----------
Tue Feb  1 11:59:56 2022
batch 0, train loss = 0.04, mean loss = 0.04
Tue Feb  1 12:00:04 2022
batch 10, train loss = 0.39, mean loss = 0.30
Tue Feb  1 12:01:14 2022
batch 20, train loss = 0.04, mean loss = 0.30
Tue Feb  1 12:02:23 2022
batch 30, train loss = 0.42, mean loss = 0.31
Tue Feb  1 12:03:32 2022
batch 40, train loss = 0.03, mean loss = 0.31
Tue Feb  1 12:04:41 2022
batch 50, train loss = 0.37, mean loss = 0.32
Tue Feb  1 12:05:49 2022
batch 60, train loss = 0.04, mean loss = 0.31
Tue Feb  1 12:06:58 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 12:08:02 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 12:09:11 2022
val Loss: 0.02

epoch 131 was done for 601.611432 seconds
Epoch 132/299
----------
Tue Feb  1 12:09:58 2022
batch 0, train loss = 0.04, mean loss = 0.04
Tue Feb  1 12:10:05 2022
batch 10, train loss = 0.39, mean loss = 0.30
Tue Feb  1 12:11:24 2022
batch 20, train loss = 0.04, mean loss = 0.30
Tue Feb  1 12:12:31 2022
batch 30, train loss = 0.42, mean loss = 0.31
Tue Feb  1 12:13:37 2022
batch 40, train loss = 0.04, mean loss = 0.31
Tue Feb  1 12:14:43 2022
batch 50, train loss = 0.38, mean loss = 0.32
Tue Feb  1 12:15:50 2022
batch 60, train loss = 0.04, mean loss = 0.32
Tue Feb  1 12:16:57 2022
train Loss: 0.32

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 12:17:59 2022
batch 10, val loss = 0.03, mean loss = 0.02
Tue Feb  1 12:19:07 2022
val Loss: 0.02

epoch 132 was done for 595.148238 seconds
Epoch 133/299
----------
Tue Feb  1 12:19:53 2022
batch 0, train loss = 0.05, mean loss = 0.05
Tue Feb  1 12:20:00 2022
batch 10, train loss = 0.38, mean loss = 0.30
Tue Feb  1 12:21:06 2022
batch 20, train loss = 0.05, mean loss = 0.30
Tue Feb  1 12:22:13 2022
batch 30, train loss = 0.42, mean loss = 0.31
Tue Feb  1 12:23:20 2022
batch 40, train loss = 0.05, mean loss = 0.32
Tue Feb  1 12:24:30 2022
batch 50, train loss = 0.39, mean loss = 0.33
Tue Feb  1 12:25:39 2022
batch 60, train loss = 0.05, mean loss = 0.32
Tue Feb  1 12:26:46 2022
train Loss: 0.32

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 12:27:49 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 12:28:56 2022
val Loss: 0.02

epoch 133 was done for 589.002296 seconds
Epoch 134/299
----------
Tue Feb  1 12:29:42 2022
batch 0, train loss = 0.05, mean loss = 0.05
Tue Feb  1 12:29:49 2022
batch 10, train loss = 0.38, mean loss = 0.31
Tue Feb  1 12:30:55 2022
batch 20, train loss = 0.05, mean loss = 0.31
Tue Feb  1 12:32:02 2022
batch 30, train loss = 0.41, mean loss = 0.32
Tue Feb  1 12:33:08 2022
batch 40, train loss = 0.05, mean loss = 0.32
Tue Feb  1 12:34:15 2022
batch 50, train loss = 0.41, mean loss = 0.33
Tue Feb  1 12:35:23 2022
batch 60, train loss = 0.05, mean loss = 0.32
Tue Feb  1 12:36:30 2022
train Loss: 0.33

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 12:37:31 2022
batch 10, val loss = 0.02, mean loss = 0.03
Tue Feb  1 12:38:38 2022
val Loss: 0.03

epoch 134 was done for 581.545083 seconds
Epoch 135/299
----------
Tue Feb  1 12:39:24 2022
batch 0, train loss = 0.06, mean loss = 0.06
Tue Feb  1 12:39:30 2022
batch 10, train loss = 0.38, mean loss = 0.31
Tue Feb  1 12:40:35 2022
batch 20, train loss = 0.06, mean loss = 0.31
Tue Feb  1 12:41:39 2022
batch 30, train loss = 0.41, mean loss = 0.32
Tue Feb  1 12:42:43 2022
batch 40, train loss = 0.06, mean loss = 0.32
Tue Feb  1 12:43:48 2022
batch 50, train loss = 0.43, mean loss = 0.33
Tue Feb  1 12:44:53 2022
batch 60, train loss = 0.06, mean loss = 0.33
Tue Feb  1 12:45:58 2022
train Loss: 0.33

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 12:46:59 2022
batch 10, val loss = 0.03, mean loss = 0.03
Tue Feb  1 12:48:05 2022
val Loss: 0.03

epoch 135 was done for 566.652223 seconds
Epoch 136/299
----------
Tue Feb  1 12:48:50 2022
batch 0, train loss = 0.07, mean loss = 0.07
Tue Feb  1 12:48:57 2022
batch 10, train loss = 0.37, mean loss = 0.32
Tue Feb  1 12:50:03 2022
batch 20, train loss = 0.07, mean loss = 0.32
Tue Feb  1 12:51:09 2022
batch 30, train loss = 0.41, mean loss = 0.32
Tue Feb  1 12:52:14 2022
batch 40, train loss = 0.08, mean loss = 0.33
Tue Feb  1 12:53:20 2022
batch 50, train loss = 0.43, mean loss = 0.34
Tue Feb  1 12:54:24 2022
batch 60, train loss = 0.07, mean loss = 0.33
Tue Feb  1 12:55:27 2022
train Loss: 0.34

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 12:56:26 2022
batch 10, val loss = 0.02, mean loss = 0.03
Tue Feb  1 12:57:28 2022
val Loss: 0.03

epoch 136 was done for 560.904685 seconds
Epoch 137/299
----------
Tue Feb  1 12:58:11 2022
batch 0, train loss = 0.08, mean loss = 0.08
Tue Feb  1 12:58:17 2022
batch 10, train loss = 0.37, mean loss = 0.33
Tue Feb  1 12:59:17 2022
batch 20, train loss = 0.08, mean loss = 0.32
Tue Feb  1 13:00:16 2022
batch 30, train loss = 0.40, mean loss = 0.33
Tue Feb  1 13:01:14 2022
batch 40, train loss = 0.09, mean loss = 0.33
Tue Feb  1 13:02:12 2022
batch 50, train loss = 0.41, mean loss = 0.34
Tue Feb  1 13:03:10 2022
batch 60, train loss = 0.09, mean loss = 0.34
Tue Feb  1 13:04:10 2022
train Loss: 0.34

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 13:05:08 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 13:06:10 2022
val Loss: 0.02

epoch 137 was done for 523.532819 seconds
Epoch 138/299
----------
Tue Feb  1 13:06:55 2022
batch 0, train loss = 0.07, mean loss = 0.07
Tue Feb  1 13:07:01 2022
batch 10, train loss = 0.39, mean loss = 0.34
Tue Feb  1 13:08:07 2022
batch 20, train loss = 0.06, mean loss = 0.33
Tue Feb  1 13:09:14 2022
batch 30, train loss = 0.41, mean loss = 0.34
Tue Feb  1 13:10:22 2022
batch 40, train loss = 0.07, mean loss = 0.33
Tue Feb  1 13:11:29 2022
batch 50, train loss = 0.40, mean loss = 0.34
Tue Feb  1 13:12:35 2022
batch 60, train loss = 0.07, mean loss = 0.34
Tue Feb  1 13:13:43 2022
train Loss: 0.34

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 13:14:47 2022
batch 10, val loss = 0.03, mean loss = 0.02
Tue Feb  1 13:15:54 2022
val Loss: 0.02

epoch 138 was done for 584.986304 seconds
Epoch 139/299
----------
Tue Feb  1 13:16:40 2022
batch 0, train loss = 0.07, mean loss = 0.07
Tue Feb  1 13:16:47 2022
batch 10, train loss = 0.41, mean loss = 0.34
Tue Feb  1 13:17:56 2022
batch 20, train loss = 0.06, mean loss = 0.33
Tue Feb  1 13:19:04 2022
batch 30, train loss = 0.42, mean loss = 0.34
Tue Feb  1 13:20:12 2022
batch 40, train loss = 0.06, mean loss = 0.33
Tue Feb  1 13:21:20 2022
batch 50, train loss = 0.40, mean loss = 0.34
Tue Feb  1 13:22:27 2022
batch 60, train loss = 0.05, mean loss = 0.33
Tue Feb  1 13:23:35 2022
train Loss: 0.34

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 13:24:38 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 13:25:46 2022
val Loss: 0.02

epoch 139 was done for 591.835720 seconds
Epoch 140/299
----------
Tue Feb  1 13:26:32 2022
batch 0, train loss = 0.08, mean loss = 0.08
Tue Feb  1 13:26:38 2022
batch 10, train loss = 0.40, mean loss = 0.33
Tue Feb  1 13:27:44 2022
batch 20, train loss = 0.07, mean loss = 0.33
Tue Feb  1 13:28:51 2022
batch 30, train loss = 0.41, mean loss = 0.33
Tue Feb  1 13:29:58 2022
batch 40, train loss = 0.07, mean loss = 0.33
Tue Feb  1 13:31:05 2022
batch 50, train loss = 0.40, mean loss = 0.33
Tue Feb  1 13:32:13 2022
batch 60, train loss = 0.05, mean loss = 0.33
Tue Feb  1 13:33:20 2022
train Loss: 0.33

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 13:34:24 2022
batch 10, val loss = 0.03, mean loss = 0.02
Tue Feb  1 13:35:32 2022
val Loss: 0.02

epoch 140 was done for 586.363797 seconds
Epoch 141/299
----------
Tue Feb  1 13:36:18 2022
batch 0, train loss = 0.08, mean loss = 0.08
Tue Feb  1 13:36:25 2022
batch 10, train loss = 0.39, mean loss = 0.33
Tue Feb  1 13:37:32 2022
batch 20, train loss = 0.07, mean loss = 0.33
Tue Feb  1 13:38:40 2022
batch 30, train loss = 0.39, mean loss = 0.33
Tue Feb  1 13:39:46 2022
batch 40, train loss = 0.07, mean loss = 0.33
Tue Feb  1 13:40:51 2022
batch 50, train loss = 0.39, mean loss = 0.33
Tue Feb  1 13:41:53 2022
batch 60, train loss = 0.06, mean loss = 0.32
Tue Feb  1 13:42:53 2022
train Loss: 0.33

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 13:43:47 2022
batch 10, val loss = 0.03, mean loss = 0.02
Tue Feb  1 13:44:43 2022
val Loss: 0.02

epoch 141 was done for 544.147824 seconds
Epoch 142/299
----------
Tue Feb  1 13:45:22 2022
batch 0, train loss = 0.06, mean loss = 0.06
Tue Feb  1 13:45:28 2022
batch 10, train loss = 0.38, mean loss = 0.32
Tue Feb  1 13:46:24 2022
batch 20, train loss = 0.07, mean loss = 0.33
Tue Feb  1 13:47:21 2022
batch 30, train loss = 0.38, mean loss = 0.33
Tue Feb  1 13:48:18 2022
batch 40, train loss = 0.07, mean loss = 0.33
Tue Feb  1 13:49:16 2022
batch 50, train loss = 0.38, mean loss = 0.33
Tue Feb  1 13:50:15 2022
batch 60, train loss = 0.06, mean loss = 0.32
Tue Feb  1 13:51:14 2022
train Loss: 0.32

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 13:52:08 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 13:53:05 2022
val Loss: 0.02

epoch 142 was done for 501.654992 seconds
Epoch 143/299
----------
Tue Feb  1 13:53:44 2022
batch 0, train loss = 0.05, mean loss = 0.05
Tue Feb  1 13:53:49 2022
batch 10, train loss = 0.38, mean loss = 0.32
Tue Feb  1 13:54:43 2022
batch 20, train loss = 0.06, mean loss = 0.31
Tue Feb  1 13:55:37 2022
batch 30, train loss = 0.39, mean loss = 0.33
Tue Feb  1 13:56:30 2022
batch 40, train loss = 0.06, mean loss = 0.32
Tue Feb  1 13:57:24 2022
batch 50, train loss = 0.38, mean loss = 0.33
Tue Feb  1 13:58:17 2022
batch 60, train loss = 0.05, mean loss = 0.32
Tue Feb  1 13:59:11 2022
train Loss: 0.32

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 14:00:00 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 14:00:54 2022
val Loss: 0.02

epoch 143 was done for 467.928565 seconds
Epoch 144/299
----------
Tue Feb  1 14:01:32 2022
batch 0, train loss = 0.04, mean loss = 0.04
Tue Feb  1 14:01:37 2022
batch 10, train loss = 0.38, mean loss = 0.31
Tue Feb  1 14:02:31 2022
batch 20, train loss = 0.04, mean loss = 0.31
Tue Feb  1 14:03:25 2022
batch 30, train loss = 0.41, mean loss = 0.32
Tue Feb  1 14:04:18 2022
batch 40, train loss = 0.04, mean loss = 0.32
Tue Feb  1 14:05:12 2022
batch 50, train loss = 0.39, mean loss = 0.33
Tue Feb  1 14:06:05 2022
batch 60, train loss = 0.04, mean loss = 0.32
Tue Feb  1 14:06:59 2022
train Loss: 0.32

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 14:07:50 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 14:08:44 2022
val Loss: 0.02

epoch 144 was done for 469.509191 seconds
Epoch 145/299
----------
Tue Feb  1 14:09:21 2022
batch 0, train loss = 0.04, mean loss = 0.04
Tue Feb  1 14:09:27 2022
batch 10, train loss = 0.39, mean loss = 0.30
Tue Feb  1 14:10:21 2022
batch 20, train loss = 0.04, mean loss = 0.30
Tue Feb  1 14:11:16 2022
batch 30, train loss = 0.42, mean loss = 0.32
Tue Feb  1 14:12:10 2022
batch 40, train loss = 0.04, mean loss = 0.31
Tue Feb  1 14:13:05 2022
batch 50, train loss = 0.39, mean loss = 0.32
Tue Feb  1 14:14:01 2022
batch 60, train loss = 0.04, mean loss = 0.32
Tue Feb  1 14:14:56 2022
train Loss: 0.32

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 14:15:47 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 14:16:44 2022
val Loss: 0.02

epoch 145 was done for 481.404174 seconds
Epoch 146/299
----------
Tue Feb  1 14:17:23 2022
batch 0, train loss = 0.04, mean loss = 0.04
Tue Feb  1 14:17:28 2022
batch 10, train loss = 0.39, mean loss = 0.30
Tue Feb  1 14:18:21 2022
batch 20, train loss = 0.04, mean loss = 0.30
Tue Feb  1 14:19:16 2022
batch 30, train loss = 0.44, mean loss = 0.31
Tue Feb  1 14:20:13 2022
batch 40, train loss = 0.04, mean loss = 0.31
Tue Feb  1 14:21:12 2022
batch 50, train loss = 0.39, mean loss = 0.32
Tue Feb  1 14:22:15 2022
batch 60, train loss = 0.03, mean loss = 0.32
Tue Feb  1 14:23:17 2022
train Loss: 0.32

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 14:24:15 2022
batch 10, val loss = 0.03, mean loss = 0.02
Tue Feb  1 14:25:18 2022
val Loss: 0.02

epoch 146 was done for 517.347032 seconds
Epoch 147/299
----------
Tue Feb  1 14:26:00 2022
batch 0, train loss = 0.04, mean loss = 0.04
Tue Feb  1 14:26:06 2022
batch 10, train loss = 0.39, mean loss = 0.31
Tue Feb  1 14:27:07 2022
batch 20, train loss = 0.03, mean loss = 0.30
Tue Feb  1 14:28:06 2022
batch 30, train loss = 0.44, mean loss = 0.31
Tue Feb  1 14:29:04 2022
batch 40, train loss = 0.04, mean loss = 0.31
Tue Feb  1 14:30:01 2022
batch 50, train loss = 0.39, mean loss = 0.32
Tue Feb  1 14:30:56 2022
batch 60, train loss = 0.03, mean loss = 0.32
Tue Feb  1 14:31:51 2022
train Loss: 0.32

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 14:32:41 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 14:33:35 2022
val Loss: 0.02

epoch 147 was done for 494.789619 seconds
Epoch 148/299
----------
Tue Feb  1 14:34:15 2022
batch 0, train loss = 0.05, mean loss = 0.05
Tue Feb  1 14:34:20 2022
batch 10, train loss = 0.39, mean loss = 0.31
Tue Feb  1 14:35:13 2022
batch 20, train loss = 0.04, mean loss = 0.30
Tue Feb  1 14:36:06 2022
batch 30, train loss = 0.43, mean loss = 0.31
Tue Feb  1 14:36:59 2022
batch 40, train loss = 0.05, mean loss = 0.32
Tue Feb  1 14:37:51 2022
batch 50, train loss = 0.39, mean loss = 0.32
Tue Feb  1 14:38:44 2022
batch 60, train loss = 0.04, mean loss = 0.32
Tue Feb  1 14:39:37 2022
train Loss: 0.32

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 14:40:27 2022
batch 10, val loss = 0.03, mean loss = 0.02
Tue Feb  1 14:41:22 2022
val Loss: 0.02

epoch 148 was done for 468.435068 seconds
Epoch 149/299
----------
Tue Feb  1 14:42:03 2022
batch 0, train loss = 0.05, mean loss = 0.05
Tue Feb  1 14:42:08 2022
batch 10, train loss = 0.37, mean loss = 0.31
Tue Feb  1 14:43:02 2022
batch 20, train loss = 0.05, mean loss = 0.31
Tue Feb  1 14:43:55 2022
batch 30, train loss = 0.41, mean loss = 0.32
Tue Feb  1 14:44:49 2022
batch 40, train loss = 0.06, mean loss = 0.32
Tue Feb  1 14:45:42 2022
batch 50, train loss = 0.38, mean loss = 0.32
Tue Feb  1 14:46:35 2022
batch 60, train loss = 0.06, mean loss = 0.32
Tue Feb  1 14:47:28 2022
train Loss: 0.32

batch 0, val loss = 0.03, mean loss = 0.03
Tue Feb  1 14:48:19 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 14:49:13 2022
val Loss: 0.02

epoch 149 was done for 466.943925 seconds
Epoch 150/299
----------
Tue Feb  1 14:49:50 2022
batch 0, train loss = 0.05, mean loss = 0.05
Tue Feb  1 14:49:56 2022
batch 10, train loss = 0.37, mean loss = 0.32
Tue Feb  1 14:50:50 2022
batch 20, train loss = 0.05, mean loss = 0.31
Tue Feb  1 14:51:46 2022
batch 30, train loss = 0.38, mean loss = 0.32
Tue Feb  1 14:52:40 2022
batch 40, train loss = 0.06, mean loss = 0.32
Tue Feb  1 14:53:34 2022
batch 50, train loss = 0.38, mean loss = 0.33
Tue Feb  1 14:54:28 2022
batch 60, train loss = 0.06, mean loss = 0.32
Tue Feb  1 14:55:22 2022
train Loss: 0.32

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 14:56:13 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 14:57:07 2022
val Loss: 0.03

epoch 150 was done for 475.197155 seconds
Epoch 151/299
----------
Tue Feb  1 14:57:45 2022
batch 0, train loss = 0.04, mean loss = 0.04
Tue Feb  1 14:57:51 2022
batch 10, train loss = 0.38, mean loss = 0.32
Tue Feb  1 14:58:43 2022
batch 20, train loss = 0.04, mean loss = 0.32
Tue Feb  1 14:59:37 2022
batch 30, train loss = 0.38, mean loss = 0.33
Tue Feb  1 15:00:30 2022
batch 40, train loss = 0.05, mean loss = 0.32
Tue Feb  1 15:01:23 2022
batch 50, train loss = 0.39, mean loss = 0.33
Tue Feb  1 15:02:16 2022
batch 60, train loss = 0.05, mean loss = 0.32
Tue Feb  1 15:03:11 2022
train Loss: 0.33

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 15:04:01 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 15:04:54 2022
val Loss: 0.02

epoch 151 was done for 466.206483 seconds
Epoch 152/299
----------
Tue Feb  1 15:05:31 2022
batch 0, train loss = 0.05, mean loss = 0.05
Tue Feb  1 15:05:37 2022
batch 10, train loss = 0.38, mean loss = 0.31
Tue Feb  1 15:06:30 2022
batch 20, train loss = 0.05, mean loss = 0.31
Tue Feb  1 15:07:24 2022
batch 30, train loss = 0.39, mean loss = 0.32
Tue Feb  1 15:08:17 2022
batch 40, train loss = 0.04, mean loss = 0.32
Tue Feb  1 15:09:11 2022
batch 50, train loss = 0.41, mean loss = 0.33
Tue Feb  1 15:10:04 2022
batch 60, train loss = 0.04, mean loss = 0.32
Tue Feb  1 15:11:01 2022
train Loss: 0.32

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 15:11:51 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 15:12:45 2022
val Loss: 0.02

epoch 152 was done for 470.979345 seconds
Epoch 153/299
----------
Tue Feb  1 15:13:22 2022
batch 0, train loss = 0.06, mean loss = 0.06
Tue Feb  1 15:13:28 2022
batch 10, train loss = 0.38, mean loss = 0.30
Tue Feb  1 15:14:22 2022
batch 20, train loss = 0.05, mean loss = 0.31
Tue Feb  1 15:15:17 2022
batch 30, train loss = 0.40, mean loss = 0.32
Tue Feb  1 15:16:11 2022
batch 40, train loss = 0.05, mean loss = 0.32
Tue Feb  1 15:17:06 2022
batch 50, train loss = 0.42, mean loss = 0.33
Tue Feb  1 15:18:00 2022
batch 60, train loss = 0.04, mean loss = 0.32
Tue Feb  1 15:18:55 2022
train Loss: 0.32

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 15:19:46 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 15:20:41 2022
val Loss: 0.03

epoch 153 was done for 476.638450 seconds
Epoch 154/299
----------
Tue Feb  1 15:21:19 2022
batch 0, train loss = 0.06, mean loss = 0.06
Tue Feb  1 15:21:24 2022
batch 10, train loss = 0.37, mean loss = 0.30
Tue Feb  1 15:22:18 2022
batch 20, train loss = 0.05, mean loss = 0.30
Tue Feb  1 15:23:11 2022
batch 30, train loss = 0.42, mean loss = 0.32
Tue Feb  1 15:24:04 2022
batch 40, train loss = 0.05, mean loss = 0.32
Tue Feb  1 15:25:00 2022
batch 50, train loss = 0.43, mean loss = 0.33
Tue Feb  1 15:25:53 2022
batch 60, train loss = 0.04, mean loss = 0.32
Tue Feb  1 15:26:47 2022
train Loss: 0.32

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 15:27:37 2022
batch 10, val loss = 0.03, mean loss = 0.02
Tue Feb  1 15:28:31 2022
val Loss: 0.03

epoch 154 was done for 468.872684 seconds
Epoch 155/299
----------
Tue Feb  1 15:29:08 2022
batch 0, train loss = 0.06, mean loss = 0.06
Tue Feb  1 15:29:13 2022
batch 10, train loss = 0.37, mean loss = 0.30
Tue Feb  1 15:30:07 2022
batch 20, train loss = 0.05, mean loss = 0.30
Tue Feb  1 15:31:01 2022
batch 30, train loss = 0.42, mean loss = 0.31
Tue Feb  1 15:31:54 2022
batch 40, train loss = 0.05, mean loss = 0.31
Tue Feb  1 15:32:48 2022
batch 50, train loss = 0.42, mean loss = 0.32
Tue Feb  1 15:33:41 2022
batch 60, train loss = 0.04, mean loss = 0.32
Tue Feb  1 15:34:35 2022
train Loss: 0.32

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 15:35:25 2022
batch 10, val loss = 0.03, mean loss = 0.02
Tue Feb  1 15:36:19 2022
val Loss: 0.03

epoch 155 was done for 468.850045 seconds
Epoch 156/299
----------
Tue Feb  1 15:36:57 2022
batch 0, train loss = 0.05, mean loss = 0.05
Tue Feb  1 15:37:02 2022
batch 10, train loss = 0.37, mean loss = 0.30
Tue Feb  1 15:37:57 2022
batch 20, train loss = 0.05, mean loss = 0.30
Tue Feb  1 15:38:52 2022
batch 30, train loss = 0.41, mean loss = 0.31
Tue Feb  1 15:39:47 2022
batch 40, train loss = 0.05, mean loss = 0.31
Tue Feb  1 15:40:41 2022
batch 50, train loss = 0.40, mean loss = 0.32
Tue Feb  1 15:41:36 2022
batch 60, train loss = 0.05, mean loss = 0.32
Tue Feb  1 15:42:31 2022
train Loss: 0.32

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 15:43:22 2022
batch 10, val loss = 0.03, mean loss = 0.02
Tue Feb  1 15:44:18 2022
val Loss: 0.02

epoch 156 was done for 478.905559 seconds
Epoch 157/299
----------
Tue Feb  1 15:44:56 2022
batch 0, train loss = 0.04, mean loss = 0.04
Tue Feb  1 15:45:01 2022
batch 10, train loss = 0.37, mean loss = 0.32
Tue Feb  1 15:45:55 2022
batch 20, train loss = 0.04, mean loss = 0.30
Tue Feb  1 15:46:48 2022
batch 30, train loss = 0.40, mean loss = 0.31
Tue Feb  1 15:47:42 2022
batch 40, train loss = 0.05, mean loss = 0.31
Tue Feb  1 15:48:36 2022
batch 50, train loss = 0.38, mean loss = 0.32
Tue Feb  1 15:49:29 2022
batch 60, train loss = 0.05, mean loss = 0.31
Tue Feb  1 15:50:23 2022
train Loss: 0.32

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 15:51:13 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 15:52:07 2022
val Loss: 0.02

epoch 157 was done for 471.666497 seconds
Epoch 158/299
----------
Tue Feb  1 15:52:47 2022
batch 0, train loss = 0.03, mean loss = 0.03
Tue Feb  1 15:52:53 2022
batch 10, train loss = 0.38, mean loss = 0.32
Tue Feb  1 15:53:48 2022
batch 20, train loss = 0.03, mean loss = 0.31
Tue Feb  1 15:54:42 2022
batch 30, train loss = 0.40, mean loss = 0.32
Tue Feb  1 15:55:36 2022
batch 40, train loss = 0.03, mean loss = 0.31
Tue Feb  1 15:56:30 2022
batch 50, train loss = 0.38, mean loss = 0.32
Tue Feb  1 15:57:24 2022
batch 60, train loss = 0.03, mean loss = 0.31
Tue Feb  1 15:58:18 2022
train Loss: 0.32

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 15:59:08 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 16:00:03 2022
val Loss: 0.02

epoch 158 was done for 472.842161 seconds
Epoch 159/299
----------
Tue Feb  1 16:00:40 2022
batch 0, train loss = 0.03, mean loss = 0.03
Tue Feb  1 16:00:46 2022
batch 10, train loss = 0.40, mean loss = 0.32
Tue Feb  1 16:01:45 2022
batch 20, train loss = 0.03, mean loss = 0.31
Tue Feb  1 16:02:48 2022
batch 30, train loss = 0.41, mean loss = 0.32
Tue Feb  1 16:03:51 2022
batch 40, train loss = 0.03, mean loss = 0.31
Tue Feb  1 16:04:54 2022
batch 50, train loss = 0.40, mean loss = 0.32
Tue Feb  1 16:05:57 2022
batch 60, train loss = 0.02, mean loss = 0.31
Tue Feb  1 16:07:00 2022
train Loss: 0.32

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 16:07:58 2022
batch 10, val loss = 0.03, mean loss = 0.02
Tue Feb  1 16:09:00 2022
val Loss: 0.02

epoch 159 was done for 541.880019 seconds
Epoch 160/299
----------
Tue Feb  1 16:09:42 2022
batch 0, train loss = 0.05, mean loss = 0.05
Tue Feb  1 16:09:48 2022
batch 10, train loss = 0.39, mean loss = 0.30
Tue Feb  1 16:10:46 2022
batch 20, train loss = 0.05, mean loss = 0.31
Tue Feb  1 16:11:42 2022
batch 30, train loss = 0.39, mean loss = 0.31
Tue Feb  1 16:12:39 2022
batch 40, train loss = 0.05, mean loss = 0.31
Tue Feb  1 16:13:36 2022
batch 50, train loss = 0.39, mean loss = 0.32
Tue Feb  1 16:14:32 2022
batch 60, train loss = 0.04, mean loss = 0.31
Tue Feb  1 16:15:29 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 16:16:23 2022
batch 10, val loss = 0.03, mean loss = 0.02
Tue Feb  1 16:17:20 2022
val Loss: 0.02

epoch 160 was done for 496.847906 seconds
Epoch 161/299
----------
Tue Feb  1 16:17:59 2022
batch 0, train loss = 0.05, mean loss = 0.05
Tue Feb  1 16:18:05 2022
batch 10, train loss = 0.37, mean loss = 0.31
Tue Feb  1 16:19:01 2022
batch 20, train loss = 0.05, mean loss = 0.31
Tue Feb  1 16:19:58 2022
batch 30, train loss = 0.38, mean loss = 0.32
Tue Feb  1 16:20:55 2022
batch 40, train loss = 0.05, mean loss = 0.32
Tue Feb  1 16:21:58 2022
batch 50, train loss = 0.37, mean loss = 0.32
Tue Feb  1 16:22:55 2022
batch 60, train loss = 0.04, mean loss = 0.31
Tue Feb  1 16:23:53 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 16:24:47 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 16:25:45 2022
val Loss: 0.02

epoch 161 was done for 506.058699 seconds
Epoch 162/299
----------
Tue Feb  1 16:26:25 2022
batch 0, train loss = 0.03, mean loss = 0.03
Tue Feb  1 16:26:31 2022
batch 10, train loss = 0.37, mean loss = 0.31
Tue Feb  1 16:27:29 2022
batch 20, train loss = 0.03, mean loss = 0.30
Tue Feb  1 16:28:29 2022
batch 30, train loss = 0.39, mean loss = 0.32
Tue Feb  1 16:29:29 2022
batch 40, train loss = 0.03, mean loss = 0.31
Tue Feb  1 16:30:32 2022
batch 50, train loss = 0.37, mean loss = 0.32
Tue Feb  1 16:31:34 2022
batch 60, train loss = 0.03, mean loss = 0.31
Tue Feb  1 16:32:36 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 16:33:34 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 16:34:42 2022
val Loss: 0.02

epoch 162 was done for 540.325160 seconds
Epoch 163/299
----------
Tue Feb  1 16:35:25 2022
batch 0, train loss = 0.02, mean loss = 0.02
Tue Feb  1 16:35:32 2022
batch 10, train loss = 0.38, mean loss = 0.30
Tue Feb  1 16:36:33 2022
batch 20, train loss = 0.02, mean loss = 0.30
Tue Feb  1 16:37:36 2022
batch 30, train loss = 0.43, mean loss = 0.31
Tue Feb  1 16:38:40 2022
batch 40, train loss = 0.02, mean loss = 0.31
Tue Feb  1 16:39:47 2022
batch 50, train loss = 0.39, mean loss = 0.32
Tue Feb  1 16:40:55 2022
batch 60, train loss = 0.02, mean loss = 0.31
Tue Feb  1 16:42:05 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Tue Feb  1 16:43:09 2022
batch 10, val loss = 0.02, mean loss = 0.02
Tue Feb  1 16:44:19 2022
val Loss: 0.02

epoch 163 was done for 581.642096 seconds
Epoch 164/299
----------
Tue Feb  1 16:45:07 2022
batch 0, train loss = 0.04, mean loss = 0.04
Tue Feb  1 16:45:14 2022
