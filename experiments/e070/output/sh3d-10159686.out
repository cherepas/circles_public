The activation script must be sourced, otherwise the virtual environment will not work.
Setting vars
iscuda= True
opt.wandb =  
93
file to frame csv ../../csv/598frame.csv
iscuda= True
opt.wandb =  
93
file to frame csv ../../csv/598frame.csv
iscuda= True
opt.wandb =  
93
file to frame csv ../../csv/598frame.csv
iscuda= True
PyTorch Version:  1.8.1
Torchvision Version:  0.9.0a0
opt:
 Namespace(ampl=441, aug_gt='orient', batch_output=2, bs=20, cencrop=700, center=False, chidden_dim=[96, 128, 256, 256, 256], classicnorm=False, cmscrop=0, conTrain='', criterion='L2', csvname='598csv9', datapath='/p/project/delia-mp/cherepashkin1/phenoseed/', dfname='598frame', downsample=1, epoch=300, expdescr='', expnum='e068', feature_extract=False, framelim=6000, gradient_predivide_factor=1.0, gttype='single_file', haf=True, hidden_dim=[9], inputt='img', kernel_sizes=[7, 3, 3, 3, 3, 3], lb='orient', loadh5=False, localexp='', lr=5e-05, machine='jureca', maintain=False, maintain_line=False, man_dist=False, measure_time=False, merging='batch', minmax=False, minmax3dimage=False, minmax_f=True, minmax_fn='', model_name='', netname=['cnet'], ngpu=4, noise_input=False, noise_output=False, normalize=False, num_input_images=3, num_sam_points=500, num_workers=0, outputt='orient', parallel='horovod', pin_memory=False, print_minibatch=10, pscale=100, rand_angle=False, rescale=500, rot_dirs=False, rotate_output=False, save_output=True, single_folder=False, specie='598', standardize=255, steplr=[1000.0, 1.0], transappendix='_image', ufmodel=100000, updateFraction=0.25, use_adasum=False, use_cuda=True, use_existing_csv=True, use_pretrained=False, use_sep_csv=True, view_sep=False, wandb='', weight_decay=0, zero_angle=True)
sys.argv:
 ['../../main.py', '-datapath', '/p/project/delia-mp/cherepashkin1/phenoseed/', '-epoch', '300', '-bs', '20', '-num_input_images', '3', '-framelim', '6000', '-criterion', 'L2', '-localexp', '', '-lr', '5e-5', '-expnum', 'e068', '-hidden_dim', '9', '-inputt', 'img', '-outputt', 'orient', '-lb', 'orient', '-no_loadh5', '-minmax_fn', '', '-parallel', 'horovod', '-machine', 'jureca', '-merging', 'batch', '-aug_gt', 'orient', '-updateFraction', '0.25', '-steplr', '1000', '1', '-print_minibatch', '10', '-dfname', '598frame']
seed =  0
path were main.py is located= ../../
opt.wandb =  
93
file to frame csv ../../csv/598frame.csv
lframe's length after laoding =  5283
lframe's length after laoding =  5283
lframe's length after laoding =  5283
lframe's length after laoding =  5283
lframe len after excluding all exceptions= 5200
lframe len after excluding all exceptions= 5200
lframe len after excluding all exceptions= 5200
lframe len after excluding all exceptions= 5200
len train =  4160
len train =  4160
len train =  4160
len train =  4160
train consists of 208 full batches with 20 tensors with 3 views
val consists of 51 full batches with 20 tensors with 3 views
Epoch 0/299
----------
Mon Jan 31 00:15:03 2022
batch 0, train loss = 0.52, mean loss = 0.52
Mon Jan 31 00:15:14 2022
batch 10, train loss = 16.81, mean loss = 35.04
Mon Jan 31 00:16:27 2022
batch 20, train loss = 16.92, mean loss = 28.15
Mon Jan 31 00:17:39 2022
batch 30, train loss = 8.68, mean loss = 20.76
Mon Jan 31 00:18:50 2022
batch 40, train loss = 1.58, mean loss = 16.85
Mon Jan 31 00:20:02 2022
batch 50, train loss = 3.17, mean loss = 14.24
Mon Jan 31 00:21:13 2022
train Loss: 14.03

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:21:27 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:22:39 2022
val Loss: 0.02

epoch 0 was done for 473.190503 seconds
Epoch 1/299
----------
Mon Jan 31 00:22:56 2022
batch 0, train loss = 2.86, mean loss = 2.86
Mon Jan 31 00:23:03 2022
batch 10, train loss = 2.69, mean loss = 1.72
Mon Jan 31 00:24:15 2022
batch 20, train loss = 1.38, mean loss = 1.89
Mon Jan 31 00:25:26 2022
batch 30, train loss = 2.13, mean loss = 1.72
Mon Jan 31 00:26:37 2022
batch 40, train loss = 0.72, mean loss = 1.62
Mon Jan 31 00:27:49 2022
batch 50, train loss = 1.38, mean loss = 1.52
Mon Jan 31 00:29:00 2022
train Loss: 1.52

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:29:15 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 00:30:27 2022
val Loss: 0.03

epoch 1 was done for 467.893198 seconds
Epoch 2/299
----------
Mon Jan 31 00:30:44 2022
batch 0, train loss = 1.35, mean loss = 1.35
Mon Jan 31 00:30:51 2022
batch 10, train loss = 0.80, mean loss = 0.74
Mon Jan 31 00:32:03 2022
batch 20, train loss = 0.75, mean loss = 0.79
Mon Jan 31 00:33:15 2022
batch 30, train loss = 0.61, mean loss = 0.70
Mon Jan 31 00:34:27 2022
batch 40, train loss = 0.53, mean loss = 0.68
Mon Jan 31 00:35:39 2022
batch 50, train loss = 0.56, mean loss = 0.64
Mon Jan 31 00:36:50 2022
train Loss: 0.64

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:37:05 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:38:18 2022
val Loss: 0.02

epoch 2 was done for 470.919524 seconds
Epoch 3/299
----------
Mon Jan 31 00:38:35 2022
batch 0, train loss = 0.48, mean loss = 0.48
Mon Jan 31 00:38:42 2022
batch 10, train loss = 0.38, mean loss = 0.46
Mon Jan 31 00:39:53 2022
batch 20, train loss = 0.46, mean loss = 0.45
Mon Jan 31 00:41:04 2022
batch 30, train loss = 0.37, mean loss = 0.43
Mon Jan 31 00:42:15 2022
batch 40, train loss = 0.36, mean loss = 0.43
Mon Jan 31 00:43:26 2022
batch 50, train loss = 0.37, mean loss = 0.42
Mon Jan 31 00:44:37 2022
train Loss: 0.41

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:44:52 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:46:03 2022
val Loss: 0.02

epoch 3 was done for 465.493757 seconds
Epoch 4/299
----------
Mon Jan 31 00:46:20 2022
batch 0, train loss = 0.29, mean loss = 0.29
Mon Jan 31 00:46:27 2022
batch 10, train loss = 0.33, mean loss = 0.36
Mon Jan 31 00:47:39 2022
batch 20, train loss = 0.34, mean loss = 0.36
Mon Jan 31 00:48:51 2022
batch 30, train loss = 0.30, mean loss = 0.36
Mon Jan 31 00:50:02 2022
batch 40, train loss = 0.30, mean loss = 0.35
Mon Jan 31 00:51:14 2022
batch 50, train loss = 0.35, mean loss = 0.35
Mon Jan 31 00:52:25 2022
train Loss: 0.35

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:52:40 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:53:52 2022
val Loss: 0.02

epoch 4 was done for 469.179595 seconds
Epoch 5/299
----------
Mon Jan 31 00:54:10 2022
batch 0, train loss = 0.28, mean loss = 0.28
Mon Jan 31 00:54:17 2022
batch 10, train loss = 0.32, mean loss = 0.33
Mon Jan 31 00:55:29 2022
batch 20, train loss = 0.30, mean loss = 0.33
Mon Jan 31 00:56:41 2022
batch 30, train loss = 0.31, mean loss = 0.33
Mon Jan 31 00:57:53 2022
batch 40, train loss = 0.28, mean loss = 0.33
Mon Jan 31 00:59:05 2022
batch 50, train loss = 0.34, mean loss = 0.33
Mon Jan 31 01:00:18 2022
train Loss: 0.33

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:00:32 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:01:46 2022
val Loss: 0.02

epoch 5 was done for 473.526908 seconds
Epoch 6/299
----------
Mon Jan 31 01:02:03 2022
batch 0, train loss = 0.27, mean loss = 0.27
Mon Jan 31 01:02:10 2022
batch 10, train loss = 0.32, mean loss = 0.32
Mon Jan 31 01:03:22 2022
batch 20, train loss = 0.28, mean loss = 0.32
Mon Jan 31 01:04:34 2022
batch 30, train loss = 0.29, mean loss = 0.32
Mon Jan 31 01:05:46 2022
batch 40, train loss = 0.26, mean loss = 0.32
Mon Jan 31 01:06:58 2022
batch 50, train loss = 0.33, mean loss = 0.32
Mon Jan 31 01:08:10 2022
train Loss: 0.32

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:08:24 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:09:37 2022
val Loss: 0.02

epoch 6 was done for 471.605510 seconds
Epoch 7/299
----------
Mon Jan 31 01:09:55 2022
batch 0, train loss = 0.26, mean loss = 0.26
Mon Jan 31 01:10:02 2022
batch 10, train loss = 0.31, mean loss = 0.31
Mon Jan 31 01:11:14 2022
batch 20, train loss = 0.27, mean loss = 0.31
Mon Jan 31 01:12:26 2022
batch 30, train loss = 0.29, mean loss = 0.32
Mon Jan 31 01:13:38 2022
batch 40, train loss = 0.25, mean loss = 0.32
Mon Jan 31 01:14:50 2022
batch 50, train loss = 0.33, mean loss = 0.32
Mon Jan 31 01:16:02 2022
train Loss: 0.32

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:16:16 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:17:29 2022
val Loss: 0.02

epoch 7 was done for 471.726781 seconds
Epoch 8/299
----------
Mon Jan 31 01:17:46 2022
batch 0, train loss = 0.25, mean loss = 0.25
Mon Jan 31 01:17:54 2022
batch 10, train loss = 0.31, mean loss = 0.31
Mon Jan 31 01:19:05 2022
batch 20, train loss = 0.26, mean loss = 0.31
Mon Jan 31 01:20:17 2022
batch 30, train loss = 0.30, mean loss = 0.31
Mon Jan 31 01:21:29 2022
batch 40, train loss = 0.24, mean loss = 0.31
Mon Jan 31 01:22:42 2022
batch 50, train loss = 0.32, mean loss = 0.31
Mon Jan 31 01:23:55 2022
train Loss: 0.32

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:24:10 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:25:23 2022
val Loss: 0.02

epoch 8 was done for 474.373648 seconds
Epoch 9/299
----------
Mon Jan 31 01:25:41 2022
batch 0, train loss = 0.24, mean loss = 0.24
Mon Jan 31 01:25:48 2022
batch 10, train loss = 0.31, mean loss = 0.31
Mon Jan 31 01:27:00 2022
batch 20, train loss = 0.26, mean loss = 0.31
Mon Jan 31 01:28:11 2022
batch 30, train loss = 0.30, mean loss = 0.31
Mon Jan 31 01:29:23 2022
batch 40, train loss = 0.23, mean loss = 0.31
Mon Jan 31 01:30:35 2022
batch 50, train loss = 0.32, mean loss = 0.31
Mon Jan 31 01:31:46 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:32:01 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:33:13 2022
val Loss: 0.02

epoch 9 was done for 469.461029 seconds
Epoch 10/299
----------
Mon Jan 31 01:33:30 2022
batch 0, train loss = 0.24, mean loss = 0.24
Mon Jan 31 01:33:37 2022
batch 10, train loss = 0.31, mean loss = 0.31
Mon Jan 31 01:34:49 2022
batch 20, train loss = 0.25, mean loss = 0.31
Mon Jan 31 01:36:01 2022
batch 30, train loss = 0.30, mean loss = 0.31
Mon Jan 31 01:37:13 2022
batch 40, train loss = 0.23, mean loss = 0.31
Mon Jan 31 01:38:25 2022
batch 50, train loss = 0.32, mean loss = 0.31
Mon Jan 31 01:39:36 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:39:51 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:41:03 2022
val Loss: 0.02

epoch 10 was done for 469.604234 seconds
Epoch 11/299
----------
Mon Jan 31 01:41:20 2022
batch 0, train loss = 0.23, mean loss = 0.23
Mon Jan 31 01:41:27 2022
batch 10, train loss = 0.30, mean loss = 0.30
Mon Jan 31 01:42:38 2022
batch 20, train loss = 0.24, mean loss = 0.30
Mon Jan 31 01:43:50 2022
batch 30, train loss = 0.30, mean loss = 0.31
Mon Jan 31 01:45:02 2022
batch 40, train loss = 0.22, mean loss = 0.31
Mon Jan 31 01:46:14 2022
batch 50, train loss = 0.32, mean loss = 0.31
Mon Jan 31 01:47:26 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:47:41 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:48:54 2022
val Loss: 0.02

epoch 11 was done for 471.591439 seconds
Epoch 12/299
----------
Mon Jan 31 01:49:11 2022
batch 0, train loss = 0.23, mean loss = 0.23
Mon Jan 31 01:49:19 2022
batch 10, train loss = 0.30, mean loss = 0.30
Mon Jan 31 01:50:30 2022
batch 20, train loss = 0.24, mean loss = 0.30
Mon Jan 31 01:51:42 2022
batch 30, train loss = 0.30, mean loss = 0.30
Mon Jan 31 01:52:54 2022
batch 40, train loss = 0.22, mean loss = 0.30
Mon Jan 31 01:54:06 2022
batch 50, train loss = 0.32, mean loss = 0.31
Mon Jan 31 01:55:18 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:55:32 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:56:45 2022
val Loss: 0.02

epoch 12 was done for 470.586201 seconds
Epoch 13/299
----------
Mon Jan 31 01:57:02 2022
batch 0, train loss = 0.23, mean loss = 0.23
Mon Jan 31 01:57:09 2022
batch 10, train loss = 0.30, mean loss = 0.30
Mon Jan 31 01:58:21 2022
batch 20, train loss = 0.23, mean loss = 0.30
Mon Jan 31 01:59:33 2022
batch 30, train loss = 0.30, mean loss = 0.30
Mon Jan 31 02:00:44 2022
batch 40, train loss = 0.21, mean loss = 0.30
Mon Jan 31 02:01:56 2022
batch 50, train loss = 0.32, mean loss = 0.31
Mon Jan 31 02:03:08 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:03:22 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:04:34 2022
val Loss: 0.02

epoch 13 was done for 469.530498 seconds
Epoch 14/299
----------
Mon Jan 31 02:04:52 2022
batch 0, train loss = 0.22, mean loss = 0.22
Mon Jan 31 02:04:59 2022
batch 10, train loss = 0.30, mean loss = 0.30
Mon Jan 31 02:06:10 2022
batch 20, train loss = 0.23, mean loss = 0.30
Mon Jan 31 02:07:21 2022
batch 30, train loss = 0.30, mean loss = 0.30
Mon Jan 31 02:08:33 2022
batch 40, train loss = 0.21, mean loss = 0.30
Mon Jan 31 02:09:45 2022
batch 50, train loss = 0.32, mean loss = 0.31
Mon Jan 31 02:10:57 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:11:11 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:12:24 2022
val Loss: 0.02

epoch 14 was done for 469.745057 seconds
Epoch 15/299
----------
Mon Jan 31 02:12:41 2022
batch 0, train loss = 0.22, mean loss = 0.22
Mon Jan 31 02:12:48 2022
batch 10, train loss = 0.30, mean loss = 0.30
Mon Jan 31 02:14:00 2022
batch 20, train loss = 0.22, mean loss = 0.30
Mon Jan 31 02:15:11 2022
batch 30, train loss = 0.30, mean loss = 0.30
Mon Jan 31 02:16:23 2022
batch 40, train loss = 0.20, mean loss = 0.30
Mon Jan 31 02:17:35 2022
batch 50, train loss = 0.32, mean loss = 0.30
Mon Jan 31 02:18:49 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:19:03 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:20:16 2022
val Loss: 0.02

epoch 15 was done for 472.014318 seconds
Epoch 16/299
----------
Mon Jan 31 02:20:33 2022
batch 0, train loss = 0.22, mean loss = 0.22
Mon Jan 31 02:20:41 2022
batch 10, train loss = 0.31, mean loss = 0.30
Mon Jan 31 02:21:52 2022
batch 20, train loss = 0.22, mean loss = 0.30
Mon Jan 31 02:23:04 2022
batch 30, train loss = 0.30, mean loss = 0.30
Mon Jan 31 02:24:16 2022
batch 40, train loss = 0.20, mean loss = 0.30
Mon Jan 31 02:25:28 2022
batch 50, train loss = 0.32, mean loss = 0.30
Mon Jan 31 02:26:39 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:26:54 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:28:06 2022
val Loss: 0.02

epoch 16 was done for 470.237018 seconds
Epoch 17/299
----------
Mon Jan 31 02:28:24 2022
batch 0, train loss = 0.21, mean loss = 0.21
Mon Jan 31 02:28:31 2022
batch 10, train loss = 0.31, mean loss = 0.30
Mon Jan 31 02:29:42 2022
batch 20, train loss = 0.21, mean loss = 0.30
Mon Jan 31 02:30:53 2022
batch 30, train loss = 0.30, mean loss = 0.30
Mon Jan 31 02:32:05 2022
batch 40, train loss = 0.19, mean loss = 0.30
Mon Jan 31 02:33:17 2022
batch 50, train loss = 0.32, mean loss = 0.30
Mon Jan 31 02:34:29 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:34:44 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:35:57 2022
val Loss: 0.02

epoch 17 was done for 470.593583 seconds
Epoch 18/299
----------
Mon Jan 31 02:36:14 2022
batch 0, train loss = 0.21, mean loss = 0.21
Mon Jan 31 02:36:21 2022
batch 10, train loss = 0.31, mean loss = 0.29
Mon Jan 31 02:37:33 2022
batch 20, train loss = 0.21, mean loss = 0.30
Mon Jan 31 02:38:45 2022
batch 30, train loss = 0.30, mean loss = 0.30
Mon Jan 31 02:39:57 2022
batch 40, train loss = 0.19, mean loss = 0.30
Mon Jan 31 02:41:09 2022
batch 50, train loss = 0.32, mean loss = 0.30
Mon Jan 31 02:42:21 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:42:35 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:43:48 2022
val Loss: 0.02

epoch 18 was done for 470.907796 seconds
Epoch 19/299
----------
Mon Jan 31 02:44:05 2022
batch 0, train loss = 0.21, mean loss = 0.21
Mon Jan 31 02:44:12 2022
batch 10, train loss = 0.31, mean loss = 0.29
Mon Jan 31 02:45:24 2022
batch 20, train loss = 0.20, mean loss = 0.29
Mon Jan 31 02:46:36 2022
batch 30, train loss = 0.30, mean loss = 0.30
Mon Jan 31 02:47:48 2022
batch 40, train loss = 0.18, mean loss = 0.30
Mon Jan 31 02:48:59 2022
batch 50, train loss = 0.32, mean loss = 0.30
Mon Jan 31 02:50:11 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:50:26 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:51:38 2022
val Loss: 0.02

epoch 19 was done for 470.418775 seconds
Epoch 20/299
----------
Mon Jan 31 02:51:55 2022
batch 0, train loss = 0.20, mean loss = 0.20
Mon Jan 31 02:52:03 2022
batch 10, train loss = 0.31, mean loss = 0.29
Mon Jan 31 02:53:14 2022
batch 20, train loss = 0.20, mean loss = 0.29
Mon Jan 31 02:54:26 2022
batch 30, train loss = 0.30, mean loss = 0.30
Mon Jan 31 02:55:38 2022
batch 40, train loss = 0.18, mean loss = 0.30
Mon Jan 31 02:56:50 2022
batch 50, train loss = 0.32, mean loss = 0.30
Mon Jan 31 02:58:02 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:58:17 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:59:30 2022
val Loss: 0.02

epoch 20 was done for 471.414137 seconds
Epoch 21/299
----------
Mon Jan 31 02:59:47 2022
batch 0, train loss = 0.20, mean loss = 0.20
Mon Jan 31 02:59:54 2022
batch 10, train loss = 0.31, mean loss = 0.29
Mon Jan 31 03:01:07 2022
batch 20, train loss = 0.19, mean loss = 0.29
Mon Jan 31 03:02:20 2022
batch 30, train loss = 0.30, mean loss = 0.30
Mon Jan 31 03:03:33 2022
batch 40, train loss = 0.17, mean loss = 0.30
Mon Jan 31 03:04:46 2022
batch 50, train loss = 0.33, mean loss = 0.30
Mon Jan 31 03:05:59 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:06:14 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:07:27 2022
val Loss: 0.02

epoch 21 was done for 478.013967 seconds
Epoch 22/299
----------
Mon Jan 31 03:07:45 2022
batch 0, train loss = 0.19, mean loss = 0.19
Mon Jan 31 03:07:52 2022
batch 10, train loss = 0.31, mean loss = 0.29
Mon Jan 31 03:09:04 2022
batch 20, train loss = 0.19, mean loss = 0.29
Mon Jan 31 03:10:15 2022
batch 30, train loss = 0.30, mean loss = 0.30
Mon Jan 31 03:11:27 2022
batch 40, train loss = 0.17, mean loss = 0.30
Mon Jan 31 03:12:39 2022
batch 50, train loss = 0.33, mean loss = 0.30
Mon Jan 31 03:13:51 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:14:05 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:15:18 2022
val Loss: 0.02

epoch 22 was done for 469.724522 seconds
Epoch 23/299
----------
Mon Jan 31 03:15:35 2022
batch 0, train loss = 0.19, mean loss = 0.19
Mon Jan 31 03:15:42 2022
batch 10, train loss = 0.31, mean loss = 0.29
Mon Jan 31 03:16:53 2022
batch 20, train loss = 0.18, mean loss = 0.29
Mon Jan 31 03:18:05 2022
batch 30, train loss = 0.30, mean loss = 0.30
Mon Jan 31 03:19:17 2022
batch 40, train loss = 0.16, mean loss = 0.30
Mon Jan 31 03:20:29 2022
batch 50, train loss = 0.33, mean loss = 0.30
Mon Jan 31 03:21:42 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:21:56 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:23:09 2022
val Loss: 0.02

epoch 23 was done for 472.027805 seconds
Epoch 24/299
----------
Mon Jan 31 03:23:27 2022
batch 0, train loss = 0.18, mean loss = 0.18
Mon Jan 31 03:23:34 2022
batch 10, train loss = 0.31, mean loss = 0.29
Mon Jan 31 03:24:46 2022
batch 20, train loss = 0.18, mean loss = 0.29
Mon Jan 31 03:25:59 2022
batch 30, train loss = 0.30, mean loss = 0.30
Mon Jan 31 03:27:11 2022
batch 40, train loss = 0.16, mean loss = 0.29
Mon Jan 31 03:28:23 2022
batch 50, train loss = 0.33, mean loss = 0.30
Mon Jan 31 03:29:35 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:29:50 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:31:03 2022
val Loss: 0.02

epoch 24 was done for 473.701877 seconds
Epoch 25/299
----------
Mon Jan 31 03:31:20 2022
batch 0, train loss = 0.18, mean loss = 0.18
Mon Jan 31 03:31:28 2022
batch 10, train loss = 0.31, mean loss = 0.29
Mon Jan 31 03:32:39 2022
batch 20, train loss = 0.17, mean loss = 0.29
Mon Jan 31 03:33:51 2022
batch 30, train loss = 0.30, mean loss = 0.30
Mon Jan 31 03:35:02 2022
batch 40, train loss = 0.15, mean loss = 0.29
Mon Jan 31 03:36:14 2022
batch 50, train loss = 0.33, mean loss = 0.30
Mon Jan 31 03:37:26 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:37:41 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:38:53 2022
val Loss: 0.02

epoch 25 was done for 469.716564 seconds
Epoch 26/299
----------
Mon Jan 31 03:39:10 2022
batch 0, train loss = 0.17, mean loss = 0.17
Mon Jan 31 03:39:17 2022
batch 10, train loss = 0.31, mean loss = 0.29
Mon Jan 31 03:40:29 2022
batch 20, train loss = 0.17, mean loss = 0.29
Mon Jan 31 03:41:41 2022
batch 30, train loss = 0.30, mean loss = 0.29
Mon Jan 31 03:42:52 2022
batch 40, train loss = 0.14, mean loss = 0.29
Mon Jan 31 03:44:05 2022
batch 50, train loss = 0.33, mean loss = 0.30
Mon Jan 31 03:45:18 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:45:32 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:46:45 2022
val Loss: 0.02

epoch 26 was done for 472.459190 seconds
Epoch 27/299
----------
Mon Jan 31 03:47:03 2022
batch 0, train loss = 0.17, mean loss = 0.17
Mon Jan 31 03:47:10 2022
batch 10, train loss = 0.31, mean loss = 0.29
Mon Jan 31 03:48:22 2022
batch 20, train loss = 0.16, mean loss = 0.29
Mon Jan 31 03:49:34 2022
batch 30, train loss = 0.30, mean loss = 0.29
Mon Jan 31 03:50:46 2022
batch 40, train loss = 0.14, mean loss = 0.29
Mon Jan 31 03:51:57 2022
batch 50, train loss = 0.33, mean loss = 0.30
Mon Jan 31 03:53:10 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:53:24 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:54:37 2022
val Loss: 0.02

epoch 27 was done for 471.286263 seconds
Epoch 28/299
----------
Mon Jan 31 03:54:54 2022
batch 0, train loss = 0.16, mean loss = 0.16
Mon Jan 31 03:55:01 2022
batch 10, train loss = 0.32, mean loss = 0.29
Mon Jan 31 03:56:13 2022
batch 20, train loss = 0.15, mean loss = 0.29
Mon Jan 31 03:57:25 2022
batch 30, train loss = 0.30, mean loss = 0.29
Mon Jan 31 03:58:37 2022
batch 40, train loss = 0.13, mean loss = 0.29
Mon Jan 31 03:59:49 2022
batch 50, train loss = 0.33, mean loss = 0.30
Mon Jan 31 04:01:01 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:01:15 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:02:28 2022
val Loss: 0.02

epoch 28 was done for 471.005002 seconds
Epoch 29/299
----------
Mon Jan 31 04:02:45 2022
batch 0, train loss = 0.15, mean loss = 0.15
Mon Jan 31 04:02:52 2022
batch 10, train loss = 0.32, mean loss = 0.29
Mon Jan 31 04:04:04 2022
batch 20, train loss = 0.15, mean loss = 0.28
Mon Jan 31 04:05:15 2022
batch 30, train loss = 0.30, mean loss = 0.29
Mon Jan 31 04:06:27 2022
batch 40, train loss = 0.13, mean loss = 0.29
Mon Jan 31 04:07:39 2022
batch 50, train loss = 0.33, mean loss = 0.30
Mon Jan 31 04:08:51 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:09:06 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:10:19 2022
val Loss: 0.02

epoch 29 was done for 471.832433 seconds
Epoch 30/299
----------
Mon Jan 31 04:10:37 2022
batch 0, train loss = 0.15, mean loss = 0.15
Mon Jan 31 04:10:44 2022
batch 10, train loss = 0.32, mean loss = 0.29
Mon Jan 31 04:11:56 2022
batch 20, train loss = 0.14, mean loss = 0.28
Mon Jan 31 04:13:08 2022
batch 30, train loss = 0.31, mean loss = 0.29
Mon Jan 31 04:14:19 2022
batch 40, train loss = 0.12, mean loss = 0.29
Mon Jan 31 04:15:31 2022
batch 50, train loss = 0.33, mean loss = 0.30
Mon Jan 31 04:16:43 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:16:58 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:18:10 2022
val Loss: 0.02

epoch 30 was done for 470.931286 seconds
Epoch 31/299
----------
Mon Jan 31 04:18:28 2022
batch 0, train loss = 0.14, mean loss = 0.14
Mon Jan 31 04:18:35 2022
batch 10, train loss = 0.32, mean loss = 0.29
Mon Jan 31 04:19:47 2022
batch 20, train loss = 0.14, mean loss = 0.28
Mon Jan 31 04:20:59 2022
batch 30, train loss = 0.31, mean loss = 0.29
Mon Jan 31 04:22:11 2022
batch 40, train loss = 0.12, mean loss = 0.29
Mon Jan 31 04:23:22 2022
batch 50, train loss = 0.34, mean loss = 0.30
Mon Jan 31 04:24:34 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:24:48 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:26:01 2022
val Loss: 0.02

epoch 31 was done for 470.567344 seconds
Epoch 32/299
----------
Mon Jan 31 04:26:18 2022
batch 0, train loss = 0.14, mean loss = 0.14
Mon Jan 31 04:26:25 2022
batch 10, train loss = 0.32, mean loss = 0.28
Mon Jan 31 04:27:37 2022
batch 20, train loss = 0.13, mean loss = 0.28
Mon Jan 31 04:28:48 2022
batch 30, train loss = 0.31, mean loss = 0.29
Mon Jan 31 04:30:00 2022
batch 40, train loss = 0.11, mean loss = 0.29
Mon Jan 31 04:31:13 2022
batch 50, train loss = 0.34, mean loss = 0.30
Mon Jan 31 04:32:25 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:32:40 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:33:53 2022
val Loss: 0.02

epoch 32 was done for 471.990154 seconds
Epoch 33/299
----------
Mon Jan 31 04:34:10 2022
batch 0, train loss = 0.13, mean loss = 0.13
Mon Jan 31 04:34:17 2022
batch 10, train loss = 0.32, mean loss = 0.28
Mon Jan 31 04:35:29 2022
batch 20, train loss = 0.13, mean loss = 0.28
Mon Jan 31 04:36:41 2022
batch 30, train loss = 0.31, mean loss = 0.29
Mon Jan 31 04:37:53 2022
batch 40, train loss = 0.11, mean loss = 0.29
Mon Jan 31 04:39:05 2022
batch 50, train loss = 0.34, mean loss = 0.30
Mon Jan 31 04:40:17 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:40:31 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:41:44 2022
val Loss: 0.02

epoch 33 was done for 470.958688 seconds
Epoch 34/299
----------
Mon Jan 31 04:42:01 2022
batch 0, train loss = 0.13, mean loss = 0.13
Mon Jan 31 04:42:08 2022
batch 10, train loss = 0.33, mean loss = 0.28
Mon Jan 31 04:43:21 2022
batch 20, train loss = 0.12, mean loss = 0.28
Mon Jan 31 04:44:33 2022
batch 30, train loss = 0.31, mean loss = 0.29
Mon Jan 31 04:45:45 2022
batch 40, train loss = 0.10, mean loss = 0.29
Mon Jan 31 04:46:57 2022
batch 50, train loss = 0.34, mean loss = 0.30
Mon Jan 31 04:48:09 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:48:23 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:49:36 2022
val Loss: 0.02

epoch 34 was done for 471.866969 seconds
Epoch 35/299
----------
Mon Jan 31 04:49:53 2022
batch 0, train loss = 0.12, mean loss = 0.12
Mon Jan 31 04:50:00 2022
batch 10, train loss = 0.33, mean loss = 0.28
Mon Jan 31 04:51:12 2022
batch 20, train loss = 0.11, mean loss = 0.28
Mon Jan 31 04:52:24 2022
batch 30, train loss = 0.32, mean loss = 0.29
Mon Jan 31 04:53:35 2022
batch 40, train loss = 0.10, mean loss = 0.29
Mon Jan 31 04:54:48 2022
batch 50, train loss = 0.34, mean loss = 0.30
Mon Jan 31 04:56:00 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:56:15 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:57:28 2022
val Loss: 0.02

epoch 35 was done for 472.703948 seconds
Epoch 36/299
----------
Mon Jan 31 04:57:46 2022
batch 0, train loss = 0.11, mean loss = 0.11
Mon Jan 31 04:57:53 2022
batch 10, train loss = 0.33, mean loss = 0.28
Mon Jan 31 04:59:05 2022
batch 20, train loss = 0.11, mean loss = 0.28
Mon Jan 31 05:00:17 2022
batch 30, train loss = 0.32, mean loss = 0.29
Mon Jan 31 05:01:29 2022
batch 40, train loss = 0.09, mean loss = 0.29
Mon Jan 31 05:02:41 2022
batch 50, train loss = 0.35, mean loss = 0.30
Mon Jan 31 05:03:53 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:04:07 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:05:20 2022
val Loss: 0.02

epoch 36 was done for 471.458925 seconds
Epoch 37/299
----------
Mon Jan 31 05:05:37 2022
batch 0, train loss = 0.11, mean loss = 0.11
Mon Jan 31 05:05:44 2022
batch 10, train loss = 0.33, mean loss = 0.28
Mon Jan 31 05:06:57 2022
batch 20, train loss = 0.10, mean loss = 0.28
Mon Jan 31 05:08:09 2022
batch 30, train loss = 0.32, mean loss = 0.29
Mon Jan 31 05:09:21 2022
batch 40, train loss = 0.09, mean loss = 0.29
Mon Jan 31 05:10:33 2022
batch 50, train loss = 0.35, mean loss = 0.30
Mon Jan 31 05:11:45 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:12:00 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:13:13 2022
val Loss: 0.02

epoch 37 was done for 473.155293 seconds
Epoch 38/299
----------
Mon Jan 31 05:13:30 2022
batch 0, train loss = 0.10, mean loss = 0.10
Mon Jan 31 05:13:37 2022
batch 10, train loss = 0.33, mean loss = 0.28
Mon Jan 31 05:14:49 2022
batch 20, train loss = 0.10, mean loss = 0.28
Mon Jan 31 05:16:01 2022
batch 30, train loss = 0.32, mean loss = 0.29
Mon Jan 31 05:17:12 2022
batch 40, train loss = 0.08, mean loss = 0.29
Mon Jan 31 05:18:25 2022
batch 50, train loss = 0.35, mean loss = 0.30
Mon Jan 31 05:19:38 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:19:53 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:21:06 2022
val Loss: 0.02

epoch 38 was done for 473.563188 seconds
Epoch 39/299
----------
Mon Jan 31 05:21:24 2022
batch 0, train loss = 0.10, mean loss = 0.10
Mon Jan 31 05:21:31 2022
batch 10, train loss = 0.33, mean loss = 0.28
Mon Jan 31 05:22:43 2022
batch 20, train loss = 0.09, mean loss = 0.28
Mon Jan 31 05:23:55 2022
batch 30, train loss = 0.32, mean loss = 0.29
Mon Jan 31 05:25:07 2022
batch 40, train loss = 0.08, mean loss = 0.29
Mon Jan 31 05:26:20 2022
batch 50, train loss = 0.36, mean loss = 0.30
Mon Jan 31 05:27:32 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:27:46 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:28:59 2022
val Loss: 0.02

epoch 39 was done for 472.449584 seconds
Epoch 40/299
----------
Mon Jan 31 05:29:16 2022
batch 0, train loss = 0.09, mean loss = 0.09
Mon Jan 31 05:29:24 2022
batch 10, train loss = 0.35, mean loss = 0.29
Mon Jan 31 05:30:36 2022
batch 20, train loss = 0.12, mean loss = 0.30
Mon Jan 31 05:31:48 2022
batch 30, train loss = 0.33, mean loss = 0.31
Mon Jan 31 05:33:00 2022
batch 40, train loss = 0.10, mean loss = 0.31
Mon Jan 31 05:34:12 2022
batch 50, train loss = 0.37, mean loss = 0.31
Mon Jan 31 05:35:24 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:35:38 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:36:51 2022
val Loss: 0.02

epoch 40 was done for 472.312484 seconds
Epoch 41/299
----------
Mon Jan 31 05:37:09 2022
batch 0, train loss = 0.11, mean loss = 0.11
Mon Jan 31 05:37:16 2022
batch 10, train loss = 0.35, mean loss = 0.30
Mon Jan 31 05:38:27 2022
batch 20, train loss = 0.09, mean loss = 0.29
Mon Jan 31 05:39:38 2022
batch 30, train loss = 0.37, mean loss = 0.32
Mon Jan 31 05:40:49 2022
batch 40, train loss = 0.13, mean loss = 0.32
Mon Jan 31 05:42:02 2022
batch 50, train loss = 0.39, mean loss = 0.33
Mon Jan 31 05:43:15 2022
train Loss: 0.33

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:43:29 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:44:42 2022
val Loss: 0.02

epoch 41 was done for 471.150417 seconds
Epoch 42/299
----------
Mon Jan 31 05:45:00 2022
batch 0, train loss = 0.11, mean loss = 0.11
Mon Jan 31 05:45:07 2022
batch 10, train loss = 0.36, mean loss = 0.32
Mon Jan 31 05:46:19 2022
batch 20, train loss = 0.11, mean loss = 0.33
Mon Jan 31 05:47:31 2022
batch 30, train loss = 0.34, mean loss = 0.34
Mon Jan 31 05:48:43 2022
batch 40, train loss = 0.10, mean loss = 0.33
Mon Jan 31 05:49:54 2022
batch 50, train loss = 0.37, mean loss = 0.33
Mon Jan 31 05:51:06 2022
train Loss: 0.33

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:51:21 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:52:33 2022
val Loss: 0.02

epoch 42 was done for 470.687422 seconds
Epoch 43/299
----------
Mon Jan 31 05:52:50 2022
batch 0, train loss = 0.09, mean loss = 0.09
Mon Jan 31 05:52:58 2022
batch 10, train loss = 0.35, mean loss = 0.29
Mon Jan 31 05:54:10 2022
batch 20, train loss = 0.08, mean loss = 0.29
Mon Jan 31 05:55:22 2022
batch 30, train loss = 0.38, mean loss = 0.31
Mon Jan 31 05:56:34 2022
batch 40, train loss = 0.09, mean loss = 0.31
Mon Jan 31 05:57:46 2022
batch 50, train loss = 0.45, mean loss = 0.32
Mon Jan 31 05:58:57 2022
train Loss: 0.32

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:59:12 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:00:24 2022
val Loss: 0.02

epoch 43 was done for 471.193496 seconds
Epoch 44/299
----------
Mon Jan 31 06:00:42 2022
batch 0, train loss = 0.14, mean loss = 0.14
Mon Jan 31 06:00:49 2022
batch 10, train loss = 0.42, mean loss = 0.33
Mon Jan 31 06:02:00 2022
batch 20, train loss = 0.14, mean loss = 0.33
Mon Jan 31 06:03:12 2022
batch 30, train loss = 0.49, mean loss = 0.35
Mon Jan 31 06:04:23 2022
batch 40, train loss = 0.18, mean loss = 0.35
Mon Jan 31 06:05:35 2022
batch 50, train loss = 0.51, mean loss = 0.37
Mon Jan 31 06:06:48 2022
train Loss: 0.37

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 06:07:03 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:08:16 2022
val Loss: 0.02

epoch 44 was done for 471.319264 seconds
Epoch 45/299
----------
Mon Jan 31 06:08:33 2022
batch 0, train loss = 0.18, mean loss = 0.18
Mon Jan 31 06:08:40 2022
batch 10, train loss = 0.40, mean loss = 0.39
Mon Jan 31 06:09:52 2022
batch 20, train loss = 0.20, mean loss = 0.42
Mon Jan 31 06:11:04 2022
batch 30, train loss = 0.45, mean loss = 0.45
Mon Jan 31 06:12:16 2022
batch 40, train loss = 0.14, mean loss = 0.46
Mon Jan 31 06:13:28 2022
batch 50, train loss = 0.68, mean loss = 0.48
Mon Jan 31 06:14:39 2022
train Loss: 0.48

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:14:53 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 06:16:06 2022
val Loss: 0.03

epoch 45 was done for 470.139261 seconds
Epoch 46/299
----------
Mon Jan 31 06:16:23 2022
batch 0, train loss = 0.45, mean loss = 0.45
Mon Jan 31 06:16:30 2022
batch 10, train loss = 0.58, mean loss = 0.52
Mon Jan 31 06:17:42 2022
batch 20, train loss = 0.40, mean loss = 0.52
Mon Jan 31 06:18:54 2022
batch 30, train loss = 0.44, mean loss = 0.53
Mon Jan 31 06:20:06 2022
batch 40, train loss = 0.17, mean loss = 0.51
Mon Jan 31 06:21:18 2022
batch 50, train loss = 0.49, mean loss = 0.50
Mon Jan 31 06:22:30 2022
train Loss: 0.50

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:22:45 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:23:58 2022
val Loss: 0.02

epoch 46 was done for 471.741079 seconds
Epoch 47/299
----------
Mon Jan 31 06:24:15 2022
batch 0, train loss = 0.19, mean loss = 0.19
Mon Jan 31 06:24:22 2022
batch 10, train loss = 0.41, mean loss = 0.36
Mon Jan 31 06:25:33 2022
batch 20, train loss = 0.12, mean loss = 0.35
Mon Jan 31 06:26:44 2022
batch 30, train loss = 0.40, mean loss = 0.36
Mon Jan 31 06:27:55 2022
batch 40, train loss = 0.17, mean loss = 0.37
Mon Jan 31 06:29:07 2022
batch 50, train loss = 0.42, mean loss = 0.37
Mon Jan 31 06:30:20 2022
train Loss: 0.37

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:30:34 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:31:47 2022
val Loss: 0.02

epoch 47 was done for 469.384747 seconds
Epoch 48/299
----------
Mon Jan 31 06:32:04 2022
batch 0, train loss = 0.13, mean loss = 0.13
Mon Jan 31 06:32:11 2022
batch 10, train loss = 0.43, mean loss = 0.37
Mon Jan 31 06:33:24 2022
batch 20, train loss = 0.12, mean loss = 0.38
Mon Jan 31 06:34:37 2022
batch 30, train loss = 0.55, mean loss = 0.40
Mon Jan 31 06:35:49 2022
batch 40, train loss = 0.25, mean loss = 0.41
Mon Jan 31 06:37:02 2022
batch 50, train loss = 0.63, mean loss = 0.43
Mon Jan 31 06:38:14 2022
train Loss: 0.44

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:38:29 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:39:43 2022
val Loss: 0.02

epoch 48 was done for 475.823286 seconds
Epoch 49/299
----------
Mon Jan 31 06:40:00 2022
batch 0, train loss = 0.32, mean loss = 0.32
Mon Jan 31 06:40:07 2022
batch 10, train loss = 0.44, mean loss = 0.49
Mon Jan 31 06:41:20 2022
batch 20, train loss = 0.22, mean loss = 0.48
Mon Jan 31 06:42:33 2022
batch 30, train loss = 0.50, mean loss = 0.50
Mon Jan 31 06:43:46 2022
batch 40, train loss = 0.19, mean loss = 0.51
Mon Jan 31 06:44:59 2022
batch 50, train loss = 0.82, mean loss = 0.54
Mon Jan 31 06:46:12 2022
train Loss: 0.55

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:46:27 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 06:47:40 2022
val Loss: 0.03

epoch 49 was done for 477.698598 seconds
Epoch 50/299
----------
Mon Jan 31 06:47:58 2022
batch 0, train loss = 0.50, mean loss = 0.50
Mon Jan 31 06:48:05 2022
batch 10, train loss = 0.81, mean loss = 0.68
Mon Jan 31 06:49:17 2022
batch 20, train loss = 0.63, mean loss = 0.69
Mon Jan 31 06:50:29 2022
batch 30, train loss = 0.64, mean loss = 0.69
Mon Jan 31 06:51:41 2022
batch 40, train loss = 0.53, mean loss = 0.68
Mon Jan 31 06:52:54 2022
batch 50, train loss = 0.69, mean loss = 0.68
Mon Jan 31 06:54:07 2022
train Loss: 0.68

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:54:21 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:55:35 2022
val Loss: 0.02

epoch 50 was done for 474.424212 seconds
Epoch 51/299
----------
Mon Jan 31 06:55:52 2022
batch 0, train loss = 0.47, mean loss = 0.47
Mon Jan 31 06:55:59 2022
batch 10, train loss = 0.67, mean loss = 0.67
Mon Jan 31 06:57:11 2022
batch 20, train loss = 0.51, mean loss = 0.67
Mon Jan 31 06:58:24 2022
batch 30, train loss = 0.62, mean loss = 0.67
Mon Jan 31 06:59:36 2022
batch 40, train loss = 0.42, mean loss = 0.65
Mon Jan 31 07:00:48 2022
batch 50, train loss = 0.54, mean loss = 0.64
Mon Jan 31 07:02:00 2022
train Loss: 0.63

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:02:14 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:03:27 2022
val Loss: 0.02

epoch 51 was done for 472.276970 seconds
Epoch 52/299
----------
Mon Jan 31 07:03:44 2022
batch 0, train loss = 0.32, mean loss = 0.32
Mon Jan 31 07:03:52 2022
batch 10, train loss = 0.52, mean loss = 0.48
Mon Jan 31 07:05:04 2022
batch 20, train loss = 0.24, mean loss = 0.46
Mon Jan 31 07:06:15 2022
batch 30, train loss = 0.48, mean loss = 0.45
Mon Jan 31 07:07:27 2022
batch 40, train loss = 0.30, mean loss = 0.45
Mon Jan 31 07:08:39 2022
batch 50, train loss = 0.53, mean loss = 0.46
Mon Jan 31 07:09:51 2022
train Loss: 0.46

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:10:05 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:11:18 2022
val Loss: 0.02

epoch 52 was done for 471.072892 seconds
Epoch 53/299
----------
Mon Jan 31 07:11:36 2022
batch 0, train loss = 0.27, mean loss = 0.27
Mon Jan 31 07:11:43 2022
batch 10, train loss = 0.37, mean loss = 0.46
Mon Jan 31 07:12:54 2022
batch 20, train loss = 0.19, mean loss = 0.47
Mon Jan 31 07:14:06 2022
batch 30, train loss = 0.43, mean loss = 0.48
Mon Jan 31 07:15:17 2022
batch 40, train loss = 0.13, mean loss = 0.47
Mon Jan 31 07:16:29 2022
batch 50, train loss = 0.49, mean loss = 0.47
Mon Jan 31 07:17:42 2022
train Loss: 0.47

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:17:56 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:19:10 2022
val Loss: 0.02

epoch 53 was done for 471.357960 seconds
Epoch 54/299
----------
Mon Jan 31 07:19:27 2022
batch 0, train loss = 0.22, mean loss = 0.22
Mon Jan 31 07:19:34 2022
batch 10, train loss = 0.47, mean loss = 0.40
Mon Jan 31 07:20:46 2022
batch 20, train loss = 0.24, mean loss = 0.40
Mon Jan 31 07:21:58 2022
batch 30, train loss = 0.50, mean loss = 0.41
Mon Jan 31 07:23:09 2022
batch 40, train loss = 0.17, mean loss = 0.40
Mon Jan 31 07:24:21 2022
batch 50, train loss = 0.43, mean loss = 0.39
Mon Jan 31 07:25:33 2022
train Loss: 0.40

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:25:47 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:27:00 2022
val Loss: 0.02

epoch 54 was done for 470.385084 seconds
Epoch 55/299
----------
Mon Jan 31 07:27:17 2022
batch 0, train loss = 0.15, mean loss = 0.15
Mon Jan 31 07:27:24 2022
batch 10, train loss = 0.36, mean loss = 0.33
Mon Jan 31 07:28:36 2022
batch 20, train loss = 0.11, mean loss = 0.33
Mon Jan 31 07:29:48 2022
batch 30, train loss = 0.40, mean loss = 0.34
Mon Jan 31 07:31:00 2022
batch 40, train loss = 0.10, mean loss = 0.34
Mon Jan 31 07:32:15 2022
batch 50, train loss = 0.40, mean loss = 0.34
Mon Jan 31 07:33:26 2022
train Loss: 0.35

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:33:41 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:34:54 2022
val Loss: 0.02

epoch 55 was done for 473.540535 seconds
Epoch 56/299
----------
Mon Jan 31 07:35:11 2022
batch 0, train loss = 0.08, mean loss = 0.08
Mon Jan 31 07:35:18 2022
batch 10, train loss = 0.35, mean loss = 0.33
Mon Jan 31 07:36:29 2022
batch 20, train loss = 0.07, mean loss = 0.33
Mon Jan 31 07:37:41 2022
batch 30, train loss = 0.40, mean loss = 0.34
Mon Jan 31 07:38:52 2022
batch 40, train loss = 0.07, mean loss = 0.34
Mon Jan 31 07:40:04 2022
batch 50, train loss = 0.44, mean loss = 0.34
Mon Jan 31 07:41:25 2022
train Loss: 0.35

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:41:39 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:42:52 2022
val Loss: 0.02

epoch 56 was done for 478.905713 seconds
Epoch 57/299
----------
Mon Jan 31 07:43:10 2022
batch 0, train loss = 0.10, mean loss = 0.10
Mon Jan 31 07:43:17 2022
batch 10, train loss = 0.37, mean loss = 0.34
Mon Jan 31 07:44:29 2022
batch 20, train loss = 0.10, mean loss = 0.34
Mon Jan 31 07:45:45 2022
batch 30, train loss = 0.44, mean loss = 0.35
Mon Jan 31 07:46:57 2022
batch 40, train loss = 0.09, mean loss = 0.35
Mon Jan 31 07:48:11 2022
batch 50, train loss = 0.43, mean loss = 0.35
Mon Jan 31 07:49:23 2022
train Loss: 0.36

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:49:37 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:50:50 2022
val Loss: 0.02

epoch 57 was done for 477.139425 seconds
Epoch 58/299
----------
Mon Jan 31 07:51:07 2022
batch 0, train loss = 0.09, mean loss = 0.09
Mon Jan 31 07:51:14 2022
batch 10, train loss = 0.37, mean loss = 0.33
Mon Jan 31 07:52:26 2022
batch 20, train loss = 0.09, mean loss = 0.33
Mon Jan 31 07:53:38 2022
batch 30, train loss = 0.44, mean loss = 0.35
Mon Jan 31 07:54:49 2022
batch 40, train loss = 0.07, mean loss = 0.35
Mon Jan 31 07:56:03 2022
batch 50, train loss = 0.48, mean loss = 0.36
Mon Jan 31 07:57:14 2022
train Loss: 0.36

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:57:29 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:58:42 2022
val Loss: 0.02

epoch 58 was done for 472.303283 seconds
Epoch 59/299
----------
Mon Jan 31 07:58:59 2022
batch 0, train loss = 0.11, mean loss = 0.11
Mon Jan 31 07:59:06 2022
batch 10, train loss = 0.38, mean loss = 0.36
Mon Jan 31 08:00:19 2022
batch 20, train loss = 0.09, mean loss = 0.35
Mon Jan 31 08:01:32 2022
batch 30, train loss = 0.43, mean loss = 0.37
Mon Jan 31 08:02:44 2022
batch 40, train loss = 0.07, mean loss = 0.36
Mon Jan 31 08:03:57 2022
batch 50, train loss = 0.49, mean loss = 0.38
Mon Jan 31 08:05:10 2022
train Loss: 0.38

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:05:25 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:06:39 2022
val Loss: 0.02

epoch 59 was done for 477.595125 seconds
Epoch 60/299
----------
Mon Jan 31 08:06:57 2022
batch 0, train loss = 0.11, mean loss = 0.11
Mon Jan 31 08:07:04 2022
batch 10, train loss = 0.43, mean loss = 0.39
Mon Jan 31 08:08:16 2022
batch 20, train loss = 0.12, mean loss = 0.39
Mon Jan 31 08:09:29 2022
batch 30, train loss = 0.45, mean loss = 0.40
Mon Jan 31 08:10:41 2022
batch 40, train loss = 0.10, mean loss = 0.40
Mon Jan 31 08:11:53 2022
batch 50, train loss = 0.53, mean loss = 0.41
Mon Jan 31 08:13:05 2022
train Loss: 0.41

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:13:20 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:14:33 2022
val Loss: 0.02

epoch 60 was done for 473.307057 seconds
Epoch 61/299
----------
Mon Jan 31 08:14:50 2022
batch 0, train loss = 0.15, mean loss = 0.15
Mon Jan 31 08:14:57 2022
batch 10, train loss = 0.48, mean loss = 0.42
Mon Jan 31 08:16:10 2022
batch 20, train loss = 0.23, mean loss = 0.44
Mon Jan 31 08:17:22 2022
batch 30, train loss = 0.46, mean loss = 0.45
Mon Jan 31 08:18:35 2022
batch 40, train loss = 0.10, mean loss = 0.44
Mon Jan 31 08:19:47 2022
batch 50, train loss = 0.55, mean loss = 0.45
Mon Jan 31 08:21:00 2022
train Loss: 0.45

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:21:15 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:22:28 2022
val Loss: 0.02

epoch 61 was done for 475.958696 seconds
Epoch 62/299
----------
Mon Jan 31 08:22:46 2022
batch 0, train loss = 0.17, mean loss = 0.17
Mon Jan 31 08:22:53 2022
batch 10, train loss = 0.59, mean loss = 0.45
Mon Jan 31 08:24:05 2022
batch 20, train loss = 0.33, mean loss = 0.49
Mon Jan 31 08:25:17 2022
batch 30, train loss = 0.58, mean loss = 0.50
Mon Jan 31 08:26:29 2022
batch 40, train loss = 0.28, mean loss = 0.50
Mon Jan 31 08:27:46 2022
batch 50, train loss = 0.46, mean loss = 0.51
Mon Jan 31 08:29:00 2022
train Loss: 0.51

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:29:15 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:30:31 2022
val Loss: 0.02

epoch 62 was done for 482.513423 seconds
Epoch 63/299
----------
Mon Jan 31 08:30:49 2022
batch 0, train loss = 0.09, mean loss = 0.09
Mon Jan 31 08:30:56 2022
batch 10, train loss = 0.83, mean loss = 0.59
Mon Jan 31 08:32:09 2022
batch 20, train loss = 0.33, mean loss = 0.63
Mon Jan 31 08:33:22 2022
batch 30, train loss = 1.18, mean loss = 0.69
Mon Jan 31 08:34:45 2022
batch 40, train loss = 0.71, mean loss = 0.71
Mon Jan 31 08:36:08 2022
batch 50, train loss = 0.84, mean loss = 0.72
Mon Jan 31 08:37:29 2022
train Loss: 0.72

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 08:37:45 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:39:05 2022
val Loss: 0.02

epoch 63 was done for 515.691477 seconds
Epoch 64/299
----------
Mon Jan 31 08:39:24 2022
batch 0, train loss = 0.61, mean loss = 0.61
Mon Jan 31 08:39:32 2022
batch 10, train loss = 0.52, mean loss = 0.70
Mon Jan 31 08:40:48 2022
batch 20, train loss = 0.20, mean loss = 0.69
Mon Jan 31 08:42:06 2022
batch 30, train loss = 0.99, mean loss = 0.77
Mon Jan 31 08:43:28 2022
batch 40, train loss = 0.28, mean loss = 0.77
Mon Jan 31 08:44:49 2022
batch 50, train loss = 0.99, mean loss = 0.79
Mon Jan 31 08:46:10 2022
train Loss: 0.79

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:46:26 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 08:47:47 2022
val Loss: 0.03

epoch 64 was done for 520.997492 seconds
Epoch 65/299
----------
Mon Jan 31 08:48:05 2022
batch 0, train loss = 0.71, mean loss = 0.71
Mon Jan 31 08:48:13 2022
batch 10, train loss = 0.84, mean loss = 0.72
Mon Jan 31 08:49:33 2022
batch 20, train loss = 0.74, mean loss = 0.72
Mon Jan 31 08:50:58 2022
batch 30, train loss = 0.71, mean loss = 0.69
Mon Jan 31 08:52:18 2022
batch 40, train loss = 0.57, mean loss = 0.68
Mon Jan 31 08:53:38 2022
batch 50, train loss = 0.57, mean loss = 0.67
Mon Jan 31 08:54:56 2022
train Loss: 0.67

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:55:10 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:56:24 2022
val Loss: 0.03

epoch 65 was done for 516.058408 seconds
Epoch 66/299
----------
Mon Jan 31 08:56:41 2022
batch 0, train loss = 0.34, mean loss = 0.34
Mon Jan 31 08:56:48 2022
batch 10, train loss = 0.43, mean loss = 0.53
Mon Jan 31 08:58:00 2022
batch 20, train loss = 0.23, mean loss = 0.50
Mon Jan 31 08:59:21 2022
batch 30, train loss = 0.52, mean loss = 0.49
Mon Jan 31 09:00:50 2022
batch 40, train loss = 0.13, mean loss = 0.46
Mon Jan 31 09:02:21 2022
batch 50, train loss = 0.40, mean loss = 0.45
Mon Jan 31 09:03:51 2022
train Loss: 0.45

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 09:04:09 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:05:39 2022
val Loss: 0.03

epoch 66 was done for 558.882073 seconds
Epoch 67/299
----------
Mon Jan 31 09:06:00 2022
batch 0, train loss = 0.12, mean loss = 0.12
Mon Jan 31 09:06:09 2022
batch 10, train loss = 0.39, mean loss = 0.38
Mon Jan 31 09:07:38 2022
batch 20, train loss = 0.20, mean loss = 0.38
Mon Jan 31 09:09:14 2022
batch 30, train loss = 0.56, mean loss = 0.40
Mon Jan 31 09:10:43 2022
batch 40, train loss = 0.18, mean loss = 0.41
Mon Jan 31 09:12:13 2022
batch 50, train loss = 0.45, mean loss = 0.43
Mon Jan 31 09:13:43 2022
train Loss: 0.43

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:14:01 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:15:30 2022
val Loss: 0.02

epoch 67 was done for 590.943629 seconds
Epoch 68/299
----------
Mon Jan 31 09:15:51 2022
batch 0, train loss = 0.13, mean loss = 0.13
Mon Jan 31 09:16:00 2022
batch 10, train loss = 0.42, mean loss = 0.45
Mon Jan 31 09:17:29 2022
batch 20, train loss = 0.20, mean loss = 0.44
Mon Jan 31 09:18:59 2022
batch 30, train loss = 0.67, mean loss = 0.46
Mon Jan 31 09:20:28 2022
batch 40, train loss = 0.15, mean loss = 0.46
Mon Jan 31 09:21:58 2022
batch 50, train loss = 0.48, mean loss = 0.48
Mon Jan 31 09:23:28 2022
train Loss: 0.48

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 09:23:46 2022
batch 10, val loss = 0.03, mean loss = 0.02
Mon Jan 31 09:25:16 2022
val Loss: 0.03

epoch 68 was done for 586.390665 seconds
Epoch 69/299
----------
Mon Jan 31 09:25:37 2022
batch 0, train loss = 0.14, mean loss = 0.14
Mon Jan 31 09:25:46 2022
batch 10, train loss = 0.50, mean loss = 0.50
Mon Jan 31 09:27:16 2022
batch 20, train loss = 0.27, mean loss = 0.50
Mon Jan 31 09:28:46 2022
batch 30, train loss = 0.77, mean loss = 0.51
Mon Jan 31 09:30:16 2022
batch 40, train loss = 0.18, mean loss = 0.52
Mon Jan 31 09:31:46 2022
batch 50, train loss = 0.57, mean loss = 0.55
Mon Jan 31 09:33:16 2022
train Loss: 0.55

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 09:33:34 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 09:35:04 2022
val Loss: 0.03

epoch 69 was done for 587.100087 seconds
Epoch 70/299
----------
Mon Jan 31 09:35:25 2022
batch 0, train loss = 0.21, mean loss = 0.21
Mon Jan 31 09:35:33 2022
batch 10, train loss = 0.57, mean loss = 0.58
Mon Jan 31 09:37:03 2022
batch 20, train loss = 0.31, mean loss = 0.57
Mon Jan 31 09:38:33 2022
batch 30, train loss = 0.83, mean loss = 0.58
Mon Jan 31 09:40:02 2022
batch 40, train loss = 0.21, mean loss = 0.59
Mon Jan 31 09:41:32 2022
batch 50, train loss = 0.65, mean loss = 0.61
Mon Jan 31 09:43:02 2022
train Loss: 0.61

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 09:43:20 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 09:44:49 2022
val Loss: 0.03

epoch 70 was done for 585.458707 seconds
Epoch 71/299
----------
Mon Jan 31 09:45:10 2022
batch 0, train loss = 0.28, mean loss = 0.28
Mon Jan 31 09:45:19 2022
batch 10, train loss = 0.69, mean loss = 0.63
Mon Jan 31 09:46:50 2022
batch 20, train loss = 0.36, mean loss = 0.62
Mon Jan 31 09:48:19 2022
batch 30, train loss = 0.88, mean loss = 0.63
Mon Jan 31 09:49:48 2022
batch 40, train loss = 0.23, mean loss = 0.64
Mon Jan 31 09:51:20 2022
batch 50, train loss = 0.73, mean loss = 0.65
Mon Jan 31 09:52:50 2022
train Loss: 0.66

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 09:53:09 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 09:54:38 2022
val Loss: 0.03

epoch 71 was done for 588.296962 seconds
Epoch 72/299
----------
Mon Jan 31 09:54:58 2022
batch 0, train loss = 0.33, mean loss = 0.33
Mon Jan 31 09:55:07 2022
batch 10, train loss = 0.82, mean loss = 0.66
Mon Jan 31 09:56:35 2022
batch 20, train loss = 0.34, mean loss = 0.64
Mon Jan 31 09:58:05 2022
batch 30, train loss = 0.85, mean loss = 0.67
Mon Jan 31 09:59:35 2022
batch 40, train loss = 0.36, mean loss = 0.69
Mon Jan 31 10:01:06 2022
batch 50, train loss = 0.90, mean loss = 0.71
Mon Jan 31 10:02:35 2022
train Loss: 0.71

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 10:02:53 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 10:04:22 2022
val Loss: 0.03

epoch 72 was done for 584.615064 seconds
Epoch 73/299
----------
Mon Jan 31 10:04:43 2022
batch 0, train loss = 0.52, mean loss = 0.52
Mon Jan 31 10:04:52 2022
batch 10, train loss = 1.09, mean loss = 0.74
Mon Jan 31 10:06:21 2022
batch 20, train loss = 0.33, mean loss = 0.72
Mon Jan 31 10:07:50 2022
batch 30, train loss = 0.91, mean loss = 0.79
Mon Jan 31 10:09:19 2022
batch 40, train loss = 0.58, mean loss = 0.83
Mon Jan 31 10:10:49 2022
batch 50, train loss = 1.13, mean loss = 0.84
Mon Jan 31 10:12:19 2022
train Loss: 0.84

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 10:12:37 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 10:14:07 2022
val Loss: 0.02

epoch 73 was done for 584.781600 seconds
Epoch 74/299
----------
Mon Jan 31 10:14:28 2022
batch 0, train loss = 0.73, mean loss = 0.73
Mon Jan 31 10:14:36 2022
batch 10, train loss = 1.15, mean loss = 0.80
Mon Jan 31 10:16:04 2022
batch 20, train loss = 0.29, mean loss = 0.77
Mon Jan 31 10:17:33 2022
batch 30, train loss = 0.84, mean loss = 0.83
Mon Jan 31 10:19:01 2022
batch 40, train loss = 0.67, mean loss = 0.86
Mon Jan 31 10:20:31 2022
batch 50, train loss = 1.23, mean loss = 0.86
Mon Jan 31 10:22:02 2022
train Loss: 0.87

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 10:22:20 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 10:23:50 2022
val Loss: 0.03

epoch 74 was done for 582.426472 seconds
Epoch 75/299
----------
Mon Jan 31 10:24:10 2022
batch 0, train loss = 0.85, mean loss = 0.85
Mon Jan 31 10:24:19 2022
batch 10, train loss = 1.20, mean loss = 0.87
Mon Jan 31 10:25:48 2022
batch 20, train loss = 0.44, mean loss = 0.89
Mon Jan 31 10:27:17 2022
batch 30, train loss = 1.24, mean loss = 0.95
Mon Jan 31 10:28:45 2022
batch 40, train loss = 0.86, mean loss = 0.97
Mon Jan 31 10:30:14 2022
batch 50, train loss = 1.11, mean loss = 0.99
Mon Jan 31 10:31:42 2022
train Loss: 1.00

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 10:32:00 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 10:33:28 2022
val Loss: 0.03

epoch 75 was done for 578.712991 seconds
Epoch 76/299
----------
Mon Jan 31 10:33:49 2022
batch 0, train loss = 0.69, mean loss = 0.69
Mon Jan 31 10:33:57 2022
batch 10, train loss = 0.88, mean loss = 1.14
Mon Jan 31 10:35:26 2022
batch 20, train loss = 1.10, mean loss = 1.16
Mon Jan 31 10:36:56 2022
batch 30, train loss = 1.59, mean loss = 1.09
Mon Jan 31 10:38:23 2022
batch 40, train loss = 0.40, mean loss = 1.04
Mon Jan 31 10:39:51 2022
batch 50, train loss = 0.75, mean loss = 1.08
Mon Jan 31 10:41:19 2022
train Loss: 1.07

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 10:41:37 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 10:43:06 2022
val Loss: 0.03

epoch 76 was done for 577.407119 seconds
Epoch 77/299
----------
Mon Jan 31 10:43:26 2022
batch 0, train loss = 0.45, mean loss = 0.45
Mon Jan 31 10:43:35 2022
batch 10, train loss = 1.05, mean loss = 1.13
Mon Jan 31 10:45:03 2022
batch 20, train loss = 0.94, mean loss = 1.00
Mon Jan 31 10:46:31 2022
batch 30, train loss = 0.83, mean loss = 1.00
Mon Jan 31 10:47:59 2022
batch 40, train loss = 0.77, mean loss = 1.01
Mon Jan 31 10:49:28 2022
batch 50, train loss = 1.34, mean loss = 0.98
Mon Jan 31 10:50:57 2022
train Loss: 0.99

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 10:51:14 2022
