The activation script must be sourced, otherwise the virtual environment will not work.
Setting vars
iscuda= True
iscuda= True
iscuda= True
opt.wandb =  
opt.wandb =  
iscuda= True
opt.wandb =  
93
93
93
file to frame csv ../../csv/598frame.csv
file to frame csv ../../csv/598frame.csv
file to frame csv ../../csv/598frame.csv
PyTorch Version:  1.8.1
Torchvision Version:  0.9.0a0
opt:
 Namespace(ampl=441, aug_gt='orient', batch_output=2, bs=25, cencrop=700, center=False, chidden_dim=[96, 128, 256, 256, 256], classicnorm=False, cmscrop=0, conTrain='', criterion='L2', csvname='598csv9', datapath='/p/project/delia-mp/cherepashkin1/phenoseed/', dfname='598frame', downsample=1, epoch=300, expdescr='', expnum='e068', feature_extract=False, framelim=6000, gradient_predivide_factor=1.0, gttype='single_file', haf=True, hidden_dim=[9], inputt='img', kernel_sizes=[7, 3, 3, 3, 3, 3], lb='orient', loadh5=False, localexp='', lr=5e-05, machine='jureca', maintain=False, maintain_line=False, man_dist=False, measure_time=False, merging='batch', minmax=False, minmax3dimage=False, minmax_f=True, minmax_fn='', model_name='', netname=['cnet'], ngpu=4, noise_input=False, noise_output=False, normalize=False, num_input_images=3, num_sam_points=500, num_workers=0, outputt='orient', parallel='horovod', pin_memory=False, print_minibatch=10, pscale=100, rand_angle=False, rescale=500, rot_dirs=False, rotate_output=False, save_output=True, single_folder=False, specie='598', standardize=255, steplr=[1000.0, 1.0], transappendix='_image', ufmodel=100000, updateFraction=0.25, use_adasum=False, use_cuda=True, use_existing_csv=True, use_pretrained=False, use_sep_csv=True, view_sep=False, wandb='', weight_decay=0, zero_angle=True)
sys.argv:
 ['../../main.py', '-datapath', '/p/project/delia-mp/cherepashkin1/phenoseed/', '-epoch', '300', '-bs', '25', '-num_input_images', '3', '-framelim', '6000', '-criterion', 'L2', '-localexp', '', '-lr', '5e-5', '-expnum', 'e068', '-hidden_dim', '9', '-inputt', 'img', '-outputt', 'orient', '-lb', 'orient', '-no_loadh5', '-minmax_fn', '', '-parallel', 'horovod', '-machine', 'jureca', '-merging', 'batch', '-aug_gt', 'orient', '-updateFraction', '0.25', '-steplr', '1000', '1', '-print_minibatch', '10', '-dfname', '598frame']
seed =  0
path were main.py is located= ../../
opt.wandb =  
93
file to frame csv ../../csv/598frame.csv
lframe's length after laoding =  5283
lframe's length after laoding =  5283
lframe's length after laoding =  5283
lframe's length after laoding =  5283
lframe len after excluding all exceptions= 5200
lframe len after excluding all exceptions= 5200
lframe len after excluding all exceptions= 5200
lframe len after excluding all exceptions= 5200
len train =  4160
len train =  4160
len train =  4160
len train =  4160
train consists of 166 full batches with 25 tensors with 3 views
the last batch has size of 10 tensors with 3 views
val consists of 41 full batches with 25 tensors with 3 views
the last batch has size of 15 tensors with 3 views
Epoch 0/299
----------
Mon Jan 31 00:15:03 2022
batch 0, train loss = 0.53, mean loss = 0.53
Mon Jan 31 00:15:16 2022
batch 10, train loss = 15.74, mean loss = 34.68
Mon Jan 31 00:16:49 2022
batch 20, train loss = 16.70, mean loss = 28.28
Mon Jan 31 00:18:21 2022
batch 30, train loss = 8.89, mean loss = 20.87
Mon Jan 31 00:19:51 2022
batch 40, train loss = 1.73, mean loss = 16.93
Mon Jan 31 00:21:20 2022
train Loss: 16.61

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 00:21:34 2022
batch 10, val loss = 0.04, mean loss = 0.04
Mon Jan 31 00:22:58 2022
val Loss: 0.04

epoch 0 was done for 477.992321 seconds
Epoch 1/299
----------
Mon Jan 31 00:23:01 2022
batch 0, train loss = 3.58, mean loss = 3.58
Mon Jan 31 00:23:10 2022
batch 10, train loss = 1.65, mean loss = 2.84
Mon Jan 31 00:24:39 2022
batch 20, train loss = 2.33, mean loss = 2.47
Mon Jan 31 00:26:08 2022
batch 30, train loss = 1.54, mean loss = 2.16
Mon Jan 31 00:27:37 2022
batch 40, train loss = 1.15, mean loss = 1.94
Mon Jan 31 00:29:06 2022
train Loss: 1.92

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:29:20 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:30:45 2022
val Loss: 0.02

epoch 1 was done for 467.016191 seconds
Epoch 2/299
----------
Mon Jan 31 00:30:48 2022
batch 0, train loss = 0.85, mean loss = 0.85
Mon Jan 31 00:30:57 2022
batch 10, train loss = 0.87, mean loss = 1.09
Mon Jan 31 00:32:27 2022
batch 20, train loss = 0.80, mean loss = 0.93
Mon Jan 31 00:33:57 2022
batch 30, train loss = 0.52, mean loss = 0.89
Mon Jan 31 00:35:27 2022
batch 40, train loss = 0.67, mean loss = 0.81
Mon Jan 31 00:36:57 2022
train Loss: 0.81

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:37:11 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 00:38:37 2022
val Loss: 0.03

epoch 2 was done for 471.969456 seconds
Epoch 3/299
----------
Mon Jan 31 00:38:40 2022
batch 0, train loss = 0.55, mean loss = 0.55
Mon Jan 31 00:38:49 2022
batch 10, train loss = 0.50, mean loss = 0.46
Mon Jan 31 00:40:18 2022
batch 20, train loss = 0.48, mean loss = 0.50
Mon Jan 31 00:41:49 2022
batch 30, train loss = 0.47, mean loss = 0.47
Mon Jan 31 00:43:18 2022
batch 40, train loss = 0.36, mean loss = 0.46
Mon Jan 31 00:44:48 2022
train Loss: 0.46

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:45:02 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:46:27 2022
val Loss: 0.02

epoch 3 was done for 469.986963 seconds
Epoch 4/299
----------
Mon Jan 31 00:46:30 2022
batch 0, train loss = 0.29, mean loss = 0.29
Mon Jan 31 00:46:39 2022
batch 10, train loss = 0.37, mean loss = 0.40
Mon Jan 31 00:48:08 2022
batch 20, train loss = 0.37, mean loss = 0.38
Mon Jan 31 00:49:38 2022
batch 30, train loss = 0.34, mean loss = 0.38
Mon Jan 31 00:51:07 2022
batch 40, train loss = 0.35, mean loss = 0.37
Mon Jan 31 00:52:37 2022
train Loss: 0.37

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:52:51 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:54:16 2022
val Loss: 0.02

epoch 4 was done for 469.216513 seconds
Epoch 5/299
----------
Mon Jan 31 00:54:19 2022
batch 0, train loss = 0.30, mean loss = 0.30
Mon Jan 31 00:54:28 2022
batch 10, train loss = 0.37, mean loss = 0.34
Mon Jan 31 00:55:58 2022
batch 20, train loss = 0.31, mean loss = 0.34
Mon Jan 31 00:57:28 2022
batch 30, train loss = 0.35, mean loss = 0.34
Mon Jan 31 00:58:57 2022
batch 40, train loss = 0.31, mean loss = 0.34
Mon Jan 31 01:00:27 2022
train Loss: 0.34

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:00:42 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:02:07 2022
val Loss: 0.02

epoch 5 was done for 470.229602 seconds
Epoch 6/299
----------
Mon Jan 31 01:02:09 2022
batch 0, train loss = 0.26, mean loss = 0.26
Mon Jan 31 01:02:18 2022
batch 10, train loss = 0.34, mean loss = 0.33
Mon Jan 31 01:03:47 2022
batch 20, train loss = 0.30, mean loss = 0.33
Mon Jan 31 01:05:17 2022
batch 30, train loss = 0.34, mean loss = 0.33
Mon Jan 31 01:06:46 2022
batch 40, train loss = 0.30, mean loss = 0.33
Mon Jan 31 01:08:16 2022
train Loss: 0.33

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:08:30 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:09:55 2022
val Loss: 0.02

epoch 6 was done for 468.105882 seconds
Epoch 7/299
----------
Mon Jan 31 01:09:57 2022
batch 0, train loss = 0.26, mean loss = 0.26
Mon Jan 31 01:10:06 2022
batch 10, train loss = 0.34, mean loss = 0.32
Mon Jan 31 01:11:36 2022
batch 20, train loss = 0.29, mean loss = 0.32
Mon Jan 31 01:13:06 2022
batch 30, train loss = 0.33, mean loss = 0.32
Mon Jan 31 01:14:35 2022
batch 40, train loss = 0.29, mean loss = 0.32
Mon Jan 31 01:16:05 2022
train Loss: 0.32

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:16:19 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:17:44 2022
val Loss: 0.02

epoch 7 was done for 469.705977 seconds
Epoch 8/299
----------
Mon Jan 31 01:17:47 2022
batch 0, train loss = 0.25, mean loss = 0.25
Mon Jan 31 01:17:56 2022
batch 10, train loss = 0.33, mean loss = 0.31
Mon Jan 31 01:19:26 2022
batch 20, train loss = 0.28, mean loss = 0.31
Mon Jan 31 01:20:56 2022
batch 30, train loss = 0.33, mean loss = 0.32
Mon Jan 31 01:22:26 2022
batch 40, train loss = 0.29, mean loss = 0.32
Mon Jan 31 01:23:56 2022
train Loss: 0.32

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:24:10 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:25:35 2022
val Loss: 0.02

epoch 8 was done for 470.884748 seconds
Epoch 9/299
----------
Mon Jan 31 01:25:38 2022
batch 0, train loss = 0.25, mean loss = 0.25
Mon Jan 31 01:25:47 2022
batch 10, train loss = 0.34, mean loss = 0.31
Mon Jan 31 01:27:16 2022
batch 20, train loss = 0.27, mean loss = 0.31
Mon Jan 31 01:28:47 2022
batch 30, train loss = 0.33, mean loss = 0.31
Mon Jan 31 01:30:16 2022
batch 40, train loss = 0.28, mean loss = 0.31
Mon Jan 31 01:31:47 2022
train Loss: 0.32

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:32:01 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:33:25 2022
val Loss: 0.02

epoch 9 was done for 470.123173 seconds
Epoch 10/299
----------
Mon Jan 31 01:33:28 2022
batch 0, train loss = 0.25, mean loss = 0.25
Mon Jan 31 01:33:37 2022
batch 10, train loss = 0.33, mean loss = 0.31
Mon Jan 31 01:35:07 2022
batch 20, train loss = 0.27, mean loss = 0.31
Mon Jan 31 01:36:38 2022
batch 30, train loss = 0.33, mean loss = 0.31
Mon Jan 31 01:38:07 2022
batch 40, train loss = 0.28, mean loss = 0.31
Mon Jan 31 01:39:38 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:39:52 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:41:17 2022
val Loss: 0.02

epoch 10 was done for 471.472208 seconds
Epoch 11/299
----------
Mon Jan 31 01:41:20 2022
batch 0, train loss = 0.25, mean loss = 0.25
Mon Jan 31 01:41:29 2022
batch 10, train loss = 0.33, mean loss = 0.31
Mon Jan 31 01:42:58 2022
batch 20, train loss = 0.27, mean loss = 0.31
Mon Jan 31 01:44:28 2022
batch 30, train loss = 0.33, mean loss = 0.31
Mon Jan 31 01:45:57 2022
batch 40, train loss = 0.27, mean loss = 0.31
Mon Jan 31 01:47:26 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:47:41 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:49:06 2022
val Loss: 0.02

epoch 11 was done for 469.138372 seconds
Epoch 12/299
----------
Mon Jan 31 01:49:09 2022
batch 0, train loss = 0.25, mean loss = 0.25
Mon Jan 31 01:49:18 2022
batch 10, train loss = 0.33, mean loss = 0.31
Mon Jan 31 01:50:47 2022
batch 20, train loss = 0.26, mean loss = 0.31
Mon Jan 31 01:52:17 2022
batch 30, train loss = 0.33, mean loss = 0.31
Mon Jan 31 01:53:46 2022
batch 40, train loss = 0.27, mean loss = 0.31
Mon Jan 31 01:55:17 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:55:31 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:56:56 2022
val Loss: 0.02

epoch 12 was done for 469.662513 seconds
Epoch 13/299
----------
Mon Jan 31 01:56:58 2022
batch 0, train loss = 0.24, mean loss = 0.24
Mon Jan 31 01:57:07 2022
batch 10, train loss = 0.33, mean loss = 0.31
Mon Jan 31 01:58:37 2022
batch 20, train loss = 0.26, mean loss = 0.31
Mon Jan 31 02:00:07 2022
batch 30, train loss = 0.33, mean loss = 0.31
Mon Jan 31 02:01:36 2022
batch 40, train loss = 0.27, mean loss = 0.31
Mon Jan 31 02:03:06 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:03:21 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:04:46 2022
val Loss: 0.02

epoch 13 was done for 469.999097 seconds
Epoch 14/299
----------
Mon Jan 31 02:04:48 2022
batch 0, train loss = 0.24, mean loss = 0.24
Mon Jan 31 02:04:57 2022
batch 10, train loss = 0.33, mean loss = 0.31
Mon Jan 31 02:06:27 2022
batch 20, train loss = 0.25, mean loss = 0.31
Mon Jan 31 02:07:57 2022
batch 30, train loss = 0.33, mean loss = 0.31
Mon Jan 31 02:09:27 2022
batch 40, train loss = 0.27, mean loss = 0.31
Mon Jan 31 02:10:57 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:11:11 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:12:37 2022
val Loss: 0.02

epoch 14 was done for 471.152618 seconds
Epoch 15/299
----------
Mon Jan 31 02:12:40 2022
batch 0, train loss = 0.24, mean loss = 0.24
Mon Jan 31 02:12:49 2022
batch 10, train loss = 0.33, mean loss = 0.31
Mon Jan 31 02:14:19 2022
batch 20, train loss = 0.25, mean loss = 0.31
Mon Jan 31 02:15:50 2022
batch 30, train loss = 0.33, mean loss = 0.31
Mon Jan 31 02:17:20 2022
batch 40, train loss = 0.26, mean loss = 0.31
Mon Jan 31 02:18:53 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:19:07 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:20:32 2022
val Loss: 0.02

epoch 15 was done for 474.938116 seconds
Epoch 16/299
----------
Mon Jan 31 02:20:34 2022
batch 0, train loss = 0.24, mean loss = 0.24
Mon Jan 31 02:20:43 2022
batch 10, train loss = 0.33, mean loss = 0.31
Mon Jan 31 02:22:13 2022
batch 20, train loss = 0.25, mean loss = 0.31
Mon Jan 31 02:23:44 2022
batch 30, train loss = 0.33, mean loss = 0.31
Mon Jan 31 02:25:14 2022
batch 40, train loss = 0.26, mean loss = 0.31
Mon Jan 31 02:26:44 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:26:59 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:28:24 2022
val Loss: 0.02

epoch 16 was done for 471.999567 seconds
Epoch 17/299
----------
Mon Jan 31 02:28:26 2022
batch 0, train loss = 0.23, mean loss = 0.23
Mon Jan 31 02:28:35 2022
batch 10, train loss = 0.34, mean loss = 0.31
Mon Jan 31 02:30:05 2022
batch 20, train loss = 0.24, mean loss = 0.31
Mon Jan 31 02:31:36 2022
batch 30, train loss = 0.33, mean loss = 0.31
Mon Jan 31 02:33:06 2022
batch 40, train loss = 0.26, mean loss = 0.31
Mon Jan 31 02:34:36 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:34:50 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:36:16 2022
val Loss: 0.02

epoch 17 was done for 472.136684 seconds
Epoch 18/299
----------
Mon Jan 31 02:36:19 2022
batch 0, train loss = 0.23, mean loss = 0.23
Mon Jan 31 02:36:28 2022
batch 10, train loss = 0.34, mean loss = 0.31
Mon Jan 31 02:37:57 2022
batch 20, train loss = 0.24, mean loss = 0.31
Mon Jan 31 02:39:28 2022
batch 30, train loss = 0.33, mean loss = 0.31
Mon Jan 31 02:40:58 2022
batch 40, train loss = 0.25, mean loss = 0.31
Mon Jan 31 02:42:28 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:42:43 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:44:07 2022
val Loss: 0.02

epoch 18 was done for 470.935413 seconds
Epoch 19/299
----------
Mon Jan 31 02:44:10 2022
batch 0, train loss = 0.23, mean loss = 0.23
Mon Jan 31 02:44:18 2022
batch 10, train loss = 0.34, mean loss = 0.31
Mon Jan 31 02:45:48 2022
batch 20, train loss = 0.24, mean loss = 0.31
Mon Jan 31 02:47:19 2022
batch 30, train loss = 0.34, mean loss = 0.31
Mon Jan 31 02:48:48 2022
batch 40, train loss = 0.25, mean loss = 0.31
Mon Jan 31 02:50:19 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:50:33 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:51:58 2022
val Loss: 0.02

epoch 19 was done for 471.545414 seconds
Epoch 20/299
----------
Mon Jan 31 02:52:01 2022
batch 0, train loss = 0.23, mean loss = 0.23
Mon Jan 31 02:52:10 2022
batch 10, train loss = 0.34, mean loss = 0.31
Mon Jan 31 02:53:40 2022
batch 20, train loss = 0.23, mean loss = 0.30
Mon Jan 31 02:55:11 2022
batch 30, train loss = 0.34, mean loss = 0.31
Mon Jan 31 02:56:40 2022
batch 40, train loss = 0.25, mean loss = 0.31
Mon Jan 31 02:58:11 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:58:25 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:59:51 2022
val Loss: 0.02

epoch 20 was done for 472.383019 seconds
Epoch 21/299
----------
Mon Jan 31 02:59:53 2022
batch 0, train loss = 0.22, mean loss = 0.22
Mon Jan 31 03:00:02 2022
batch 10, train loss = 0.34, mean loss = 0.31
Mon Jan 31 03:01:32 2022
batch 20, train loss = 0.23, mean loss = 0.30
Mon Jan 31 03:03:03 2022
batch 30, train loss = 0.34, mean loss = 0.31
Mon Jan 31 03:04:33 2022
batch 40, train loss = 0.24, mean loss = 0.31
Mon Jan 31 03:06:03 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:06:18 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:07:43 2022
val Loss: 0.02

epoch 21 was done for 471.669547 seconds
Epoch 22/299
----------
Mon Jan 31 03:07:45 2022
batch 0, train loss = 0.22, mean loss = 0.22
Mon Jan 31 03:07:54 2022
batch 10, train loss = 0.34, mean loss = 0.31
Mon Jan 31 03:09:24 2022
batch 20, train loss = 0.23, mean loss = 0.30
Mon Jan 31 03:10:55 2022
batch 30, train loss = 0.34, mean loss = 0.31
Mon Jan 31 03:12:24 2022
batch 40, train loss = 0.24, mean loss = 0.31
Mon Jan 31 03:13:54 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:14:09 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:15:33 2022
val Loss: 0.02

epoch 22 was done for 470.776699 seconds
Epoch 23/299
----------
Mon Jan 31 03:15:36 2022
batch 0, train loss = 0.22, mean loss = 0.22
Mon Jan 31 03:15:45 2022
batch 10, train loss = 0.34, mean loss = 0.31
Mon Jan 31 03:17:15 2022
batch 20, train loss = 0.22, mean loss = 0.30
Mon Jan 31 03:18:45 2022
batch 30, train loss = 0.34, mean loss = 0.31
Mon Jan 31 03:20:15 2022
batch 40, train loss = 0.24, mean loss = 0.31
Mon Jan 31 03:21:45 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:22:00 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:23:25 2022
val Loss: 0.02

epoch 23 was done for 471.320475 seconds
Epoch 24/299
----------
Mon Jan 31 03:23:27 2022
batch 0, train loss = 0.21, mean loss = 0.21
Mon Jan 31 03:23:36 2022
batch 10, train loss = 0.34, mean loss = 0.31
Mon Jan 31 03:25:06 2022
batch 20, train loss = 0.22, mean loss = 0.30
Mon Jan 31 03:26:36 2022
batch 30, train loss = 0.34, mean loss = 0.31
Mon Jan 31 03:28:06 2022
batch 40, train loss = 0.23, mean loss = 0.31
Mon Jan 31 03:29:37 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:29:51 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:31:16 2022
val Loss: 0.02

epoch 24 was done for 471.577379 seconds
Epoch 25/299
----------
Mon Jan 31 03:31:19 2022
batch 0, train loss = 0.21, mean loss = 0.21
Mon Jan 31 03:31:28 2022
batch 10, train loss = 0.34, mean loss = 0.31
Mon Jan 31 03:32:58 2022
batch 20, train loss = 0.21, mean loss = 0.30
Mon Jan 31 03:34:28 2022
batch 30, train loss = 0.34, mean loss = 0.31
Mon Jan 31 03:35:58 2022
batch 40, train loss = 0.23, mean loss = 0.31
Mon Jan 31 03:37:29 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:37:43 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:39:08 2022
val Loss: 0.02

epoch 25 was done for 471.453048 seconds
Epoch 26/299
----------
Mon Jan 31 03:39:10 2022
batch 0, train loss = 0.20, mean loss = 0.20
Mon Jan 31 03:39:19 2022
batch 10, train loss = 0.34, mean loss = 0.31
Mon Jan 31 03:40:49 2022
batch 20, train loss = 0.21, mean loss = 0.30
Mon Jan 31 03:42:19 2022
batch 30, train loss = 0.34, mean loss = 0.31
Mon Jan 31 03:43:49 2022
batch 40, train loss = 0.22, mean loss = 0.31
Mon Jan 31 03:45:19 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:45:34 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:47:00 2022
val Loss: 0.02

epoch 26 was done for 471.836655 seconds
Epoch 27/299
----------
Mon Jan 31 03:47:02 2022
batch 0, train loss = 0.20, mean loss = 0.20
Mon Jan 31 03:47:11 2022
batch 10, train loss = 0.34, mean loss = 0.31
Mon Jan 31 03:48:41 2022
batch 20, train loss = 0.20, mean loss = 0.30
Mon Jan 31 03:50:12 2022
batch 30, train loss = 0.34, mean loss = 0.31
Mon Jan 31 03:51:41 2022
batch 40, train loss = 0.22, mean loss = 0.31
Mon Jan 31 03:53:12 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:53:26 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:54:51 2022
val Loss: 0.02

epoch 27 was done for 471.279542 seconds
Epoch 28/299
----------
Mon Jan 31 03:54:53 2022
batch 0, train loss = 0.20, mean loss = 0.20
Mon Jan 31 03:55:02 2022
batch 10, train loss = 0.35, mean loss = 0.31
Mon Jan 31 03:56:33 2022
batch 20, train loss = 0.20, mean loss = 0.30
Mon Jan 31 03:58:03 2022
batch 30, train loss = 0.34, mean loss = 0.31
Mon Jan 31 03:59:33 2022
batch 40, train loss = 0.21, mean loss = 0.31
Mon Jan 31 04:01:03 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:01:17 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:02:42 2022
val Loss: 0.02

epoch 28 was done for 471.356440 seconds
Epoch 29/299
----------
Mon Jan 31 04:02:45 2022
batch 0, train loss = 0.19, mean loss = 0.19
Mon Jan 31 04:02:54 2022
batch 10, train loss = 0.35, mean loss = 0.31
Mon Jan 31 04:04:23 2022
batch 20, train loss = 0.19, mean loss = 0.30
Mon Jan 31 04:05:53 2022
batch 30, train loss = 0.35, mean loss = 0.30
Mon Jan 31 04:07:23 2022
batch 40, train loss = 0.21, mean loss = 0.31
Mon Jan 31 04:08:53 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:09:08 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:10:34 2022
val Loss: 0.02

epoch 29 was done for 471.757883 seconds
Epoch 30/299
----------
Mon Jan 31 04:10:36 2022
batch 0, train loss = 0.19, mean loss = 0.19
Mon Jan 31 04:10:46 2022
batch 10, train loss = 0.35, mean loss = 0.30
Mon Jan 31 04:12:15 2022
batch 20, train loss = 0.19, mean loss = 0.30
Mon Jan 31 04:13:45 2022
batch 30, train loss = 0.35, mean loss = 0.30
Mon Jan 31 04:15:15 2022
batch 40, train loss = 0.21, mean loss = 0.31
Mon Jan 31 04:16:45 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:16:59 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:18:25 2022
val Loss: 0.02

epoch 30 was done for 470.773740 seconds
Epoch 31/299
----------
Mon Jan 31 04:18:27 2022
batch 0, train loss = 0.18, mean loss = 0.18
Mon Jan 31 04:18:36 2022
batch 10, train loss = 0.35, mean loss = 0.30
Mon Jan 31 04:20:06 2022
batch 20, train loss = 0.18, mean loss = 0.30
Mon Jan 31 04:21:36 2022
batch 30, train loss = 0.35, mean loss = 0.30
Mon Jan 31 04:23:06 2022
batch 40, train loss = 0.20, mean loss = 0.30
Mon Jan 31 04:24:36 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:24:50 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:26:15 2022
val Loss: 0.02

epoch 31 was done for 470.506348 seconds
Epoch 32/299
----------
Mon Jan 31 04:26:18 2022
batch 0, train loss = 0.18, mean loss = 0.18
Mon Jan 31 04:26:27 2022
batch 10, train loss = 0.35, mean loss = 0.30
Mon Jan 31 04:27:56 2022
batch 20, train loss = 0.18, mean loss = 0.30
Mon Jan 31 04:29:26 2022
batch 30, train loss = 0.35, mean loss = 0.30
Mon Jan 31 04:30:56 2022
batch 40, train loss = 0.20, mean loss = 0.30
Mon Jan 31 04:32:27 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:32:42 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:34:07 2022
val Loss: 0.02

epoch 32 was done for 472.171301 seconds
Epoch 33/299
----------
Mon Jan 31 04:34:10 2022
batch 0, train loss = 0.18, mean loss = 0.18
Mon Jan 31 04:34:19 2022
batch 10, train loss = 0.35, mean loss = 0.30
Mon Jan 31 04:35:49 2022
batch 20, train loss = 0.17, mean loss = 0.30
Mon Jan 31 04:37:19 2022
batch 30, train loss = 0.35, mean loss = 0.30
Mon Jan 31 04:38:48 2022
batch 40, train loss = 0.19, mean loss = 0.30
Mon Jan 31 04:40:19 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:40:33 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:41:58 2022
val Loss: 0.02

epoch 33 was done for 470.989435 seconds
Epoch 34/299
----------
Mon Jan 31 04:42:01 2022
batch 0, train loss = 0.17, mean loss = 0.17
Mon Jan 31 04:42:10 2022
batch 10, train loss = 0.36, mean loss = 0.30
Mon Jan 31 04:43:40 2022
batch 20, train loss = 0.17, mean loss = 0.30
Mon Jan 31 04:45:10 2022
batch 30, train loss = 0.35, mean loss = 0.30
Mon Jan 31 04:46:40 2022
batch 40, train loss = 0.19, mean loss = 0.30
Mon Jan 31 04:48:11 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:48:25 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:49:50 2022
val Loss: 0.02

epoch 34 was done for 472.015308 seconds
Epoch 35/299
----------
Mon Jan 31 04:49:53 2022
batch 0, train loss = 0.17, mean loss = 0.17
Mon Jan 31 04:50:02 2022
batch 10, train loss = 0.36, mean loss = 0.30
Mon Jan 31 04:51:32 2022
batch 20, train loss = 0.16, mean loss = 0.30
Mon Jan 31 04:53:02 2022
batch 30, train loss = 0.35, mean loss = 0.30
Mon Jan 31 04:54:32 2022
batch 40, train loss = 0.18, mean loss = 0.30
Mon Jan 31 04:56:02 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:56:17 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:57:43 2022
val Loss: 0.02

epoch 35 was done for 472.575349 seconds
Epoch 36/299
----------
Mon Jan 31 04:57:46 2022
batch 0, train loss = 0.16, mean loss = 0.16
Mon Jan 31 04:57:55 2022
batch 10, train loss = 0.36, mean loss = 0.30
Mon Jan 31 04:59:25 2022
batch 20, train loss = 0.16, mean loss = 0.30
Mon Jan 31 05:00:55 2022
batch 30, train loss = 0.35, mean loss = 0.30
Mon Jan 31 05:02:25 2022
batch 40, train loss = 0.18, mean loss = 0.30
Mon Jan 31 05:03:55 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:04:10 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:05:35 2022
val Loss: 0.02

epoch 36 was done for 471.926157 seconds
Epoch 37/299
----------
Mon Jan 31 05:05:37 2022
batch 0, train loss = 0.16, mean loss = 0.16
Mon Jan 31 05:05:46 2022
batch 10, train loss = 0.36, mean loss = 0.30
Mon Jan 31 05:07:16 2022
batch 20, train loss = 0.15, mean loss = 0.30
Mon Jan 31 05:08:47 2022
batch 30, train loss = 0.36, mean loss = 0.30
Mon Jan 31 05:10:17 2022
batch 40, train loss = 0.17, mean loss = 0.30
Mon Jan 31 05:11:47 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:12:02 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:13:28 2022
val Loss: 0.02

epoch 37 was done for 472.667719 seconds
Epoch 38/299
----------
Mon Jan 31 05:13:30 2022
batch 0, train loss = 0.16, mean loss = 0.16
Mon Jan 31 05:13:39 2022
batch 10, train loss = 0.36, mean loss = 0.30
Mon Jan 31 05:15:09 2022
batch 20, train loss = 0.15, mean loss = 0.30
Mon Jan 31 05:16:40 2022
batch 30, train loss = 0.36, mean loss = 0.30
Mon Jan 31 05:18:10 2022
batch 40, train loss = 0.17, mean loss = 0.30
Mon Jan 31 05:19:40 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:19:55 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:21:21 2022
val Loss: 0.02

epoch 38 was done for 473.655882 seconds
Epoch 39/299
----------
Mon Jan 31 05:21:24 2022
batch 0, train loss = 0.15, mean loss = 0.15
Mon Jan 31 05:21:33 2022
batch 10, train loss = 0.37, mean loss = 0.30
Mon Jan 31 05:23:03 2022
batch 20, train loss = 0.14, mean loss = 0.30
Mon Jan 31 05:24:34 2022
batch 30, train loss = 0.36, mean loss = 0.30
Mon Jan 31 05:26:04 2022
batch 40, train loss = 0.16, mean loss = 0.30
Mon Jan 31 05:27:34 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:27:48 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:29:14 2022
val Loss: 0.02

epoch 39 was done for 472.449070 seconds
Epoch 40/299
----------
Mon Jan 31 05:29:16 2022
batch 0, train loss = 0.15, mean loss = 0.15
Mon Jan 31 05:29:25 2022
batch 10, train loss = 0.37, mean loss = 0.30
Mon Jan 31 05:30:56 2022
batch 20, train loss = 0.14, mean loss = 0.30
Mon Jan 31 05:32:26 2022
batch 30, train loss = 0.36, mean loss = 0.30
Mon Jan 31 05:33:56 2022
batch 40, train loss = 0.16, mean loss = 0.30
Mon Jan 31 05:35:26 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:35:40 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:37:06 2022
val Loss: 0.02

epoch 40 was done for 472.074535 seconds
Epoch 41/299
----------
Mon Jan 31 05:37:08 2022
batch 0, train loss = 0.14, mean loss = 0.14
Mon Jan 31 05:37:17 2022
batch 10, train loss = 0.37, mean loss = 0.30
Mon Jan 31 05:38:46 2022
batch 20, train loss = 0.13, mean loss = 0.30
Mon Jan 31 05:40:17 2022
batch 30, train loss = 0.36, mean loss = 0.30
Mon Jan 31 05:41:47 2022
batch 40, train loss = 0.15, mean loss = 0.30
Mon Jan 31 05:43:17 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:43:32 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:44:58 2022
val Loss: 0.02

epoch 41 was done for 472.185802 seconds
Epoch 42/299
----------
Mon Jan 31 05:45:00 2022
batch 0, train loss = 0.14, mean loss = 0.14
Mon Jan 31 05:45:09 2022
batch 10, train loss = 0.38, mean loss = 0.30
Mon Jan 31 05:46:39 2022
batch 20, train loss = 0.13, mean loss = 0.30
Mon Jan 31 05:48:09 2022
batch 30, train loss = 0.37, mean loss = 0.30
Mon Jan 31 05:49:38 2022
batch 40, train loss = 0.14, mean loss = 0.30
Mon Jan 31 05:51:08 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:51:23 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:52:48 2022
val Loss: 0.02

epoch 42 was done for 469.847559 seconds
Epoch 43/299
----------
Mon Jan 31 05:52:50 2022
batch 0, train loss = 0.13, mean loss = 0.13
Mon Jan 31 05:52:59 2022
batch 10, train loss = 0.38, mean loss = 0.30
Mon Jan 31 05:54:29 2022
batch 20, train loss = 0.12, mean loss = 0.30
Mon Jan 31 05:55:59 2022
batch 30, train loss = 0.37, mean loss = 0.30
Mon Jan 31 05:57:29 2022
batch 40, train loss = 0.14, mean loss = 0.30
Mon Jan 31 05:58:59 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:59:13 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:00:39 2022
val Loss: 0.02

epoch 43 was done for 470.697686 seconds
Epoch 44/299
----------
Mon Jan 31 06:00:41 2022
batch 0, train loss = 0.13, mean loss = 0.13
Mon Jan 31 06:00:50 2022
batch 10, train loss = 0.38, mean loss = 0.30
Mon Jan 31 06:02:20 2022
batch 20, train loss = 0.12, mean loss = 0.30
Mon Jan 31 06:03:49 2022
batch 30, train loss = 0.37, mean loss = 0.30
Mon Jan 31 06:05:19 2022
batch 40, train loss = 0.13, mean loss = 0.31
Mon Jan 31 06:06:49 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:07:03 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:08:29 2022
val Loss: 0.02

epoch 44 was done for 470.397150 seconds
Epoch 45/299
----------
Mon Jan 31 06:08:31 2022
batch 0, train loss = 0.12, mean loss = 0.12
Mon Jan 31 06:08:40 2022
batch 10, train loss = 0.39, mean loss = 0.30
Mon Jan 31 06:10:10 2022
batch 20, train loss = 0.11, mean loss = 0.30
Mon Jan 31 06:11:40 2022
batch 30, train loss = 0.37, mean loss = 0.31
Mon Jan 31 06:13:10 2022
batch 40, train loss = 0.13, mean loss = 0.31
Mon Jan 31 06:14:40 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:14:54 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:16:20 2022
val Loss: 0.02

epoch 45 was done for 470.901035 seconds
Epoch 46/299
----------
Mon Jan 31 06:16:22 2022
batch 0, train loss = 0.12, mean loss = 0.12
Mon Jan 31 06:16:31 2022
batch 10, train loss = 0.39, mean loss = 0.30
Mon Jan 31 06:18:01 2022
batch 20, train loss = 0.11, mean loss = 0.30
Mon Jan 31 06:19:31 2022
batch 30, train loss = 0.38, mean loss = 0.31
Mon Jan 31 06:21:00 2022
batch 40, train loss = 0.12, mean loss = 0.31
Mon Jan 31 06:22:30 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:22:44 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:24:10 2022
val Loss: 0.02

epoch 46 was done for 470.486876 seconds
Epoch 47/299
----------
Mon Jan 31 06:24:13 2022
batch 0, train loss = 0.11, mean loss = 0.11
Mon Jan 31 06:24:22 2022
batch 10, train loss = 0.39, mean loss = 0.30
Mon Jan 31 06:25:51 2022
batch 20, train loss = 0.10, mean loss = 0.30
Mon Jan 31 06:27:22 2022
batch 30, train loss = 0.38, mean loss = 0.31
Mon Jan 31 06:28:52 2022
batch 40, train loss = 0.12, mean loss = 0.31
Mon Jan 31 06:30:22 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:30:37 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:32:03 2022
val Loss: 0.02

epoch 47 was done for 472.245661 seconds
Epoch 48/299
----------
Mon Jan 31 06:32:05 2022
batch 0, train loss = 0.11, mean loss = 0.11
Mon Jan 31 06:32:14 2022
batch 10, train loss = 0.40, mean loss = 0.31
Mon Jan 31 06:33:49 2022
batch 20, train loss = 0.10, mean loss = 0.30
Mon Jan 31 06:35:34 2022
batch 30, train loss = 0.38, mean loss = 0.31
Mon Jan 31 06:37:05 2022
batch 40, train loss = 0.12, mean loss = 0.31
Mon Jan 31 06:38:35 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:38:50 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:40:15 2022
val Loss: 0.02

epoch 48 was done for 492.130536 seconds
Epoch 49/299
----------
Mon Jan 31 06:40:17 2022
batch 0, train loss = 0.11, mean loss = 0.11
Mon Jan 31 06:40:26 2022
batch 10, train loss = 0.41, mean loss = 0.31
Mon Jan 31 06:41:56 2022
batch 20, train loss = 0.09, mean loss = 0.30
Mon Jan 31 06:43:26 2022
batch 30, train loss = 0.39, mean loss = 0.31
Mon Jan 31 06:44:55 2022
batch 40, train loss = 0.11, mean loss = 0.31
Mon Jan 31 06:46:25 2022
train Loss: 0.32

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:46:39 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:48:05 2022
val Loss: 0.02

epoch 49 was done for 469.964978 seconds
Epoch 50/299
----------
Mon Jan 31 06:48:07 2022
batch 0, train loss = 0.10, mean loss = 0.10
Mon Jan 31 06:48:16 2022
batch 10, train loss = 0.44, mean loss = 0.33
Mon Jan 31 06:49:45 2022
batch 20, train loss = 0.11, mean loss = 0.32
Mon Jan 31 06:51:15 2022
batch 30, train loss = 0.45, mean loss = 0.33
Mon Jan 31 06:52:45 2022
batch 40, train loss = 0.10, mean loss = 0.33
Mon Jan 31 06:54:15 2022
train Loss: 0.33

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:54:30 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:55:55 2022
val Loss: 0.02

epoch 50 was done for 470.453918 seconds
Epoch 51/299
----------
Mon Jan 31 06:55:58 2022
batch 0, train loss = 0.11, mean loss = 0.11
Mon Jan 31 06:56:06 2022
batch 10, train loss = 0.41, mean loss = 0.33
Mon Jan 31 06:57:36 2022
batch 20, train loss = 0.15, mean loss = 0.34
Mon Jan 31 06:59:05 2022
batch 30, train loss = 0.47, mean loss = 0.37
Mon Jan 31 07:00:34 2022
batch 40, train loss = 0.32, mean loss = 0.38
Mon Jan 31 07:02:04 2022
train Loss: 0.38

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:02:18 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:03:43 2022
val Loss: 0.02

epoch 51 was done for 467.984466 seconds
Epoch 52/299
----------
Mon Jan 31 07:03:46 2022
batch 0, train loss = 0.16, mean loss = 0.16
Mon Jan 31 07:03:55 2022
batch 10, train loss = 0.44, mean loss = 0.38
Mon Jan 31 07:05:24 2022
batch 20, train loss = 0.21, mean loss = 0.37
Mon Jan 31 07:06:53 2022
batch 30, train loss = 0.45, mean loss = 0.37
Mon Jan 31 07:08:23 2022
batch 40, train loss = 0.18, mean loss = 0.38
Mon Jan 31 07:09:52 2022
train Loss: 0.38

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 07:10:07 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:11:32 2022
val Loss: 0.02

epoch 52 was done for 468.936439 seconds
Epoch 53/299
----------
Mon Jan 31 07:11:35 2022
batch 0, train loss = 0.29, mean loss = 0.29
Mon Jan 31 07:11:44 2022
batch 10, train loss = 0.46, mean loss = 0.41
Mon Jan 31 07:13:13 2022
batch 20, train loss = 0.15, mean loss = 0.40
Mon Jan 31 07:14:43 2022
batch 30, train loss = 0.41, mean loss = 0.41
Mon Jan 31 07:16:13 2022
batch 40, train loss = 0.19, mean loss = 0.41
Mon Jan 31 07:17:43 2022
train Loss: 0.41

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 07:17:57 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:19:23 2022
val Loss: 0.02

epoch 53 was done for 470.790514 seconds
Epoch 54/299
----------
Mon Jan 31 07:19:25 2022
batch 0, train loss = 0.19, mean loss = 0.19
Mon Jan 31 07:19:34 2022
batch 10, train loss = 0.42, mean loss = 0.41
Mon Jan 31 07:21:03 2022
batch 20, train loss = 0.16, mean loss = 0.40
Mon Jan 31 07:22:34 2022
batch 30, train loss = 0.49, mean loss = 0.41
Mon Jan 31 07:24:03 2022
batch 40, train loss = 0.24, mean loss = 0.41
Mon Jan 31 07:25:34 2022
train Loss: 0.42

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 07:25:48 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:27:14 2022
val Loss: 0.02

epoch 54 was done for 471.147285 seconds
Epoch 55/299
----------
Mon Jan 31 07:27:16 2022
batch 0, train loss = 0.21, mean loss = 0.21
Mon Jan 31 07:27:25 2022
batch 10, train loss = 0.51, mean loss = 0.45
Mon Jan 31 07:28:56 2022
batch 20, train loss = 0.20, mean loss = 0.46
Mon Jan 31 07:30:26 2022
batch 30, train loss = 0.64, mean loss = 0.48
Mon Jan 31 07:31:59 2022
batch 40, train loss = 0.43, mean loss = 0.50
Mon Jan 31 07:33:28 2022
train Loss: 0.50

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 07:33:43 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:35:07 2022
val Loss: 0.02

epoch 55 was done for 473.467502 seconds
Epoch 56/299
----------
Mon Jan 31 07:35:10 2022
batch 0, train loss = 0.19, mean loss = 0.19
Mon Jan 31 07:35:19 2022
batch 10, train loss = 0.92, mean loss = 0.61
Mon Jan 31 07:36:48 2022
batch 20, train loss = 0.56, mean loss = 0.65
Mon Jan 31 07:38:18 2022
batch 30, train loss = 0.78, mean loss = 0.66
Mon Jan 31 07:39:48 2022
batch 40, train loss = 0.57, mean loss = 0.66
Mon Jan 31 07:41:26 2022
train Loss: 0.65

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:41:41 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:43:06 2022
val Loss: 0.02

epoch 56 was done for 478.927469 seconds
Epoch 57/299
----------
Mon Jan 31 07:43:09 2022
batch 0, train loss = 0.22, mean loss = 0.22
Mon Jan 31 07:43:18 2022
batch 10, train loss = 0.75, mean loss = 0.60
Mon Jan 31 07:44:47 2022
batch 20, train loss = 0.44, mean loss = 0.56
Mon Jan 31 07:46:22 2022
batch 30, train loss = 0.53, mean loss = 0.55
Mon Jan 31 07:47:55 2022
batch 40, train loss = 0.25, mean loss = 0.54
Mon Jan 31 07:49:24 2022
train Loss: 0.54

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 07:49:39 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 07:51:04 2022
val Loss: 0.03

epoch 57 was done for 477.760109 seconds
Epoch 58/299
----------
Mon Jan 31 07:51:07 2022
batch 0, train loss = 0.34, mean loss = 0.34
Mon Jan 31 07:51:16 2022
batch 10, train loss = 0.60, mean loss = 0.55
Mon Jan 31 07:52:45 2022
batch 20, train loss = 0.23, mean loss = 0.53
Mon Jan 31 07:54:15 2022
batch 30, train loss = 0.86, mean loss = 0.56
Mon Jan 31 07:55:46 2022
batch 40, train loss = 0.49, mean loss = 0.56
Mon Jan 31 07:57:16 2022
train Loss: 0.56

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:57:31 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:58:59 2022
val Loss: 0.02

epoch 58 was done for 474.744888 seconds
Epoch 59/299
----------
Mon Jan 31 07:59:01 2022
batch 0, train loss = 0.25, mean loss = 0.25
Mon Jan 31 07:59:11 2022
batch 10, train loss = 0.63, mean loss = 0.53
Mon Jan 31 08:00:42 2022
batch 20, train loss = 0.32, mean loss = 0.50
Mon Jan 31 08:02:17 2022
batch 30, train loss = 0.67, mean loss = 0.50
Mon Jan 31 08:04:00 2022
batch 40, train loss = 0.29, mean loss = 0.51
Mon Jan 31 08:05:42 2022
train Loss: 0.51

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 08:05:58 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:07:25 2022
val Loss: 0.02

epoch 59 was done for 506.501764 seconds
Epoch 60/299
----------
Mon Jan 31 08:07:28 2022
batch 0, train loss = 0.34, mean loss = 0.34
Mon Jan 31 08:07:37 2022
batch 10, train loss = 0.72, mean loss = 0.54
Mon Jan 31 08:09:08 2022
batch 20, train loss = 0.30, mean loss = 0.53
Mon Jan 31 08:10:37 2022
batch 30, train loss = 0.76, mean loss = 0.54
Mon Jan 31 08:12:07 2022
batch 40, train loss = 0.31, mean loss = 0.55
Mon Jan 31 08:13:36 2022
train Loss: 0.56

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 08:13:50 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:15:15 2022
val Loss: 0.02

epoch 60 was done for 469.692936 seconds
Epoch 61/299
----------
Mon Jan 31 08:15:18 2022
batch 0, train loss = 0.46, mean loss = 0.46
Mon Jan 31 08:15:26 2022
batch 10, train loss = 0.85, mean loss = 0.61
Mon Jan 31 08:16:55 2022
batch 20, train loss = 0.32, mean loss = 0.61
Mon Jan 31 08:18:25 2022
batch 30, train loss = 0.69, mean loss = 0.62
Mon Jan 31 08:19:55 2022
batch 40, train loss = 0.48, mean loss = 0.63
Mon Jan 31 08:21:24 2022
train Loss: 0.64

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 08:21:39 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:23:04 2022
val Loss: 0.02

epoch 61 was done for 468.860269 seconds
Epoch 62/299
----------
Mon Jan 31 08:23:06 2022
batch 0, train loss = 0.34, mean loss = 0.34
Mon Jan 31 08:23:15 2022
batch 10, train loss = 0.68, mean loss = 0.65
Mon Jan 31 08:24:45 2022
batch 20, train loss = 0.47, mean loss = 0.67
Mon Jan 31 08:26:15 2022
batch 30, train loss = 0.75, mean loss = 0.68
Mon Jan 31 08:27:47 2022
batch 40, train loss = 0.65, mean loss = 0.68
Mon Jan 31 08:29:22 2022
train Loss: 0.69

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:29:36 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:31:03 2022
val Loss: 0.02

epoch 62 was done for 479.682521 seconds
Epoch 63/299
----------
Mon Jan 31 08:31:06 2022
batch 0, train loss = 0.45, mean loss = 0.45
Mon Jan 31 08:31:15 2022
batch 10, train loss = 0.67, mean loss = 0.67
Mon Jan 31 08:32:49 2022
batch 20, train loss = 0.62, mean loss = 0.67
Mon Jan 31 08:34:25 2022
batch 30, train loss = 0.89, mean loss = 0.67
Mon Jan 31 08:35:56 2022
batch 40, train loss = 0.36, mean loss = 0.66
Mon Jan 31 08:37:31 2022
train Loss: 0.66

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 08:37:47 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:39:21 2022
val Loss: 0.02

epoch 63 was done for 497.923826 seconds
Epoch 64/299
----------
Mon Jan 31 08:39:24 2022
batch 0, train loss = 0.58, mean loss = 0.58
Mon Jan 31 08:39:34 2022
batch 10, train loss = 0.78, mean loss = 0.60
Mon Jan 31 08:41:09 2022
batch 20, train loss = 0.30, mean loss = 0.60
Mon Jan 31 08:42:49 2022
batch 30, train loss = 0.54, mean loss = 0.61
Mon Jan 31 08:44:31 2022
batch 40, train loss = 0.37, mean loss = 0.59
Mon Jan 31 08:46:12 2022
train Loss: 0.59

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 08:46:28 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 08:48:02 2022
val Loss: 0.03

epoch 64 was done for 521.005230 seconds
Epoch 65/299
----------
Mon Jan 31 08:48:05 2022
batch 0, train loss = 0.27, mean loss = 0.27
Mon Jan 31 08:48:15 2022
batch 10, train loss = 0.58, mean loss = 0.55
Mon Jan 31 08:49:56 2022
batch 20, train loss = 0.39, mean loss = 0.56
Mon Jan 31 08:51:41 2022
batch 30, train loss = 0.60, mean loss = 0.57
Mon Jan 31 08:53:20 2022
batch 40, train loss = 0.37, mean loss = 0.56
Mon Jan 31 08:54:58 2022
train Loss: 0.56

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:55:12 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 08:56:38 2022
val Loss: 0.03

epoch 65 was done for 515.962136 seconds
Epoch 66/299
----------
Mon Jan 31 08:56:41 2022
batch 0, train loss = 0.32, mean loss = 0.32
Mon Jan 31 08:56:50 2022
batch 10, train loss = 0.64, mean loss = 0.50
Mon Jan 31 08:58:20 2022
batch 20, train loss = 0.28, mean loss = 0.51
Mon Jan 31 09:00:08 2022
batch 30, train loss = 0.43, mean loss = 0.54
Mon Jan 31 09:02:01 2022
batch 40, train loss = 0.40, mean loss = 0.54
Mon Jan 31 09:03:54 2022
train Loss: 0.54

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:04:12 2022
batch 10, val loss = 0.03, mean loss = 0.02
Mon Jan 31 09:05:57 2022
val Loss: 0.02

epoch 66 was done for 559.037178 seconds
Epoch 67/299
----------
Mon Jan 31 09:06:00 2022
batch 0, train loss = 0.22, mean loss = 0.22
Mon Jan 31 09:06:11 2022
batch 10, train loss = 0.48, mean loss = 0.53
Mon Jan 31 09:08:03 2022
batch 20, train loss = 0.37, mean loss = 0.55
Mon Jan 31 09:10:01 2022
batch 30, train loss = 0.61, mean loss = 0.58
Mon Jan 31 09:11:53 2022
batch 40, train loss = 0.64, mean loss = 0.59
Mon Jan 31 09:13:45 2022
train Loss: 0.59

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:14:03 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:15:48 2022
val Loss: 0.02

epoch 67 was done for 590.879790 seconds
Epoch 68/299
----------
Mon Jan 31 09:15:51 2022
batch 0, train loss = 0.26, mean loss = 0.26
Mon Jan 31 09:16:02 2022
batch 10, train loss = 0.51, mean loss = 0.60
Mon Jan 31 09:17:53 2022
batch 20, train loss = 0.52, mean loss = 0.59
Mon Jan 31 09:19:46 2022
batch 30, train loss = 0.75, mean loss = 0.59
Mon Jan 31 09:21:37 2022
batch 40, train loss = 0.50, mean loss = 0.59
Mon Jan 31 09:23:30 2022
train Loss: 0.60

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:23:48 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:25:34 2022
val Loss: 0.02

epoch 68 was done for 585.597181 seconds
Epoch 69/299
----------
Mon Jan 31 09:25:37 2022
batch 0, train loss = 0.56, mean loss = 0.56
Mon Jan 31 09:25:48 2022
batch 10, train loss = 0.71, mean loss = 0.60
Mon Jan 31 09:27:41 2022
batch 20, train loss = 0.52, mean loss = 0.58
Mon Jan 31 09:29:34 2022
batch 30, train loss = 0.85, mean loss = 0.60
Mon Jan 31 09:31:26 2022
batch 40, train loss = 0.23, mean loss = 0.59
Mon Jan 31 09:33:19 2022
train Loss: 0.60

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:33:37 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:35:22 2022
val Loss: 0.02

epoch 69 was done for 587.938159 seconds
Epoch 70/299
----------
Mon Jan 31 09:35:24 2022
batch 0, train loss = 0.61, mean loss = 0.61
Mon Jan 31 09:35:36 2022
batch 10, train loss = 0.80, mean loss = 0.61
Mon Jan 31 09:37:28 2022
batch 20, train loss = 0.20, mean loss = 0.58
Mon Jan 31 09:39:20 2022
batch 30, train loss = 0.45, mean loss = 0.59
Mon Jan 31 09:41:12 2022
batch 40, train loss = 0.47, mean loss = 0.58
Mon Jan 31 09:43:04 2022
train Loss: 0.58

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:43:22 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:45:07 2022
val Loss: 0.02

epoch 70 was done for 585.561444 seconds
Epoch 71/299
----------
Mon Jan 31 09:45:10 2022
batch 0, train loss = 0.16, mean loss = 0.16
Mon Jan 31 09:45:21 2022
batch 10, train loss = 0.41, mean loss = 0.51
Mon Jan 31 09:47:14 2022
batch 20, train loss = 0.37, mean loss = 0.49
Mon Jan 31 09:49:06 2022
batch 30, train loss = 0.59, mean loss = 0.47
Mon Jan 31 09:51:00 2022
batch 40, train loss = 0.25, mean loss = 0.46
Mon Jan 31 09:52:53 2022
train Loss: 0.46

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:53:11 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:54:56 2022
val Loss: 0.02

epoch 71 was done for 588.196360 seconds
Epoch 72/299
----------
Mon Jan 31 09:54:58 2022
batch 0, train loss = 0.28, mean loss = 0.28
Mon Jan 31 09:55:09 2022
batch 10, train loss = 0.49, mean loss = 0.40
Mon Jan 31 09:57:00 2022
batch 20, train loss = 0.22, mean loss = 0.39
Mon Jan 31 09:58:52 2022
batch 30, train loss = 0.56, mean loss = 0.41
Mon Jan 31 10:00:46 2022
batch 40, train loss = 0.21, mean loss = 0.41
Mon Jan 31 10:02:37 2022
train Loss: 0.41

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:02:56 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:04:40 2022
val Loss: 0.02

epoch 72 was done for 584.615147 seconds
Epoch 73/299
----------
Mon Jan 31 10:04:43 2022
batch 0, train loss = 0.27, mean loss = 0.27
Mon Jan 31 10:04:54 2022
batch 10, train loss = 0.56, mean loss = 0.41
Mon Jan 31 10:06:46 2022
batch 20, train loss = 0.26, mean loss = 0.41
Mon Jan 31 10:08:38 2022
batch 30, train loss = 0.66, mean loss = 0.42
Mon Jan 31 10:10:29 2022
batch 40, train loss = 0.12, mean loss = 0.43
Mon Jan 31 10:12:21 2022
train Loss: 0.43

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:12:39 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:14:25 2022
val Loss: 0.02

epoch 73 was done for 584.796049 seconds
Epoch 74/299
----------
Mon Jan 31 10:14:28 2022
batch 0, train loss = 0.41, mean loss = 0.41
Mon Jan 31 10:14:39 2022
batch 10, train loss = 0.74, mean loss = 0.47
Mon Jan 31 10:16:29 2022
batch 20, train loss = 0.14, mean loss = 0.46
Mon Jan 31 10:18:19 2022
batch 30, train loss = 0.52, mean loss = 0.49
Mon Jan 31 10:20:11 2022
batch 40, train loss = 0.22, mean loss = 0.49
Mon Jan 31 10:22:05 2022
train Loss: 0.50

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:22:23 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:24:07 2022
val Loss: 0.02

epoch 74 was done for 582.418061 seconds
Epoch 75/299
----------
Mon Jan 31 10:24:10 2022
batch 0, train loss = 0.24, mean loss = 0.24
Mon Jan 31 10:24:22 2022
batch 10, train loss = 0.63, mean loss = 0.54
Mon Jan 31 10:26:13 2022
batch 20, train loss = 0.19, mean loss = 0.54
Mon Jan 31 10:28:03 2022
batch 30, train loss = 0.40, mean loss = 0.57
Mon Jan 31 10:29:54 2022
batch 40, train loss = 0.60, mean loss = 0.57
Mon Jan 31 10:31:45 2022
train Loss: 0.57

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:32:03 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 10:33:46 2022
val Loss: 0.03

epoch 75 was done for 578.621041 seconds
Epoch 76/299
----------
Mon Jan 31 10:33:49 2022
batch 0, train loss = 0.13, mean loss = 0.13
Mon Jan 31 10:34:00 2022
batch 10, train loss = 0.40, mean loss = 0.56
Mon Jan 31 10:35:51 2022
batch 20, train loss = 0.56, mean loss = 0.57
Mon Jan 31 10:37:42 2022
batch 30, train loss = 0.77, mean loss = 0.56
Mon Jan 31 10:39:31 2022
batch 40, train loss = 0.38, mean loss = 0.55
Mon Jan 31 10:41:21 2022
train Loss: 0.55

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 10:41:39 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 10:43:23 2022
val Loss: 0.03

epoch 76 was done for 577.404790 seconds
Epoch 77/299
----------
Mon Jan 31 10:43:26 2022
batch 0, train loss = 0.52, mean loss = 0.52
Mon Jan 31 10:43:37 2022
batch 10, train loss = 0.67, mean loss = 0.47
Mon Jan 31 10:45:28 2022
batch 20, train loss = 0.25, mean loss = 0.44
Mon Jan 31 10:47:18 2022
batch 30, train loss = 0.50, mean loss = 0.45
Mon Jan 31 10:49:08 2022
batch 40, train loss = 0.13, mean loss = 0.44
Mon Jan 31 10:50:59 2022
train Loss: 0.44

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:51:17 2022
