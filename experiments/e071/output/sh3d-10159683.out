The activation script must be sourced, otherwise the virtual environment will not work.
Setting vars
iscuda= True
opt.wandb =  
93
file to frame csv ../../csv/598frame.csv
iscuda= True
opt.wandb =  
93
file to frame csv ../../csv/598frame.csv
iscuda= True
iscuda= True
opt.wandb =  
PyTorch Version:  1.8.1
Torchvision Version:  0.9.0a0
opt:
 Namespace(ampl=441, aug_gt='orient', batch_output=2, bs=15, cencrop=700, center=False, chidden_dim=[96, 128, 256, 256, 256], classicnorm=False, cmscrop=0, conTrain='', criterion='L1', csvname='598csv9', datapath='/p/project/delia-mp/cherepashkin1/phenoseed/', dfname='598frame', downsample=1, epoch=300, expdescr='', expnum='e068', feature_extract=False, framelim=6000, gradient_predivide_factor=1.0, gttype='single_file', haf=True, hidden_dim=[9], inputt='img', kernel_sizes=[7, 3, 3, 3, 3, 3], lb='orient', loadh5=False, localexp='', lr=5e-05, machine='jureca', maintain=False, maintain_line=False, man_dist=False, measure_time=False, merging='batch', minmax=False, minmax3dimage=False, minmax_f=True, minmax_fn='', model_name='', netname=['cnet'], ngpu=4, noise_input=False, noise_output=False, normalize=False, num_input_images=3, num_sam_points=500, num_workers=0, outputt='orient', parallel='horovod', pin_memory=False, print_minibatch=10, pscale=100, rand_angle=False, rescale=500, rot_dirs=False, rotate_output=False, save_output=True, single_folder=False, specie='598', standardize=255, steplr=[1000.0, 1.0], transappendix='_image', ufmodel=100000, updateFraction=0.1, use_adasum=False, use_cuda=True, use_existing_csv=True, use_pretrained=False, use_sep_csv=True, view_sep=False, wandb='', weight_decay=0, zero_angle=True)
sys.argv:
 ['../../main.py', '-datapath', '/p/project/delia-mp/cherepashkin1/phenoseed/', '-epoch', '300', '-bs', '15', '-num_input_images', '3', '-framelim', '6000', '-criterion', 'L1', '-localexp', '', '-lr', '5e-5', '-expnum', 'e068', '-hidden_dim', '9', '-inputt', 'img', '-outputt', 'orient', '-lb', 'orient', '-no_loadh5', '-minmax_fn', '', '-parallel', 'horovod', '-machine', 'jureca', '-merging', 'batch', '-aug_gt', 'orient', '-updateFraction', '0.1', '-steplr', '1000', '1', '-print_minibatch', '10', '-dfname', '598frame']
seed =  0
path were main.py is located= ../../
opt.wandb =  
93
file to frame csv ../../csv/598frame.csv
93
file to frame csv ../../csv/598frame.csv
lframe's length after laoding =  5283
lframe's length after laoding =  5283
lframe's length after laoding =  5283
lframe's length after laoding =  5283
lframe len after excluding all exceptions= 5200
lframe len after excluding all exceptions= 5200
lframe len after excluding all exceptions= 5200
lframe len after excluding all exceptions= 5200
len train =  4160
len train =  4160
len train =  4160
len train =  4160
train consists of 277 full batches with 15 tensors with 3 views
the last batch has size of 5 tensors with 3 views
val consists of 69 full batches with 15 tensors with 3 views
the last batch has size of 5 tensors with 3 views
Epoch 0/299
----------
Mon Jan 31 00:14:52 2022
batch 0, train loss = 0.58, mean loss = 0.58
Mon Jan 31 00:15:01 2022
batch 10, train loss = 7.60, mean loss = 6.90
Mon Jan 31 00:15:56 2022
batch 20, train loss = 3.64, mean loss = 5.31
Mon Jan 31 00:16:51 2022
batch 30, train loss = 3.64, mean loss = 4.75
Mon Jan 31 00:17:45 2022
batch 40, train loss = 2.96, mean loss = 4.31
Mon Jan 31 00:18:39 2022
batch 50, train loss = 1.64, mean loss = 3.79
Mon Jan 31 00:19:33 2022
batch 60, train loss = 1.74, mean loss = 3.45
Mon Jan 31 00:20:27 2022
train Loss: 3.26

batch 0, val loss = 0.15, mean loss = 0.15
Mon Jan 31 00:21:17 2022
batch 10, val loss = 0.15, mean loss = 0.15
Mon Jan 31 00:22:11 2022
val Loss: 0.15

epoch 0 was done for 476.044885 seconds
Epoch 1/299
----------
Mon Jan 31 00:22:48 2022
batch 0, train loss = 1.96, mean loss = 1.96
Mon Jan 31 00:22:54 2022
batch 10, train loss = 2.04, mean loss = 1.98
Mon Jan 31 00:23:47 2022
batch 20, train loss = 1.41, mean loss = 1.70
Mon Jan 31 00:24:41 2022
batch 30, train loss = 1.37, mean loss = 1.57
Mon Jan 31 00:25:34 2022
batch 40, train loss = 1.37, mean loss = 1.53
Mon Jan 31 00:26:27 2022
batch 50, train loss = 1.30, mean loss = 1.50
Mon Jan 31 00:27:20 2022
batch 60, train loss = 1.01, mean loss = 1.42
Mon Jan 31 00:28:13 2022
train Loss: 1.37

batch 0, val loss = 0.13, mean loss = 0.13
Mon Jan 31 00:29:03 2022
batch 10, val loss = 0.12, mean loss = 0.13
Mon Jan 31 00:29:56 2022
val Loss: 0.13

epoch 1 was done for 465.250208 seconds
Epoch 2/299
----------
Mon Jan 31 00:30:33 2022
batch 0, train loss = 0.96, mean loss = 0.96
Mon Jan 31 00:30:39 2022
batch 10, train loss = 1.07, mean loss = 1.11
Mon Jan 31 00:31:33 2022
batch 20, train loss = 0.99, mean loss = 1.06
Mon Jan 31 00:32:27 2022
batch 30, train loss = 0.86, mean loss = 1.00
Mon Jan 31 00:33:21 2022
batch 40, train loss = 0.89, mean loss = 0.99
Mon Jan 31 00:34:15 2022
batch 50, train loss = 0.88, mean loss = 0.98
Mon Jan 31 00:35:09 2022
batch 60, train loss = 0.81, mean loss = 0.95
Mon Jan 31 00:36:04 2022
train Loss: 0.94

batch 0, val loss = 0.12, mean loss = 0.12
Mon Jan 31 00:36:54 2022
batch 10, val loss = 0.13, mean loss = 0.13
Mon Jan 31 00:37:49 2022
val Loss: 0.13

epoch 2 was done for 472.889790 seconds
Epoch 3/299
----------
Mon Jan 31 00:38:26 2022
batch 0, train loss = 0.82, mean loss = 0.82
Mon Jan 31 00:38:32 2022
batch 10, train loss = 0.84, mean loss = 0.89
Mon Jan 31 00:39:25 2022
batch 20, train loss = 0.74, mean loss = 0.82
Mon Jan 31 00:40:19 2022
batch 30, train loss = 0.74, mean loss = 0.80
Mon Jan 31 00:41:12 2022
batch 40, train loss = 0.66, mean loss = 0.78
Mon Jan 31 00:42:05 2022
batch 50, train loss = 0.71, mean loss = 0.78
Mon Jan 31 00:42:59 2022
batch 60, train loss = 0.62, mean loss = 0.76
Mon Jan 31 00:43:52 2022
train Loss: 0.75

batch 0, val loss = 0.12, mean loss = 0.12
Mon Jan 31 00:44:42 2022
batch 10, val loss = 0.13, mean loss = 0.13
Mon Jan 31 00:45:36 2022
val Loss: 0.12

epoch 3 was done for 467.270718 seconds
Epoch 4/299
----------
Mon Jan 31 00:46:14 2022
batch 0, train loss = 0.59, mean loss = 0.59
Mon Jan 31 00:46:19 2022
batch 10, train loss = 0.63, mean loss = 0.66
Mon Jan 31 00:47:12 2022
batch 20, train loss = 0.64, mean loss = 0.67
Mon Jan 31 00:48:06 2022
batch 30, train loss = 0.60, mean loss = 0.67
Mon Jan 31 00:48:59 2022
batch 40, train loss = 0.61, mean loss = 0.67
Mon Jan 31 00:49:52 2022
batch 50, train loss = 0.58, mean loss = 0.67
Mon Jan 31 00:50:46 2022
batch 60, train loss = 0.58, mean loss = 0.66
Mon Jan 31 00:51:40 2022
train Loss: 0.66

batch 0, val loss = 0.13, mean loss = 0.13
Mon Jan 31 00:52:30 2022
batch 10, val loss = 0.12, mean loss = 0.12
Mon Jan 31 00:53:24 2022
val Loss: 0.12

epoch 4 was done for 467.743244 seconds
Epoch 5/299
----------
Mon Jan 31 00:54:01 2022
batch 0, train loss = 0.61, mean loss = 0.61
Mon Jan 31 00:54:07 2022
batch 10, train loss = 0.68, mean loss = 0.68
Mon Jan 31 00:55:01 2022
batch 20, train loss = 0.57, mean loss = 0.65
Mon Jan 31 00:55:55 2022
batch 30, train loss = 0.57, mean loss = 0.64
Mon Jan 31 00:56:49 2022
batch 40, train loss = 0.63, mean loss = 0.65
Mon Jan 31 00:57:43 2022
batch 50, train loss = 0.66, mean loss = 0.66
Mon Jan 31 00:58:37 2022
batch 60, train loss = 0.46, mean loss = 0.64
Mon Jan 31 00:59:31 2022
train Loss: 0.64

batch 0, val loss = 0.13, mean loss = 0.13
Mon Jan 31 01:00:21 2022
batch 10, val loss = 0.13, mean loss = 0.13
Mon Jan 31 01:01:16 2022
val Loss: 0.13

epoch 5 was done for 471.995179 seconds
Epoch 6/299
----------
Mon Jan 31 01:01:53 2022
batch 0, train loss = 0.53, mean loss = 0.53
Mon Jan 31 01:01:59 2022
batch 10, train loss = 0.68, mean loss = 0.70
Mon Jan 31 01:02:52 2022
batch 20, train loss = 0.59, mean loss = 0.67
Mon Jan 31 01:03:45 2022
batch 30, train loss = 0.42, mean loss = 0.62
Mon Jan 31 01:04:38 2022
batch 40, train loss = 0.53, mean loss = 0.62
Mon Jan 31 01:05:32 2022
batch 50, train loss = 0.66, mean loss = 0.64
Mon Jan 31 01:06:25 2022
batch 60, train loss = 0.53, mean loss = 0.63
Mon Jan 31 01:07:18 2022
train Loss: 0.62

batch 0, val loss = 0.12, mean loss = 0.12
Mon Jan 31 01:08:08 2022
batch 10, val loss = 0.12, mean loss = 0.13
Mon Jan 31 01:09:02 2022
val Loss: 0.12

epoch 6 was done for 465.063565 seconds
Epoch 7/299
----------
Mon Jan 31 01:09:38 2022
batch 0, train loss = 0.46, mean loss = 0.46
Mon Jan 31 01:09:44 2022
batch 10, train loss = 0.57, mean loss = 0.62
Mon Jan 31 01:10:37 2022
batch 20, train loss = 0.58, mean loss = 0.63
Mon Jan 31 01:11:30 2022
batch 30, train loss = 0.47, mean loss = 0.61
Mon Jan 31 01:12:24 2022
batch 40, train loss = 0.42, mean loss = 0.59
Mon Jan 31 01:13:17 2022
batch 50, train loss = 0.56, mean loss = 0.60
Mon Jan 31 01:14:10 2022
batch 60, train loss = 0.48, mean loss = 0.60
Mon Jan 31 01:15:04 2022
train Loss: 0.59

batch 0, val loss = 0.12, mean loss = 0.12
Mon Jan 31 01:15:54 2022
batch 10, val loss = 0.13, mean loss = 0.12
Mon Jan 31 01:16:48 2022
val Loss: 0.12

epoch 7 was done for 466.444242 seconds
Epoch 8/299
----------
Mon Jan 31 01:17:25 2022
batch 0, train loss = 0.49, mean loss = 0.49
Mon Jan 31 01:17:30 2022
batch 10, train loss = 0.51, mean loss = 0.59
Mon Jan 31 01:18:23 2022
batch 20, train loss = 0.53, mean loss = 0.60
Mon Jan 31 01:19:16 2022
batch 30, train loss = 0.44, mean loss = 0.59
Mon Jan 31 01:20:09 2022
batch 40, train loss = 0.48, mean loss = 0.59
Mon Jan 31 01:21:02 2022
batch 50, train loss = 0.53, mean loss = 0.59
Mon Jan 31 01:21:56 2022
batch 60, train loss = 0.47, mean loss = 0.59
Mon Jan 31 01:22:49 2022
train Loss: 0.59

batch 0, val loss = 0.13, mean loss = 0.13
Mon Jan 31 01:23:38 2022
batch 10, val loss = 0.13, mean loss = 0.13
Mon Jan 31 01:24:33 2022
val Loss: 0.13

epoch 8 was done for 465.366877 seconds
Epoch 9/299
----------
Mon Jan 31 01:25:10 2022
batch 0, train loss = 0.52, mean loss = 0.52
Mon Jan 31 01:25:16 2022
batch 10, train loss = 0.48, mean loss = 0.57
Mon Jan 31 01:26:09 2022
batch 20, train loss = 0.50, mean loss = 0.58
Mon Jan 31 01:27:02 2022
batch 30, train loss = 0.45, mean loss = 0.57
Mon Jan 31 01:27:56 2022
batch 40, train loss = 0.39, mean loss = 0.56
Mon Jan 31 01:28:49 2022
batch 50, train loss = 0.48, mean loss = 0.56
Mon Jan 31 01:29:42 2022
batch 60, train loss = 0.46, mean loss = 0.56
Mon Jan 31 01:30:35 2022
train Loss: 0.56

batch 0, val loss = 0.12, mean loss = 0.12
Mon Jan 31 01:31:25 2022
batch 10, val loss = 0.13, mean loss = 0.13
Mon Jan 31 01:32:19 2022
val Loss: 0.13

epoch 9 was done for 465.957525 seconds
Epoch 10/299
----------
Mon Jan 31 01:32:56 2022
batch 0, train loss = 0.47, mean loss = 0.47
Mon Jan 31 01:33:02 2022
batch 10, train loss = 0.48, mean loss = 0.56
Mon Jan 31 01:33:56 2022
batch 20, train loss = 0.48, mean loss = 0.56
Mon Jan 31 01:34:49 2022
batch 30, train loss = 0.42, mean loss = 0.55
Mon Jan 31 01:35:43 2022
batch 40, train loss = 0.47, mean loss = 0.56
Mon Jan 31 01:36:37 2022
batch 50, train loss = 0.40, mean loss = 0.56
Mon Jan 31 01:37:31 2022
batch 60, train loss = 0.51, mean loss = 0.56
Mon Jan 31 01:38:25 2022
train Loss: 0.55

batch 0, val loss = 0.13, mean loss = 0.13
Mon Jan 31 01:39:15 2022
batch 10, val loss = 0.12, mean loss = 0.13
Mon Jan 31 01:40:10 2022
val Loss: 0.13

epoch 10 was done for 471.197694 seconds
Epoch 11/299
----------
Mon Jan 31 01:40:47 2022
batch 0, train loss = 0.36, mean loss = 0.36
Mon Jan 31 01:40:53 2022
batch 10, train loss = 0.50, mean loss = 0.55
Mon Jan 31 01:41:46 2022
batch 20, train loss = 0.45, mean loss = 0.55
Mon Jan 31 01:42:39 2022
batch 30, train loss = 0.48, mean loss = 0.56
Mon Jan 31 01:43:32 2022
batch 40, train loss = 0.38, mean loss = 0.56
Mon Jan 31 01:44:25 2022
batch 50, train loss = 0.47, mean loss = 0.56
Mon Jan 31 01:45:19 2022
batch 60, train loss = 0.42, mean loss = 0.55
Mon Jan 31 01:46:12 2022
train Loss: 0.56

batch 0, val loss = 0.12, mean loss = 0.12
Mon Jan 31 01:47:02 2022
batch 10, val loss = 0.13, mean loss = 0.12
Mon Jan 31 01:47:56 2022
val Loss: 0.12

epoch 11 was done for 465.605071 seconds
Epoch 12/299
----------
Mon Jan 31 01:48:33 2022
batch 0, train loss = 0.43, mean loss = 0.43
Mon Jan 31 01:48:38 2022
batch 10, train loss = 0.39, mean loss = 0.51
Mon Jan 31 01:49:32 2022
batch 20, train loss = 0.44, mean loss = 0.51
Mon Jan 31 01:50:26 2022
batch 30, train loss = 0.44, mean loss = 0.52
Mon Jan 31 01:51:19 2022
batch 40, train loss = 0.44, mean loss = 0.52
Mon Jan 31 01:52:14 2022
batch 50, train loss = 0.47, mean loss = 0.54
Mon Jan 31 01:53:08 2022
batch 60, train loss = 0.49, mean loss = 0.55
Mon Jan 31 01:54:02 2022
train Loss: 0.55

batch 0, val loss = 0.13, mean loss = 0.13
Mon Jan 31 01:54:52 2022
batch 10, val loss = 0.12, mean loss = 0.12
Mon Jan 31 01:55:47 2022
val Loss: 0.12

epoch 12 was done for 470.929744 seconds
Epoch 13/299
----------
Mon Jan 31 01:56:24 2022
batch 0, train loss = 0.42, mean loss = 0.42
Mon Jan 31 01:56:29 2022
batch 10, train loss = 0.51, mean loss = 0.59
Mon Jan 31 01:57:23 2022
batch 20, train loss = 0.41, mean loss = 0.56
Mon Jan 31 01:58:18 2022
batch 30, train loss = 0.49, mean loss = 0.56
Mon Jan 31 01:59:12 2022
batch 40, train loss = 0.37, mean loss = 0.55
Mon Jan 31 02:00:06 2022
batch 50, train loss = 0.40, mean loss = 0.54
Mon Jan 31 02:01:00 2022
batch 60, train loss = 0.42, mean loss = 0.54
Mon Jan 31 02:01:54 2022
train Loss: 0.55

batch 0, val loss = 0.13, mean loss = 0.13
Mon Jan 31 02:02:44 2022
batch 10, val loss = 0.13, mean loss = 0.13
Mon Jan 31 02:03:39 2022
val Loss: 0.13

epoch 13 was done for 472.675282 seconds
Epoch 14/299
----------
Mon Jan 31 02:04:17 2022
batch 0, train loss = 0.42, mean loss = 0.42
Mon Jan 31 02:04:22 2022
batch 10, train loss = 0.44, mean loss = 0.56
Mon Jan 31 02:05:15 2022
batch 20, train loss = 0.46, mean loss = 0.57
Mon Jan 31 02:06:08 2022
batch 30, train loss = 0.41, mean loss = 0.56
Mon Jan 31 02:07:02 2022
batch 40, train loss = 0.38, mean loss = 0.56
Mon Jan 31 02:07:55 2022
batch 50, train loss = 0.43, mean loss = 0.57
Mon Jan 31 02:08:48 2022
batch 60, train loss = 0.38, mean loss = 0.56
Mon Jan 31 02:09:42 2022
train Loss: 0.56

batch 0, val loss = 0.12, mean loss = 0.12
Mon Jan 31 02:10:31 2022
batch 10, val loss = 0.13, mean loss = 0.12
Mon Jan 31 02:11:25 2022
val Loss: 0.12

epoch 14 was done for 465.558253 seconds
Epoch 15/299
----------
Mon Jan 31 02:12:02 2022
batch 0, train loss = 0.37, mean loss = 0.37
Mon Jan 31 02:12:08 2022
batch 10, train loss = 0.39, mean loss = 0.53
Mon Jan 31 02:13:01 2022
batch 20, train loss = 0.40, mean loss = 0.53
Mon Jan 31 02:13:55 2022
batch 30, train loss = 0.34, mean loss = 0.52
Mon Jan 31 02:14:48 2022
batch 40, train loss = 0.39, mean loss = 0.52
Mon Jan 31 02:15:42 2022
batch 50, train loss = 0.35, mean loss = 0.52
Mon Jan 31 02:16:36 2022
batch 60, train loss = 0.39, mean loss = 0.52
Mon Jan 31 02:17:30 2022
train Loss: 0.52

batch 0, val loss = 0.13, mean loss = 0.13
Mon Jan 31 02:18:23 2022
batch 10, val loss = 0.12, mean loss = 0.12
Mon Jan 31 02:19:17 2022
val Loss: 0.12

epoch 15 was done for 472.113471 seconds
Epoch 16/299
----------
Mon Jan 31 02:19:54 2022
batch 0, train loss = 0.36, mean loss = 0.36
Mon Jan 31 02:20:00 2022
batch 10, train loss = 0.39, mean loss = 0.53
Mon Jan 31 02:20:54 2022
batch 20, train loss = 0.41, mean loss = 0.53
Mon Jan 31 02:21:48 2022
batch 30, train loss = 0.39, mean loss = 0.53
Mon Jan 31 02:22:43 2022
batch 40, train loss = 0.37, mean loss = 0.52
Mon Jan 31 02:23:37 2022
batch 50, train loss = 0.34, mean loss = 0.52
Mon Jan 31 02:24:31 2022
batch 60, train loss = 0.37, mean loss = 0.53
Mon Jan 31 02:25:25 2022
train Loss: 0.53

batch 0, val loss = 0.12, mean loss = 0.12
Mon Jan 31 02:26:15 2022
batch 10, val loss = 0.13, mean loss = 0.12
Mon Jan 31 02:27:10 2022
val Loss: 0.12

epoch 16 was done for 473.342157 seconds
Epoch 17/299
----------
Mon Jan 31 02:27:48 2022
batch 0, train loss = 0.40, mean loss = 0.40
Mon Jan 31 02:27:53 2022
batch 10, train loss = 0.40, mean loss = 0.56
Mon Jan 31 02:28:46 2022
batch 20, train loss = 0.41, mean loss = 0.55
Mon Jan 31 02:29:40 2022
batch 30, train loss = 0.40, mean loss = 0.55
Mon Jan 31 02:30:33 2022
batch 40, train loss = 0.34, mean loss = 0.54
Mon Jan 31 02:31:26 2022
batch 50, train loss = 0.37, mean loss = 0.54
Mon Jan 31 02:32:20 2022
batch 60, train loss = 0.37, mean loss = 0.54
Mon Jan 31 02:33:13 2022
train Loss: 0.54

batch 0, val loss = 0.12, mean loss = 0.12
Mon Jan 31 02:34:03 2022
batch 10, val loss = 0.12, mean loss = 0.12
Mon Jan 31 02:34:57 2022
val Loss: 0.12

epoch 17 was done for 466.083058 seconds
Epoch 18/299
----------
Mon Jan 31 02:35:34 2022
batch 0, train loss = 0.36, mean loss = 0.36
Mon Jan 31 02:35:39 2022
batch 10, train loss = 0.45, mean loss = 0.58
Mon Jan 31 02:36:33 2022
batch 20, train loss = 0.44, mean loss = 0.58
Mon Jan 31 02:37:26 2022
batch 30, train loss = 0.51, mean loss = 0.59
Mon Jan 31 02:38:19 2022
batch 40, train loss = 0.45, mean loss = 0.60
Mon Jan 31 02:39:13 2022
batch 50, train loss = 0.40, mean loss = 0.59
Mon Jan 31 02:40:06 2022
batch 60, train loss = 0.39, mean loss = 0.58
Mon Jan 31 02:41:00 2022
train Loss: 0.58

batch 0, val loss = 0.13, mean loss = 0.13
Mon Jan 31 02:41:49 2022
batch 10, val loss = 0.13, mean loss = 0.13
Mon Jan 31 02:42:44 2022
val Loss: 0.13

epoch 18 was done for 466.800755 seconds
Epoch 19/299
----------
Mon Jan 31 02:43:21 2022
batch 0, train loss = 0.43, mean loss = 0.43
Mon Jan 31 02:43:26 2022
batch 10, train loss = 0.43, mean loss = 0.55
Mon Jan 31 02:44:20 2022
batch 20, train loss = 0.49, mean loss = 0.59
Mon Jan 31 02:45:14 2022
batch 30, train loss = 0.45, mean loss = 0.58
Mon Jan 31 02:46:08 2022
batch 40, train loss = 0.44, mean loss = 0.59
Mon Jan 31 02:47:03 2022
batch 50, train loss = 0.49, mean loss = 0.60
Mon Jan 31 02:47:57 2022
batch 60, train loss = 0.51, mean loss = 0.61
Mon Jan 31 02:48:51 2022
train Loss: 0.61

batch 0, val loss = 0.12, mean loss = 0.12
Mon Jan 31 02:49:41 2022
batch 10, val loss = 0.13, mean loss = 0.12
Mon Jan 31 02:50:36 2022
val Loss: 0.13

epoch 19 was done for 472.819743 seconds
Epoch 20/299
----------
Mon Jan 31 02:51:13 2022
batch 0, train loss = 0.48, mean loss = 0.48
Mon Jan 31 02:51:19 2022
batch 10, train loss = 0.45, mean loss = 0.59
Mon Jan 31 02:52:12 2022
batch 20, train loss = 0.51, mean loss = 0.62
Mon Jan 31 02:53:05 2022
batch 30, train loss = 0.64, mean loss = 0.65
Mon Jan 31 02:53:58 2022
batch 40, train loss = 0.50, mean loss = 0.63
Mon Jan 31 02:54:52 2022
batch 50, train loss = 0.59, mean loss = 0.65
Mon Jan 31 02:55:45 2022
batch 60, train loss = 0.66, mean loss = 0.67
Mon Jan 31 02:56:39 2022
train Loss: 0.67

batch 0, val loss = 0.13, mean loss = 0.13
Mon Jan 31 02:57:28 2022
batch 10, val loss = 0.13, mean loss = 0.13
Mon Jan 31 02:58:22 2022
val Loss: 0.13

epoch 20 was done for 465.622924 seconds
Epoch 21/299
----------
Mon Jan 31 02:58:59 2022
batch 0, train loss = 0.50, mean loss = 0.50
Mon Jan 31 02:59:04 2022
batch 10, train loss = 0.60, mean loss = 0.65
Mon Jan 31 02:59:58 2022
batch 20, train loss = 0.53, mean loss = 0.66
Mon Jan 31 03:00:51 2022
batch 30, train loss = 0.66, mean loss = 0.69
Mon Jan 31 03:01:45 2022
batch 40, train loss = 0.56, mean loss = 0.69
Mon Jan 31 03:02:39 2022
batch 50, train loss = 0.43, mean loss = 0.67
Mon Jan 31 03:03:32 2022
batch 60, train loss = 0.49, mean loss = 0.66
Mon Jan 31 03:04:26 2022
train Loss: 0.66

batch 0, val loss = 0.12, mean loss = 0.12
Mon Jan 31 03:05:15 2022
batch 10, val loss = 0.13, mean loss = 0.13
Mon Jan 31 03:06:10 2022
val Loss: 0.13

epoch 21 was done for 467.714130 seconds
Epoch 22/299
----------
Mon Jan 31 03:06:47 2022
batch 0, train loss = 0.43, mean loss = 0.43
Mon Jan 31 03:06:52 2022
batch 10, train loss = 0.58, mean loss = 0.66
Mon Jan 31 03:07:45 2022
batch 20, train loss = 0.47, mean loss = 0.63
Mon Jan 31 03:08:39 2022
batch 30, train loss = 0.48, mean loss = 0.62
Mon Jan 31 03:09:32 2022
batch 40, train loss = 0.53, mean loss = 0.64
Mon Jan 31 03:10:25 2022
batch 50, train loss = 0.46, mean loss = 0.63
Mon Jan 31 03:11:19 2022
batch 60, train loss = 0.49, mean loss = 0.63
Mon Jan 31 03:12:12 2022
train Loss: 0.63

batch 0, val loss = 0.13, mean loss = 0.13
Mon Jan 31 03:13:01 2022
batch 10, val loss = 0.13, mean loss = 0.13
Mon Jan 31 03:13:55 2022
val Loss: 0.12

epoch 22 was done for 465.618398 seconds
Epoch 23/299
----------
Mon Jan 31 03:14:32 2022
batch 0, train loss = 0.42, mean loss = 0.42
Mon Jan 31 03:14:38 2022
batch 10, train loss = 0.62, mean loss = 0.69
Mon Jan 31 03:15:31 2022
batch 20, train loss = 0.55, mean loss = 0.70
Mon Jan 31 03:16:24 2022
batch 30, train loss = 0.54, mean loss = 0.68
Mon Jan 31 03:17:18 2022
batch 40, train loss = 0.44, mean loss = 0.66
Mon Jan 31 03:18:11 2022
batch 50, train loss = 0.51, mean loss = 0.67
Mon Jan 31 03:19:04 2022
batch 60, train loss = 0.55, mean loss = 0.67
Mon Jan 31 03:19:58 2022
train Loss: 0.66

batch 0, val loss = 0.13, mean loss = 0.13
Mon Jan 31 03:20:47 2022
batch 10, val loss = 0.12, mean loss = 0.13
Mon Jan 31 03:21:41 2022
val Loss: 0.13

epoch 23 was done for 465.806922 seconds
Epoch 24/299
----------
Mon Jan 31 03:22:18 2022
batch 0, train loss = 0.46, mean loss = 0.46
Mon Jan 31 03:22:24 2022
batch 10, train loss = 0.46, mean loss = 0.57
Mon Jan 31 03:23:17 2022
batch 20, train loss = 0.47, mean loss = 0.59
Mon Jan 31 03:24:11 2022
batch 30, train loss = 0.48, mean loss = 0.59
Mon Jan 31 03:25:05 2022
batch 40, train loss = 0.35, mean loss = 0.57
Mon Jan 31 03:25:59 2022
batch 50, train loss = 0.43, mean loss = 0.58
Mon Jan 31 03:26:53 2022
batch 60, train loss = 0.50, mean loss = 0.59
Mon Jan 31 03:27:47 2022
train Loss: 0.59

batch 0, val loss = 0.12, mean loss = 0.12
Mon Jan 31 03:28:37 2022
batch 10, val loss = 0.14, mean loss = 0.13
Mon Jan 31 03:29:32 2022
val Loss: 0.13

epoch 24 was done for 470.813853 seconds
Epoch 25/299
----------
Mon Jan 31 03:30:09 2022
batch 0, train loss = 0.41, mean loss = 0.41
Mon Jan 31 03:30:14 2022
batch 10, train loss = 0.50, mean loss = 0.59
Mon Jan 31 03:31:08 2022
batch 20, train loss = 0.48, mean loss = 0.60
Mon Jan 31 03:32:01 2022
batch 30, train loss = 0.43, mean loss = 0.60
Mon Jan 31 03:32:55 2022
batch 40, train loss = 0.46, mean loss = 0.60
Mon Jan 31 03:33:48 2022
batch 50, train loss = 0.39, mean loss = 0.59
Mon Jan 31 03:34:41 2022
batch 60, train loss = 0.42, mean loss = 0.59
Mon Jan 31 03:35:34 2022
train Loss: 0.61

batch 0, val loss = 0.12, mean loss = 0.12
Mon Jan 31 03:36:24 2022
batch 10, val loss = 0.12, mean loss = 0.12
Mon Jan 31 03:37:17 2022
val Loss: 0.12

epoch 25 was done for 465.370268 seconds
Epoch 26/299
----------
Mon Jan 31 03:37:54 2022
batch 0, train loss = 0.48, mean loss = 0.48
Mon Jan 31 03:38:00 2022
batch 10, train loss = 0.42, mean loss = 0.58
Mon Jan 31 03:38:53 2022
batch 20, train loss = 0.42, mean loss = 0.58
Mon Jan 31 03:39:46 2022
batch 30, train loss = 0.37, mean loss = 0.58
Mon Jan 31 03:40:40 2022
batch 40, train loss = 0.37, mean loss = 0.57
Mon Jan 31 03:41:33 2022
batch 50, train loss = 0.35, mean loss = 0.57
Mon Jan 31 03:42:27 2022
batch 60, train loss = 0.38, mean loss = 0.56
Mon Jan 31 03:43:20 2022
train Loss: 0.56

batch 0, val loss = 0.12, mean loss = 0.12
Mon Jan 31 03:44:10 2022
batch 10, val loss = 0.13, mean loss = 0.12
Mon Jan 31 03:45:04 2022
val Loss: 0.12

epoch 26 was done for 466.906076 seconds
Epoch 27/299
----------
Mon Jan 31 03:45:41 2022
batch 0, train loss = 0.34, mean loss = 0.34
Mon Jan 31 03:45:47 2022
batch 10, train loss = 0.39, mean loss = 0.52
Mon Jan 31 03:46:41 2022
batch 20, train loss = 0.40, mean loss = 0.54
Mon Jan 31 03:47:35 2022
batch 30, train loss = 0.33, mean loss = 0.53
Mon Jan 31 03:48:29 2022
batch 40, train loss = 0.43, mean loss = 0.55
Mon Jan 31 03:49:24 2022
batch 50, train loss = 0.29, mean loss = 0.54
Mon Jan 31 03:50:18 2022
batch 60, train loss = 0.47, mean loss = 0.55
Mon Jan 31 03:51:12 2022
train Loss: 0.57

batch 0, val loss = 0.12, mean loss = 0.12
Mon Jan 31 03:52:03 2022
batch 10, val loss = 0.13, mean loss = 0.12
Mon Jan 31 03:52:58 2022
val Loss: 0.12

epoch 27 was done for 474.030897 seconds
Epoch 28/299
----------
Mon Jan 31 03:53:35 2022
batch 0, train loss = 0.44, mean loss = 0.44
Mon Jan 31 03:53:41 2022
batch 10, train loss = 0.34, mean loss = 0.51
Mon Jan 31 03:54:34 2022
batch 20, train loss = 0.40, mean loss = 0.55
Mon Jan 31 03:55:28 2022
batch 30, train loss = 0.44, mean loss = 0.57
Mon Jan 31 03:56:22 2022
batch 40, train loss = 0.38, mean loss = 0.57
Mon Jan 31 03:57:16 2022
batch 50, train loss = 0.33, mean loss = 0.56
Mon Jan 31 03:58:09 2022
batch 60, train loss = 0.42, mean loss = 0.57
Mon Jan 31 03:59:03 2022
train Loss: 0.59

batch 0, val loss = 0.13, mean loss = 0.13
Mon Jan 31 03:59:53 2022
batch 10, val loss = 0.12, mean loss = 0.12
Mon Jan 31 04:00:47 2022
val Loss: 0.12

epoch 28 was done for 468.603382 seconds
Epoch 29/299
----------
Mon Jan 31 04:01:24 2022
batch 0, train loss = 0.48, mean loss = 0.48
Mon Jan 31 04:01:29 2022
batch 10, train loss = 0.54, mean loss = 0.65
Mon Jan 31 04:02:23 2022
batch 20, train loss = 0.33, mean loss = 0.58
Mon Jan 31 04:03:17 2022
batch 30, train loss = 0.53, mean loss = 0.62
Mon Jan 31 04:04:11 2022
batch 40, train loss = 0.62, mean loss = 0.66
Mon Jan 31 04:05:05 2022
batch 50, train loss = 0.54, mean loss = 0.66
Mon Jan 31 04:05:58 2022
batch 60, train loss = 0.50, mean loss = 0.66
Mon Jan 31 04:06:52 2022
train Loss: 0.66

batch 0, val loss = 0.13, mean loss = 0.13
Mon Jan 31 04:07:42 2022
batch 10, val loss = 0.13, mean loss = 0.12
Mon Jan 31 04:08:36 2022
val Loss: 0.12

epoch 29 was done for 469.771584 seconds
Epoch 30/299
----------
Mon Jan 31 04:09:14 2022
batch 0, train loss = 0.49, mean loss = 0.49
Mon Jan 31 04:09:19 2022
batch 10, train loss = 0.46, mean loss = 0.63
Mon Jan 31 04:10:13 2022
batch 20, train loss = 0.56, mean loss = 0.66
Mon Jan 31 04:11:08 2022
batch 30, train loss = 0.49, mean loss = 0.66
Mon Jan 31 04:12:02 2022
batch 40, train loss = 0.39, mean loss = 0.64
Mon Jan 31 04:12:57 2022
batch 50, train loss = 0.41, mean loss = 0.63
Mon Jan 31 04:13:51 2022
batch 60, train loss = 0.47, mean loss = 0.63
Mon Jan 31 04:14:46 2022
train Loss: 0.62

batch 0, val loss = 0.13, mean loss = 0.13
Mon Jan 31 04:15:37 2022
batch 10, val loss = 0.12, mean loss = 0.12
Mon Jan 31 04:16:32 2022
val Loss: 0.12

epoch 30 was done for 475.946249 seconds
Epoch 31/299
----------
Mon Jan 31 04:17:10 2022
batch 0, train loss = 0.36, mean loss = 0.36
Mon Jan 31 04:17:15 2022
batch 10, train loss = 0.39, mean loss = 0.55
Mon Jan 31 04:18:09 2022
batch 20, train loss = 0.46, mean loss = 0.58
Mon Jan 31 04:19:02 2022
batch 30, train loss = 0.43, mean loss = 0.59
Mon Jan 31 04:19:56 2022
batch 40, train loss = 0.39, mean loss = 0.58
Mon Jan 31 04:20:50 2022
batch 50, train loss = 0.42, mean loss = 0.58
Mon Jan 31 04:21:44 2022
batch 60, train loss = 0.44, mean loss = 0.58
Mon Jan 31 04:22:37 2022
train Loss: 0.59

batch 0, val loss = 0.11, mean loss = 0.11
Mon Jan 31 04:23:27 2022
batch 10, val loss = 0.12, mean loss = 0.13
Mon Jan 31 04:24:22 2022
val Loss: 0.13

epoch 31 was done for 469.088505 seconds
Epoch 32/299
----------
Mon Jan 31 04:24:59 2022
batch 0, train loss = 0.38, mean loss = 0.38
Mon Jan 31 04:25:04 2022
batch 10, train loss = 0.33, mean loss = 0.51
Mon Jan 31 04:25:58 2022
batch 20, train loss = 0.39, mean loss = 0.55
Mon Jan 31 04:26:52 2022
batch 30, train loss = 0.42, mean loss = 0.56
Mon Jan 31 04:27:46 2022
batch 40, train loss = 0.30, mean loss = 0.55
Mon Jan 31 04:28:40 2022
batch 50, train loss = 0.39, mean loss = 0.55
Mon Jan 31 04:29:33 2022
batch 60, train loss = 0.37, mean loss = 0.55
Mon Jan 31 04:30:28 2022
train Loss: 0.56

batch 0, val loss = 0.13, mean loss = 0.13
Mon Jan 31 04:31:18 2022
batch 10, val loss = 0.13, mean loss = 0.13
Mon Jan 31 04:32:12 2022
val Loss: 0.13

epoch 32 was done for 470.696943 seconds
Epoch 33/299
----------
Mon Jan 31 04:32:49 2022
batch 0, train loss = 0.37, mean loss = 0.37
Mon Jan 31 04:32:55 2022
batch 10, train loss = 0.35, mean loss = 0.53
Mon Jan 31 04:33:49 2022
batch 20, train loss = 0.44, mean loss = 0.57
Mon Jan 31 04:34:44 2022
batch 30, train loss = 0.40, mean loss = 0.58
Mon Jan 31 04:35:38 2022
batch 40, train loss = 0.36, mean loss = 0.57
Mon Jan 31 04:36:32 2022
batch 50, train loss = 0.38, mean loss = 0.57
Mon Jan 31 04:37:27 2022
batch 60, train loss = 0.36, mean loss = 0.56
Mon Jan 31 04:38:21 2022
train Loss: 0.56

batch 0, val loss = 0.11, mean loss = 0.11
Mon Jan 31 04:39:12 2022
batch 10, val loss = 0.12, mean loss = 0.12
Mon Jan 31 04:40:07 2022
val Loss: 0.13

epoch 33 was done for 475.470744 seconds
Epoch 34/299
----------
Mon Jan 31 04:40:45 2022
batch 0, train loss = 0.33, mean loss = 0.33
Mon Jan 31 04:40:50 2022
batch 10, train loss = 0.36, mean loss = 0.56
Mon Jan 31 04:41:44 2022
batch 20, train loss = 0.35, mean loss = 0.56
Mon Jan 31 04:42:38 2022
batch 30, train loss = 0.29, mean loss = 0.55
Mon Jan 31 04:43:32 2022
batch 40, train loss = 0.39, mean loss = 0.56
Mon Jan 31 04:44:25 2022
batch 50, train loss = 0.38, mean loss = 0.57
Mon Jan 31 04:45:19 2022
batch 60, train loss = 0.39, mean loss = 0.57
Mon Jan 31 04:46:13 2022
train Loss: 0.57

batch 0, val loss = 0.13, mean loss = 0.13
Mon Jan 31 04:47:03 2022
batch 10, val loss = 0.12, mean loss = 0.13
Mon Jan 31 04:47:58 2022
val Loss: 0.13

epoch 34 was done for 470.170258 seconds
Epoch 35/299
----------
Mon Jan 31 04:48:35 2022
batch 0, train loss = 0.32, mean loss = 0.32
Mon Jan 31 04:48:40 2022
batch 10, train loss = 0.40, mean loss = 0.57
Mon Jan 31 04:49:34 2022
batch 20, train loss = 0.43, mean loss = 0.60
Mon Jan 31 04:50:29 2022
batch 30, train loss = 0.43, mean loss = 0.60
Mon Jan 31 04:51:23 2022
batch 40, train loss = 0.34, mean loss = 0.59
Mon Jan 31 04:52:17 2022
batch 50, train loss = 0.39, mean loss = 0.59
Mon Jan 31 04:53:10 2022
batch 60, train loss = 0.41, mean loss = 0.59
Mon Jan 31 04:54:04 2022
train Loss: 0.58

batch 0, val loss = 0.13, mean loss = 0.13
Mon Jan 31 04:54:55 2022
batch 10, val loss = 0.13, mean loss = 0.13
Mon Jan 31 04:55:49 2022
val Loss: 0.13

epoch 35 was done for 471.736324 seconds
Epoch 36/299
----------
Mon Jan 31 04:56:27 2022
batch 0, train loss = 0.32, mean loss = 0.32
Mon Jan 31 04:56:32 2022
batch 10, train loss = 0.38, mean loss = 0.57
Mon Jan 31 04:57:26 2022
batch 20, train loss = 0.42, mean loss = 0.60
Mon Jan 31 04:58:19 2022
batch 30, train loss = 0.44, mean loss = 0.60
Mon Jan 31 04:59:13 2022
batch 40, train loss = 0.35, mean loss = 0.59
Mon Jan 31 05:00:06 2022
batch 50, train loss = 0.51, mean loss = 0.61
Mon Jan 31 05:01:00 2022
batch 60, train loss = 0.47, mean loss = 0.62
Mon Jan 31 05:01:54 2022
train Loss: 0.64

batch 0, val loss = 0.12, mean loss = 0.12
Mon Jan 31 05:02:44 2022
batch 10, val loss = 0.12, mean loss = 0.12
Mon Jan 31 05:03:39 2022
val Loss: 0.12

epoch 36 was done for 470.338842 seconds
Epoch 37/299
----------
Mon Jan 31 05:04:17 2022
batch 0, train loss = 0.59, mean loss = 0.59
Mon Jan 31 05:04:23 2022
batch 10, train loss = 0.60, mean loss = 0.67
Mon Jan 31 05:05:16 2022
batch 20, train loss = 0.31, mean loss = 0.60
Mon Jan 31 05:06:10 2022
batch 30, train loss = 0.49, mean loss = 0.62
Mon Jan 31 05:07:04 2022
batch 40, train loss = 0.53, mean loss = 0.64
Mon Jan 31 05:07:58 2022
batch 50, train loss = 0.43, mean loss = 0.63
Mon Jan 31 05:08:51 2022
batch 60, train loss = 0.37, mean loss = 0.62
Mon Jan 31 05:09:45 2022
train Loss: 0.64

batch 0, val loss = 0.12, mean loss = 0.12
Mon Jan 31 05:10:35 2022
batch 10, val loss = 0.12, mean loss = 0.12
Mon Jan 31 05:11:29 2022
val Loss: 0.12

epoch 37 was done for 469.770891 seconds
Epoch 38/299
----------
Mon Jan 31 05:12:07 2022
batch 0, train loss = 0.54, mean loss = 0.54
Mon Jan 31 05:12:12 2022
batch 10, train loss = 0.53, mean loss = 0.66
Mon Jan 31 05:13:06 2022
batch 20, train loss = 0.37, mean loss = 0.63
Mon Jan 31 05:14:00 2022
batch 30, train loss = 0.43, mean loss = 0.63
Mon Jan 31 05:14:54 2022
batch 40, train loss = 0.52, mean loss = 0.65
Mon Jan 31 05:15:47 2022
batch 50, train loss = 0.52, mean loss = 0.65
Mon Jan 31 05:16:41 2022
batch 60, train loss = 0.43, mean loss = 0.65
Mon Jan 31 05:17:35 2022
train Loss: 0.64

batch 0, val loss = 0.13, mean loss = 0.13
Mon Jan 31 05:18:25 2022
batch 10, val loss = 0.13, mean loss = 0.13
Mon Jan 31 05:19:20 2022
val Loss: 0.13

epoch 38 was done for 470.164707 seconds
Epoch 39/299
----------
Mon Jan 31 05:19:57 2022
batch 0, train loss = 0.44, mean loss = 0.44
Mon Jan 31 05:20:02 2022
batch 10, train loss = 0.50, mean loss = 0.63
Mon Jan 31 05:20:56 2022
batch 20, train loss = 0.47, mean loss = 0.65
Mon Jan 31 05:21:50 2022
batch 30, train loss = 0.54, mean loss = 0.66
Mon Jan 31 05:22:44 2022
batch 40, train loss = 0.45, mean loss = 0.65
Mon Jan 31 05:23:38 2022
batch 50, train loss = 0.53, mean loss = 0.66
Mon Jan 31 05:24:31 2022
batch 60, train loss = 0.48, mean loss = 0.65
Mon Jan 31 05:25:25 2022
train Loss: 0.66

batch 0, val loss = 0.13, mean loss = 0.13
Mon Jan 31 05:26:15 2022
batch 10, val loss = 0.13, mean loss = 0.13
Mon Jan 31 05:27:11 2022
val Loss: 0.12

epoch 39 was done for 471.677444 seconds
Epoch 40/299
----------
Mon Jan 31 05:27:49 2022
batch 0, train loss = 0.57, mean loss = 0.57
Mon Jan 31 05:27:54 2022
batch 10, train loss = 0.52, mean loss = 0.67
Mon Jan 31 05:28:48 2022
batch 20, train loss = 0.42, mean loss = 0.64
Mon Jan 31 05:29:42 2022
batch 30, train loss = 0.47, mean loss = 0.63
Mon Jan 31 05:30:36 2022
batch 40, train loss = 0.51, mean loss = 0.64
Mon Jan 31 05:31:29 2022
batch 50, train loss = 0.44, mean loss = 0.63
Mon Jan 31 05:32:23 2022
batch 60, train loss = 0.46, mean loss = 0.64
Mon Jan 31 05:33:17 2022
train Loss: 0.64

batch 0, val loss = 0.12, mean loss = 0.12
Mon Jan 31 05:34:07 2022
batch 10, val loss = 0.13, mean loss = 0.12
Mon Jan 31 05:35:02 2022
val Loss: 0.12

epoch 40 was done for 470.919240 seconds
Epoch 41/299
----------
Mon Jan 31 05:35:40 2022
batch 0, train loss = 0.51, mean loss = 0.51
Mon Jan 31 05:35:45 2022
batch 10, train loss = 0.42, mean loss = 0.61
Mon Jan 31 05:36:39 2022
batch 20, train loss = 0.41, mean loss = 0.63
Mon Jan 31 05:37:33 2022
batch 30, train loss = 0.46, mean loss = 0.63
Mon Jan 31 05:38:27 2022
batch 40, train loss = 0.37, mean loss = 0.61
Mon Jan 31 05:39:21 2022
batch 50, train loss = 0.39, mean loss = 0.61
Mon Jan 31 05:40:15 2022
batch 60, train loss = 0.41, mean loss = 0.61
Mon Jan 31 05:41:09 2022
train Loss: 0.61

batch 0, val loss = 0.13, mean loss = 0.13
Mon Jan 31 05:41:59 2022
batch 10, val loss = 0.13, mean loss = 0.13
Mon Jan 31 05:42:54 2022
val Loss: 0.13

epoch 41 was done for 471.690910 seconds
Epoch 42/299
----------
Mon Jan 31 05:43:31 2022
batch 0, train loss = 0.40, mean loss = 0.40
Mon Jan 31 05:43:37 2022
batch 10, train loss = 0.42, mean loss = 0.58
Mon Jan 31 05:44:30 2022
batch 20, train loss = 0.38, mean loss = 0.60
Mon Jan 31 05:45:23 2022
batch 30, train loss = 0.39, mean loss = 0.60
Mon Jan 31 05:46:17 2022
batch 40, train loss = 0.38, mean loss = 0.59
Mon Jan 31 05:47:10 2022
batch 50, train loss = 0.37, mean loss = 0.59
Mon Jan 31 05:48:04 2022
batch 60, train loss = 0.33, mean loss = 0.59
Mon Jan 31 05:48:57 2022
train Loss: 0.59

batch 0, val loss = 0.13, mean loss = 0.13
Mon Jan 31 05:49:47 2022
batch 10, val loss = 0.13, mean loss = 0.13
Mon Jan 31 05:50:43 2022
val Loss: 0.13

epoch 42 was done for 468.958500 seconds
Epoch 43/299
----------
Mon Jan 31 05:51:20 2022
batch 0, train loss = 0.44, mean loss = 0.44
Mon Jan 31 05:51:26 2022
batch 10, train loss = 0.32, mean loss = 0.55
Mon Jan 31 05:52:19 2022
batch 20, train loss = 0.34, mean loss = 0.55
Mon Jan 31 05:53:13 2022
batch 30, train loss = 0.34, mean loss = 0.56
Mon Jan 31 05:54:07 2022
batch 40, train loss = 0.32, mean loss = 0.56
Mon Jan 31 05:55:01 2022
batch 50, train loss = 0.30, mean loss = 0.56
Mon Jan 31 05:55:55 2022
batch 60, train loss = 0.39, mean loss = 0.57
Mon Jan 31 05:56:49 2022
train Loss: 0.58

batch 0, val loss = 0.14, mean loss = 0.14
Mon Jan 31 05:57:39 2022
batch 10, val loss = 0.13, mean loss = 0.13
Mon Jan 31 05:58:34 2022
val Loss: 0.13

epoch 43 was done for 470.727495 seconds
Epoch 44/299
----------
Mon Jan 31 05:59:11 2022
batch 0, train loss = 0.42, mean loss = 0.42
Mon Jan 31 05:59:16 2022
batch 10, train loss = 0.29, mean loss = 0.53
Mon Jan 31 06:00:11 2022
batch 20, train loss = 0.32, mean loss = 0.55
Mon Jan 31 06:01:06 2022
batch 30, train loss = 0.39, mean loss = 0.57
Mon Jan 31 06:02:00 2022
batch 40, train loss = 0.33, mean loss = 0.57
Mon Jan 31 06:02:54 2022
batch 50, train loss = 0.26, mean loss = 0.56
Mon Jan 31 06:03:48 2022
batch 60, train loss = 0.35, mean loss = 0.57
Mon Jan 31 06:04:43 2022
train Loss: 0.58

batch 0, val loss = 0.12, mean loss = 0.12
Mon Jan 31 06:05:33 2022
batch 10, val loss = 0.12, mean loss = 0.13
Mon Jan 31 06:06:29 2022
val Loss: 0.13

epoch 44 was done for 475.166698 seconds
Epoch 45/299
----------
Mon Jan 31 06:07:06 2022
batch 0, train loss = 0.36, mean loss = 0.36
Mon Jan 31 06:07:12 2022
batch 10, train loss = 0.33, mean loss = 0.53
Mon Jan 31 06:08:05 2022
batch 20, train loss = 0.28, mean loss = 0.54
Mon Jan 31 06:08:59 2022
batch 30, train loss = 0.41, mean loss = 0.58
Mon Jan 31 06:09:52 2022
batch 40, train loss = 0.37, mean loss = 0.59
Mon Jan 31 06:10:46 2022
batch 50, train loss = 0.28, mean loss = 0.58
Mon Jan 31 06:11:39 2022
batch 60, train loss = 0.34, mean loss = 0.58
Mon Jan 31 06:12:33 2022
train Loss: 0.59

batch 0, val loss = 0.14, mean loss = 0.14
Mon Jan 31 06:13:22 2022
batch 10, val loss = 0.12, mean loss = 0.13
Mon Jan 31 06:14:17 2022
val Loss: 0.13

epoch 45 was done for 467.433933 seconds
Epoch 46/299
----------
Mon Jan 31 06:14:54 2022
batch 0, train loss = 0.38, mean loss = 0.38
Mon Jan 31 06:14:59 2022
batch 10, train loss = 0.39, mean loss = 0.59
Mon Jan 31 06:15:53 2022
batch 20, train loss = 0.26, mean loss = 0.56
Mon Jan 31 06:16:47 2022
batch 30, train loss = 0.40, mean loss = 0.60
Mon Jan 31 06:17:40 2022
batch 40, train loss = 0.46, mean loss = 0.62
Mon Jan 31 06:18:34 2022
batch 50, train loss = 0.40, mean loss = 0.62
Mon Jan 31 06:19:28 2022
batch 60, train loss = 0.26, mean loss = 0.60
Mon Jan 31 06:20:22 2022
train Loss: 0.62

batch 0, val loss = 0.13, mean loss = 0.13
Mon Jan 31 06:21:12 2022
batch 10, val loss = 0.13, mean loss = 0.13
Mon Jan 31 06:22:07 2022
val Loss: 0.13

epoch 46 was done for 470.670493 seconds
Epoch 47/299
----------
Mon Jan 31 06:22:44 2022
batch 0, train loss = 0.46, mean loss = 0.46
Mon Jan 31 06:22:50 2022
batch 10, train loss = 0.53, mean loss = 0.71
Mon Jan 31 06:23:44 2022
batch 20, train loss = 0.47, mean loss = 0.69
Mon Jan 31 06:24:39 2022
batch 30, train loss = 0.39, mean loss = 0.66
Mon Jan 31 06:25:33 2022
batch 40, train loss = 0.37, mean loss = 0.65
Mon Jan 31 06:26:28 2022
batch 50, train loss = 0.51, mean loss = 0.67
Mon Jan 31 06:27:23 2022
batch 60, train loss = 0.53, mean loss = 0.67
Mon Jan 31 06:28:17 2022
train Loss: 0.67

batch 0, val loss = 0.11, mean loss = 0.11
Mon Jan 31 06:29:08 2022
batch 10, val loss = 0.12, mean loss = 0.12
Mon Jan 31 06:30:03 2022
val Loss: 0.12

epoch 47 was done for 476.952926 seconds
Epoch 48/299
----------
Mon Jan 31 06:30:41 2022
batch 0, train loss = 0.39, mean loss = 0.39
Mon Jan 31 06:30:47 2022
batch 10, train loss = 0.42, mean loss = 0.64
Mon Jan 31 06:31:40 2022
batch 20, train loss = 0.55, mean loss = 0.69
Mon Jan 31 06:32:33 2022
batch 30, train loss = 0.54, mean loss = 0.69
Mon Jan 31 06:33:27 2022
batch 40, train loss = 0.31, mean loss = 0.65
Mon Jan 31 06:34:28 2022
batch 50, train loss = 0.50, mean loss = 0.66
Mon Jan 31 06:35:27 2022
batch 60, train loss = 0.60, mean loss = 0.68
Mon Jan 31 06:36:22 2022
train Loss: 0.69

batch 0, val loss = 0.12, mean loss = 0.12
Mon Jan 31 06:37:13 2022
batch 10, val loss = 0.13, mean loss = 0.13
Mon Jan 31 06:38:08 2022
val Loss: 0.12

epoch 48 was done for 483.902046 seconds
Epoch 49/299
----------
Mon Jan 31 06:38:45 2022
batch 0, train loss = 0.60, mean loss = 0.60
Mon Jan 31 06:38:51 2022
batch 10, train loss = 0.38, mean loss = 0.57
Mon Jan 31 06:39:45 2022
batch 20, train loss = 0.45, mean loss = 0.62
Mon Jan 31 06:40:39 2022
batch 30, train loss = 0.70, mean loss = 0.69
Mon Jan 31 06:41:33 2022
batch 40, train loss = 0.54, mean loss = 0.69
Mon Jan 31 06:42:27 2022
batch 50, train loss = 0.33, mean loss = 0.67
Mon Jan 31 06:43:21 2022
batch 60, train loss = 0.49, mean loss = 0.67
Mon Jan 31 06:44:15 2022
train Loss: 0.68

batch 0, val loss = 0.12, mean loss = 0.12
Mon Jan 31 06:45:06 2022
batch 10, val loss = 0.12, mean loss = 0.12
Mon Jan 31 06:46:01 2022
val Loss: 0.13

epoch 49 was done for 473.215825 seconds
Epoch 50/299
----------
Mon Jan 31 06:46:38 2022
batch 0, train loss = 0.59, mean loss = 0.59
Mon Jan 31 06:46:44 2022
batch 10, train loss = 0.45, mean loss = 0.63
Mon Jan 31 06:47:38 2022
batch 20, train loss = 0.35, mean loss = 0.61
Mon Jan 31 06:48:33 2022
batch 30, train loss = 0.50, mean loss = 0.63
Mon Jan 31 06:49:27 2022
batch 40, train loss = 0.47, mean loss = 0.64
Mon Jan 31 06:50:22 2022
batch 50, train loss = 0.38, mean loss = 0.63
Mon Jan 31 06:51:16 2022
batch 60, train loss = 0.39, mean loss = 0.62
Mon Jan 31 06:52:11 2022
train Loss: 0.63

batch 0, val loss = 0.12, mean loss = 0.12
Mon Jan 31 06:53:02 2022
batch 10, val loss = 0.13, mean loss = 0.13
Mon Jan 31 06:53:57 2022
val Loss: 0.13

epoch 50 was done for 476.063939 seconds
Epoch 51/299
----------
Mon Jan 31 06:54:34 2022
batch 0, train loss = 0.51, mean loss = 0.51
Mon Jan 31 06:54:40 2022
batch 10, train loss = 0.50, mean loss = 0.65
Mon Jan 31 06:55:34 2022
batch 20, train loss = 0.39, mean loss = 0.62
Mon Jan 31 06:56:28 2022
batch 30, train loss = 0.38, mean loss = 0.61
Mon Jan 31 06:57:22 2022
batch 40, train loss = 0.48, mean loss = 0.62
Mon Jan 31 06:58:15 2022
batch 50, train loss = 0.48, mean loss = 0.63
Mon Jan 31 06:59:09 2022
batch 60, train loss = 0.35, mean loss = 0.61
Mon Jan 31 07:00:03 2022
train Loss: 0.61

batch 0, val loss = 0.13, mean loss = 0.13
Mon Jan 31 07:00:53 2022
batch 10, val loss = 0.12, mean loss = 0.13
Mon Jan 31 07:01:48 2022
val Loss: 0.13

epoch 51 was done for 470.597474 seconds
Epoch 52/299
----------
Mon Jan 31 07:02:25 2022
batch 0, train loss = 0.36, mean loss = 0.36
Mon Jan 31 07:02:30 2022
batch 10, train loss = 0.45, mean loss = 0.62
Mon Jan 31 07:03:24 2022
batch 20, train loss = 0.46, mean loss = 0.64
Mon Jan 31 07:04:18 2022
batch 30, train loss = 0.38, mean loss = 0.62
Mon Jan 31 07:05:12 2022
batch 40, train loss = 0.39, mean loss = 0.62
Mon Jan 31 07:06:06 2022
batch 50, train loss = 0.53, mean loss = 0.64
Mon Jan 31 07:07:00 2022
batch 60, train loss = 0.43, mean loss = 0.63
Mon Jan 31 07:07:54 2022
train Loss: 0.63

batch 0, val loss = 0.12, mean loss = 0.12
Mon Jan 31 07:08:44 2022
batch 10, val loss = 0.13, mean loss = 0.12
Mon Jan 31 07:09:38 2022
val Loss: 0.12

epoch 52 was done for 470.614512 seconds
Epoch 53/299
----------
Mon Jan 31 07:10:16 2022
batch 0, train loss = 0.32, mean loss = 0.32
Mon Jan 31 07:10:21 2022
batch 10, train loss = 0.48, mean loss = 0.66
Mon Jan 31 07:11:16 2022
batch 20, train loss = 0.50, mean loss = 0.67
Mon Jan 31 07:12:10 2022
batch 30, train loss = 0.50, mean loss = 0.66
Mon Jan 31 07:13:05 2022
batch 40, train loss = 0.36, mean loss = 0.64
Mon Jan 31 07:14:00 2022
batch 50, train loss = 0.48, mean loss = 0.64
Mon Jan 31 07:14:54 2022
batch 60, train loss = 0.54, mean loss = 0.65
Mon Jan 31 07:15:49 2022
train Loss: 0.66

batch 0, val loss = 0.12, mean loss = 0.12
Mon Jan 31 07:16:40 2022
batch 10, val loss = 0.11, mean loss = 0.12
Mon Jan 31 07:17:35 2022
val Loss: 0.12

epoch 53 was done for 477.469279 seconds
Epoch 54/299
----------
Mon Jan 31 07:18:13 2022
batch 0, train loss = 0.54, mean loss = 0.54
Mon Jan 31 07:18:18 2022
batch 10, train loss = 0.48, mean loss = 0.62
Mon Jan 31 07:19:12 2022
batch 20, train loss = 0.36, mean loss = 0.60
Mon Jan 31 07:20:06 2022
batch 30, train loss = 0.47, mean loss = 0.62
Mon Jan 31 07:21:00 2022
batch 40, train loss = 0.62, mean loss = 0.65
Mon Jan 31 07:21:54 2022
batch 50, train loss = 0.60, mean loss = 0.66
Mon Jan 31 07:22:47 2022
batch 60, train loss = 0.47, mean loss = 0.65
Mon Jan 31 07:23:41 2022
train Loss: 0.65

batch 0, val loss = 0.13, mean loss = 0.13
Mon Jan 31 07:24:31 2022
batch 10, val loss = 0.12, mean loss = 0.13
Mon Jan 31 07:25:26 2022
val Loss: 0.13

epoch 54 was done for 469.962347 seconds
Epoch 55/299
----------
Mon Jan 31 07:26:03 2022
batch 0, train loss = 0.47, mean loss = 0.47
Mon Jan 31 07:26:08 2022
batch 10, train loss = 0.61, mean loss = 0.67
Mon Jan 31 07:27:03 2022
batch 20, train loss = 0.49, mean loss = 0.66
Mon Jan 31 07:27:57 2022
batch 30, train loss = 0.50, mean loss = 0.65
Mon Jan 31 07:28:52 2022
batch 40, train loss = 0.53, mean loss = 0.65
Mon Jan 31 07:29:46 2022
batch 50, train loss = 0.45, mean loss = 0.64
Mon Jan 31 07:30:40 2022
batch 60, train loss = 0.40, mean loss = 0.63
Mon Jan 31 07:31:34 2022
train Loss: 0.63

batch 0, val loss = 0.11, mean loss = 0.11
Mon Jan 31 07:32:26 2022
batch 10, val loss = 0.12, mean loss = 0.12
Mon Jan 31 07:33:21 2022
val Loss: 0.12

epoch 55 was done for 475.619996 seconds
Epoch 56/299
----------
Mon Jan 31 07:33:59 2022
batch 0, train loss = 0.48, mean loss = 0.48
Mon Jan 31 07:34:04 2022
batch 10, train loss = 0.57, mean loss = 0.63
Mon Jan 31 07:34:58 2022
batch 20, train loss = 0.37, mean loss = 0.60
Mon Jan 31 07:35:51 2022
batch 30, train loss = 0.35, mean loss = 0.57
Mon Jan 31 07:36:45 2022
batch 40, train loss = 0.47, mean loss = 0.58
Mon Jan 31 07:37:39 2022
batch 50, train loss = 0.49, mean loss = 0.59
Mon Jan 31 07:38:33 2022
batch 60, train loss = 0.37, mean loss = 0.58
Mon Jan 31 07:39:27 2022
train Loss: 0.59

batch 0, val loss = 0.13, mean loss = 0.13
Mon Jan 31 07:40:22 2022
batch 10, val loss = 0.12, mean loss = 0.12
Mon Jan 31 07:41:19 2022
val Loss: 0.12

epoch 56 was done for 478.628211 seconds
Epoch 57/299
----------
Mon Jan 31 07:41:57 2022
batch 0, train loss = 0.41, mean loss = 0.41
Mon Jan 31 07:42:03 2022
batch 10, train loss = 0.52, mean loss = 0.59
Mon Jan 31 07:42:56 2022
batch 20, train loss = 0.42, mean loss = 0.59
Mon Jan 31 07:43:50 2022
batch 30, train loss = 0.41, mean loss = 0.59
Mon Jan 31 07:44:44 2022
batch 40, train loss = 0.41, mean loss = 0.58
Mon Jan 31 07:45:43 2022
batch 50, train loss = 0.42, mean loss = 0.58
Mon Jan 31 07:46:37 2022
batch 60, train loss = 0.47, mean loss = 0.59
Mon Jan 31 07:47:33 2022
train Loss: 0.59

batch 0, val loss = 0.12, mean loss = 0.12
Mon Jan 31 07:48:23 2022
batch 10, val loss = 0.13, mean loss = 0.12
Mon Jan 31 07:49:18 2022
val Loss: 0.12

epoch 57 was done for 478.044228 seconds
Epoch 58/299
----------
Mon Jan 31 07:49:55 2022
batch 0, train loss = 0.43, mean loss = 0.43
Mon Jan 31 07:50:01 2022
batch 10, train loss = 0.41, mean loss = 0.55
Mon Jan 31 07:50:55 2022
batch 20, train loss = 0.38, mean loss = 0.56
Mon Jan 31 07:51:49 2022
batch 30, train loss = 0.48, mean loss = 0.57
Mon Jan 31 07:52:43 2022
batch 40, train loss = 0.47, mean loss = 0.57
Mon Jan 31 07:53:37 2022
batch 50, train loss = 0.37, mean loss = 0.57
Mon Jan 31 07:54:31 2022
batch 60, train loss = 0.43, mean loss = 0.57
Mon Jan 31 07:55:28 2022
train Loss: 0.58

batch 0, val loss = 0.13, mean loss = 0.13
Mon Jan 31 07:56:19 2022
batch 10, val loss = 0.12, mean loss = 0.13
Mon Jan 31 07:57:14 2022
val Loss: 0.13

epoch 58 was done for 476.900682 seconds
Epoch 59/299
----------
Mon Jan 31 07:57:52 2022
batch 0, train loss = 0.46, mean loss = 0.46
Mon Jan 31 07:57:58 2022
batch 10, train loss = 0.41, mean loss = 0.55
Mon Jan 31 07:58:52 2022
batch 20, train loss = 0.36, mean loss = 0.55
Mon Jan 31 07:59:47 2022
batch 30, train loss = 0.44, mean loss = 0.56
Mon Jan 31 08:00:42 2022
batch 40, train loss = 0.43, mean loss = 0.56
Mon Jan 31 08:01:38 2022
batch 50, train loss = 0.38, mean loss = 0.55
Mon Jan 31 08:02:37 2022
batch 60, train loss = 0.34, mean loss = 0.55
Mon Jan 31 08:03:39 2022
train Loss: 0.55

batch 0, val loss = 0.11, mean loss = 0.11
Mon Jan 31 08:04:36 2022
batch 10, val loss = 0.13, mean loss = 0.12
Mon Jan 31 08:05:34 2022
val Loss: 0.12

epoch 59 was done for 500.001831 seconds
Epoch 60/299
----------
Mon Jan 31 08:06:12 2022
batch 0, train loss = 0.40, mean loss = 0.40
Mon Jan 31 08:06:19 2022
batch 10, train loss = 0.40, mean loss = 0.52
Mon Jan 31 08:07:16 2022
batch 20, train loss = 0.32, mean loss = 0.52
Mon Jan 31 08:08:11 2022
batch 30, train loss = 0.36, mean loss = 0.54
Mon Jan 31 08:09:05 2022
batch 40, train loss = 0.43, mean loss = 0.55
Mon Jan 31 08:09:59 2022
batch 50, train loss = 0.35, mean loss = 0.54
Mon Jan 31 08:10:53 2022
batch 60, train loss = 0.30, mean loss = 0.54
Mon Jan 31 08:11:47 2022
train Loss: 0.55

batch 0, val loss = 0.12, mean loss = 0.12
Mon Jan 31 08:12:37 2022
batch 10, val loss = 0.13, mean loss = 0.12
Mon Jan 31 08:13:32 2022
val Loss: 0.12

epoch 60 was done for 476.434815 seconds
Epoch 61/299
----------
Mon Jan 31 08:14:09 2022
batch 0, train loss = 0.42, mean loss = 0.42
Mon Jan 31 08:14:14 2022
batch 10, train loss = 0.46, mean loss = 0.56
Mon Jan 31 08:15:08 2022
batch 20, train loss = 0.28, mean loss = 0.53
Mon Jan 31 08:16:03 2022
batch 30, train loss = 0.35, mean loss = 0.54
Mon Jan 31 08:16:57 2022
batch 40, train loss = 0.47, mean loss = 0.56
Mon Jan 31 08:17:51 2022
batch 50, train loss = 0.45, mean loss = 0.57
Mon Jan 31 08:18:45 2022
batch 60, train loss = 0.29, mean loss = 0.56
Mon Jan 31 08:19:40 2022
train Loss: 0.56

batch 0, val loss = 0.12, mean loss = 0.12
Mon Jan 31 08:20:30 2022
batch 10, val loss = 0.14, mean loss = 0.13
Mon Jan 31 08:21:26 2022
val Loss: 0.13

epoch 61 was done for 474.655496 seconds
Epoch 62/299
----------
Mon Jan 31 08:22:03 2022
batch 0, train loss = 0.35, mean loss = 0.35
Mon Jan 31 08:22:09 2022
batch 10, train loss = 0.47, mean loss = 0.59
Mon Jan 31 08:23:02 2022
batch 20, train loss = 0.37, mean loss = 0.59
Mon Jan 31 08:23:55 2022
batch 30, train loss = 0.37, mean loss = 0.57
Mon Jan 31 08:24:49 2022
batch 40, train loss = 0.34, mean loss = 0.57
Mon Jan 31 08:25:42 2022
batch 50, train loss = 0.44, mean loss = 0.58
Mon Jan 31 08:26:35 2022
batch 60, train loss = 0.44, mean loss = 0.59
Mon Jan 31 08:27:28 2022
train Loss: 0.60

batch 0, val loss = 0.13, mean loss = 0.13
Mon Jan 31 08:28:22 2022
batch 10, val loss = 0.12, mean loss = 0.13
Mon Jan 31 08:29:16 2022
val Loss: 0.13

epoch 62 was done for 471.400252 seconds
Epoch 63/299
----------
Mon Jan 31 08:29:55 2022
batch 0, train loss = 0.44, mean loss = 0.44
Mon Jan 31 08:30:00 2022
batch 10, train loss = 0.35, mean loss = 0.54
Mon Jan 31 08:30:54 2022
batch 20, train loss = 0.42, mean loss = 0.58
Mon Jan 31 08:31:51 2022
batch 30, train loss = 0.47, mean loss = 0.60
Mon Jan 31 08:32:47 2022
batch 40, train loss = 0.44, mean loss = 0.60
Mon Jan 31 08:33:42 2022
batch 50, train loss = 0.44, mean loss = 0.60
Mon Jan 31 08:34:43 2022
batch 60, train loss = 0.43, mean loss = 0.60
Mon Jan 31 08:35:45 2022
train Loss: 0.60

batch 0, val loss = 0.12, mean loss = 0.12
Mon Jan 31 08:36:41 2022
batch 10, val loss = 0.13, mean loss = 0.13
Mon Jan 31 08:37:41 2022
val Loss: 0.13

epoch 63 was done for 506.338985 seconds
Epoch 64/299
----------
Mon Jan 31 08:38:21 2022
batch 0, train loss = 0.45, mean loss = 0.45
Mon Jan 31 08:38:27 2022
batch 10, train loss = 0.38, mean loss = 0.56
Mon Jan 31 08:39:24 2022
batch 20, train loss = 0.42, mean loss = 0.57
Mon Jan 31 08:40:19 2022
batch 30, train loss = 0.44, mean loss = 0.58
Mon Jan 31 08:41:13 2022
batch 40, train loss = 0.41, mean loss = 0.57
Mon Jan 31 08:42:07 2022
batch 50, train loss = 0.32, mean loss = 0.56
Mon Jan 31 08:43:01 2022
batch 60, train loss = 0.37, mean loss = 0.57
Mon Jan 31 08:44:00 2022
train Loss: 0.58

batch 0, val loss = 0.12, mean loss = 0.12
Mon Jan 31 08:45:00 2022
batch 10, val loss = 0.12, mean loss = 0.12
Mon Jan 31 08:46:03 2022
val Loss: 0.13

epoch 64 was done for 504.542514 seconds
Epoch 65/299
----------
Mon Jan 31 08:46:46 2022
batch 0, train loss = 0.52, mean loss = 0.52
Mon Jan 31 08:46:52 2022
batch 10, train loss = 0.44, mean loss = 0.60
Mon Jan 31 08:47:56 2022
batch 20, train loss = 0.35, mean loss = 0.58
Mon Jan 31 08:48:59 2022
batch 30, train loss = 0.35, mean loss = 0.57
Mon Jan 31 08:50:03 2022
batch 40, train loss = 0.43, mean loss = 0.57
Mon Jan 31 08:51:12 2022
batch 50, train loss = 0.38, mean loss = 0.56
Mon Jan 31 08:52:17 2022
batch 60, train loss = 0.30, mean loss = 0.56
Mon Jan 31 08:53:22 2022
train Loss: 0.56

batch 0, val loss = 0.13, mean loss = 0.13
Mon Jan 31 08:54:24 2022
batch 10, val loss = 0.13, mean loss = 0.13
Mon Jan 31 08:55:29 2022
val Loss: 0.13

epoch 65 was done for 568.786475 seconds
Epoch 66/299
----------
Mon Jan 31 08:56:14 2022
batch 0, train loss = 0.41, mean loss = 0.41
Mon Jan 31 08:56:21 2022
batch 10, train loss = 0.43, mean loss = 0.58
Mon Jan 31 08:57:28 2022
batch 20, train loss = 0.37, mean loss = 0.57
Mon Jan 31 08:58:35 2022
batch 30, train loss = 0.28, mean loss = 0.55
Mon Jan 31 08:59:42 2022
batch 40, train loss = 0.36, mean loss = 0.54
Mon Jan 31 09:00:48 2022
batch 50, train loss = 0.41, mean loss = 0.55
Mon Jan 31 09:01:56 2022
batch 60, train loss = 0.35, mean loss = 0.54
Mon Jan 31 09:03:03 2022
train Loss: 0.54

batch 0, val loss = 0.14, mean loss = 0.14
Mon Jan 31 09:04:07 2022
batch 10, val loss = 0.13, mean loss = 0.13
Mon Jan 31 09:05:14 2022
val Loss: 0.13

epoch 66 was done for 585.604373 seconds
Epoch 67/299
----------
Mon Jan 31 09:06:00 2022
batch 0, train loss = 0.35, mean loss = 0.35
Mon Jan 31 09:06:07 2022
batch 10, train loss = 0.36, mean loss = 0.53
Mon Jan 31 09:07:14 2022
batch 20, train loss = 0.32, mean loss = 0.54
Mon Jan 31 09:08:27 2022
batch 30, train loss = 0.35, mean loss = 0.54
Mon Jan 31 09:09:34 2022
batch 40, train loss = 0.36, mean loss = 0.54
Mon Jan 31 09:10:41 2022
batch 50, train loss = 0.34, mean loss = 0.54
Mon Jan 31 09:11:49 2022
batch 60, train loss = 0.32, mean loss = 0.53
Mon Jan 31 09:12:56 2022
train Loss: 0.54

batch 0, val loss = 0.14, mean loss = 0.14
Mon Jan 31 09:13:59 2022
batch 10, val loss = 0.12, mean loss = 0.13
Mon Jan 31 09:15:06 2022
val Loss: 0.13

epoch 67 was done for 590.962445 seconds
Epoch 68/299
----------
Mon Jan 31 09:15:51 2022
batch 0, train loss = 0.41, mean loss = 0.41
Mon Jan 31 09:15:58 2022
batch 10, train loss = 0.38, mean loss = 0.53
Mon Jan 31 09:17:04 2022
batch 20, train loss = 0.32, mean loss = 0.52
Mon Jan 31 09:18:12 2022
batch 30, train loss = 0.31, mean loss = 0.52
Mon Jan 31 09:19:19 2022
batch 40, train loss = 0.35, mean loss = 0.52
Mon Jan 31 09:20:26 2022
batch 50, train loss = 0.40, mean loss = 0.53
Mon Jan 31 09:21:33 2022
batch 60, train loss = 0.31, mean loss = 0.52
Mon Jan 31 09:22:41 2022
train Loss: 0.53

batch 0, val loss = 0.13, mean loss = 0.13
Mon Jan 31 09:23:44 2022
batch 10, val loss = 0.13, mean loss = 0.13
Mon Jan 31 09:24:51 2022
val Loss: 0.13

epoch 68 was done for 585.625082 seconds
Epoch 69/299
----------
Mon Jan 31 09:25:37 2022
batch 0, train loss = 0.34, mean loss = 0.34
Mon Jan 31 09:25:44 2022
batch 10, train loss = 0.36, mean loss = 0.51
Mon Jan 31 09:26:51 2022
batch 20, train loss = 0.34, mean loss = 0.52
Mon Jan 31 09:27:59 2022
batch 30, train loss = 0.28, mean loss = 0.51
Mon Jan 31 09:29:07 2022
batch 40, train loss = 0.26, mean loss = 0.51
Mon Jan 31 09:30:14 2022
batch 50, train loss = 0.35, mean loss = 0.51
Mon Jan 31 09:31:22 2022
batch 60, train loss = 0.29, mean loss = 0.52
Mon Jan 31 09:32:29 2022
train Loss: 0.52

batch 0, val loss = 0.14, mean loss = 0.14
Mon Jan 31 09:33:32 2022
batch 10, val loss = 0.13, mean loss = 0.12
Mon Jan 31 09:34:39 2022
val Loss: 0.12

epoch 69 was done for 587.870561 seconds
Epoch 70/299
----------
Mon Jan 31 09:35:24 2022
batch 0, train loss = 0.26, mean loss = 0.26
Mon Jan 31 09:35:31 2022
batch 10, train loss = 0.30, mean loss = 0.51
Mon Jan 31 09:36:39 2022
batch 20, train loss = 0.33, mean loss = 0.53
Mon Jan 31 09:37:46 2022
batch 30, train loss = 0.30, mean loss = 0.52
Mon Jan 31 09:38:53 2022
batch 40, train loss = 0.26, mean loss = 0.51
Mon Jan 31 09:40:00 2022
batch 50, train loss = 0.34, mean loss = 0.52
Mon Jan 31 09:41:08 2022
batch 60, train loss = 0.34, mean loss = 0.53
Mon Jan 31 09:42:15 2022
train Loss: 0.54

batch 0, val loss = 0.13, mean loss = 0.13
Mon Jan 31 09:43:18 2022
batch 10, val loss = 0.14, mean loss = 0.13
Mon Jan 31 09:44:25 2022
val Loss: 0.13

epoch 70 was done for 585.560471 seconds
Epoch 71/299
----------
Mon Jan 31 09:45:10 2022
batch 0, train loss = 0.37, mean loss = 0.37
Mon Jan 31 09:45:16 2022
batch 10, train loss = 0.30, mean loss = 0.50
Mon Jan 31 09:46:25 2022
batch 20, train loss = 0.26, mean loss = 0.49
Mon Jan 31 09:47:32 2022
batch 30, train loss = 0.31, mean loss = 0.51
Mon Jan 31 09:48:39 2022
batch 40, train loss = 0.34, mean loss = 0.51
Mon Jan 31 09:49:46 2022
batch 50, train loss = 0.32, mean loss = 0.52
Mon Jan 31 09:50:56 2022
batch 60, train loss = 0.25, mean loss = 0.52
Mon Jan 31 09:52:03 2022
train Loss: 0.52

batch 0, val loss = 0.12, mean loss = 0.12
Mon Jan 31 09:53:06 2022
batch 10, val loss = 0.12, mean loss = 0.12
Mon Jan 31 09:54:13 2022
val Loss: 0.12

epoch 71 was done for 588.300068 seconds
Epoch 72/299
----------
Mon Jan 31 09:54:58 2022
batch 0, train loss = 0.31, mean loss = 0.31
Mon Jan 31 09:55:05 2022
batch 10, train loss = 0.33, mean loss = 0.53
Mon Jan 31 09:56:11 2022
batch 20, train loss = 0.29, mean loss = 0.53
Mon Jan 31 09:57:18 2022
batch 30, train loss = 0.27, mean loss = 0.53
Mon Jan 31 09:58:25 2022
batch 40, train loss = 0.29, mean loss = 0.52
Mon Jan 31 09:59:33 2022
batch 50, train loss = 0.33, mean loss = 0.53
Mon Jan 31 10:00:42 2022
batch 60, train loss = 0.27, mean loss = 0.53
Mon Jan 31 10:01:48 2022
train Loss: 0.53

batch 0, val loss = 0.13, mean loss = 0.13
Mon Jan 31 10:02:51 2022
batch 10, val loss = 0.13, mean loss = 0.12
Mon Jan 31 10:03:58 2022
val Loss: 0.12

epoch 72 was done for 584.489198 seconds
Epoch 73/299
----------
Mon Jan 31 10:04:43 2022
batch 0, train loss = 0.26, mean loss = 0.26
Mon Jan 31 10:04:49 2022
batch 10, train loss = 0.32, mean loss = 0.52
Mon Jan 31 10:05:56 2022
batch 20, train loss = 0.33, mean loss = 0.54
Mon Jan 31 10:07:04 2022
batch 30, train loss = 0.26, mean loss = 0.53
Mon Jan 31 10:08:11 2022
batch 40, train loss = 0.26, mean loss = 0.52
Mon Jan 31 10:09:17 2022
batch 50, train loss = 0.34, mean loss = 0.52
Mon Jan 31 10:10:25 2022
batch 60, train loss = 0.34, mean loss = 0.53
Mon Jan 31 10:11:31 2022
train Loss: 0.53

batch 0, val loss = 0.12, mean loss = 0.12
Mon Jan 31 10:12:34 2022
batch 10, val loss = 0.13, mean loss = 0.12
Mon Jan 31 10:13:42 2022
val Loss: 0.12

epoch 73 was done for 584.865469 seconds
Epoch 74/299
----------
Mon Jan 31 10:14:28 2022
batch 0, train loss = 0.31, mean loss = 0.31
Mon Jan 31 10:14:34 2022
batch 10, train loss = 0.29, mean loss = 0.49
Mon Jan 31 10:15:40 2022
batch 20, train loss = 0.27, mean loss = 0.51
Mon Jan 31 10:16:46 2022
batch 30, train loss = 0.37, mean loss = 0.53
Mon Jan 31 10:17:52 2022
batch 40, train loss = 0.30, mean loss = 0.52
Mon Jan 31 10:18:59 2022
batch 50, train loss = 0.31, mean loss = 0.53
Mon Jan 31 10:20:06 2022
batch 60, train loss = 0.32, mean loss = 0.53
Mon Jan 31 10:21:13 2022
train Loss: 0.54

batch 0, val loss = 0.12, mean loss = 0.12
Mon Jan 31 10:22:18 2022
batch 10, val loss = 0.12, mean loss = 0.12
Mon Jan 31 10:23:25 2022
val Loss: 0.12

epoch 74 was done for 582.496900 seconds
Epoch 75/299
----------
Mon Jan 31 10:24:10 2022
batch 0, train loss = 0.35, mean loss = 0.35
Mon Jan 31 10:24:17 2022
batch 10, train loss = 0.31, mean loss = 0.51
Mon Jan 31 10:25:24 2022
batch 20, train loss = 0.30, mean loss = 0.53
Mon Jan 31 10:26:30 2022
batch 30, train loss = 0.35, mean loss = 0.54
Mon Jan 31 10:27:37 2022
batch 40, train loss = 0.36, mean loss = 0.55
Mon Jan 31 10:28:43 2022
batch 50, train loss = 0.35, mean loss = 0.55
Mon Jan 31 10:29:49 2022
batch 60, train loss = 0.26, mean loss = 0.54
Mon Jan 31 10:30:55 2022
train Loss: 0.55

batch 0, val loss = 0.12, mean loss = 0.12
Mon Jan 31 10:31:58 2022
batch 10, val loss = 0.12, mean loss = 0.12
Mon Jan 31 10:33:04 2022
val Loss: 0.12

epoch 75 was done for 578.437943 seconds
Epoch 76/299
----------
Mon Jan 31 10:33:49 2022
batch 0, train loss = 0.35, mean loss = 0.35
Mon Jan 31 10:33:55 2022
batch 10, train loss = 0.44, mean loss = 0.59
Mon Jan 31 10:35:02 2022
batch 20, train loss = 0.36, mean loss = 0.57
Mon Jan 31 10:36:09 2022
batch 30, train loss = 0.27, mean loss = 0.55
Mon Jan 31 10:37:15 2022
batch 40, train loss = 0.33, mean loss = 0.55
Mon Jan 31 10:38:21 2022
batch 50, train loss = 0.44, mean loss = 0.56
Mon Jan 31 10:39:27 2022
batch 60, train loss = 0.35, mean loss = 0.55
Mon Jan 31 10:40:32 2022
train Loss: 0.55

batch 0, val loss = 0.12, mean loss = 0.12
Mon Jan 31 10:41:35 2022
batch 10, val loss = 0.13, mean loss = 0.13
Mon Jan 31 10:42:41 2022
val Loss: 0.13

epoch 76 was done for 577.474657 seconds
Epoch 77/299
----------
Mon Jan 31 10:43:26 2022
batch 0, train loss = 0.23, mean loss = 0.23
Mon Jan 31 10:43:33 2022
batch 10, train loss = 0.40, mean loss = 0.56
Mon Jan 31 10:44:39 2022
batch 20, train loss = 0.45, mean loss = 0.60
Mon Jan 31 10:45:45 2022
batch 30, train loss = 0.36, mean loss = 0.58
Mon Jan 31 10:46:51 2022
batch 40, train loss = 0.23, mean loss = 0.56
Mon Jan 31 10:47:57 2022
batch 50, train loss = 0.40, mean loss = 0.57
Mon Jan 31 10:49:04 2022
batch 60, train loss = 0.44, mean loss = 0.57
Mon Jan 31 10:50:10 2022
train Loss: 0.57

batch 0, val loss = 0.14, mean loss = 0.14
Mon Jan 31 10:51:12 2022
