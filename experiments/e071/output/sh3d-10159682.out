The activation script must be sourced, otherwise the virtual environment will not work.
Setting vars
iscuda= True
opt.wandb =  
93
iscuda= True
opt.wandb =  
file to frame csv ../../csv/598frame.csv
93
file to frame csv ../../csv/598frame.csv
iscuda= True
iscuda= True
PyTorch Version:  1.8.1
opt.wandb =  
Torchvision Version:  0.9.0a0
opt:
 Namespace(ampl=441, aug_gt='orient', batch_output=2, bs=15, cencrop=700, center=False, chidden_dim=[96, 128, 256, 256, 256], classicnorm=False, cmscrop=0, conTrain='', criterion='L2', csvname='598csv9', datapath='/p/project/delia-mp/cherepashkin1/phenoseed/', dfname='598frame', downsample=1, epoch=300, expdescr='', expnum='e068', feature_extract=False, framelim=6000, gradient_predivide_factor=1.0, gttype='single_file', haf=True, hidden_dim=[9], inputt='img', kernel_sizes=[7, 3, 3, 3, 3, 3], lb='orient', loadh5=False, localexp='', lr=5e-05, machine='jureca', maintain=False, maintain_line=False, man_dist=False, measure_time=False, merging='batch', minmax=False, minmax3dimage=False, minmax_f=True, minmax_fn='', model_name='', netname=['cnet'], ngpu=4, noise_input=False, noise_output=False, normalize=False, num_input_images=3, num_sam_points=500, num_workers=0, outputt='orient', parallel='horovod', pin_memory=False, print_minibatch=10, pscale=100, rand_angle=False, rescale=500, rot_dirs=False, rotate_output=False, save_output=True, single_folder=False, specie='598', standardize=255, steplr=[1000.0, 1.0], transappendix='_image', ufmodel=100000, updateFraction=0.1, use_adasum=False, use_cuda=True, use_existing_csv=True, use_pretrained=False, use_sep_csv=True, view_sep=False, wandb='', weight_decay=0, zero_angle=True)
sys.argv:
 ['../../main.py', '-datapath', '/p/project/delia-mp/cherepashkin1/phenoseed/', '-epoch', '300', '-bs', '15', '-num_input_images', '3', '-framelim', '6000', '-criterion', 'L2', '-localexp', '', '-lr', '5e-5', '-expnum', 'e068', '-hidden_dim', '9', '-inputt', 'img', '-outputt', 'orient', '-lb', 'orient', '-no_loadh5', '-minmax_fn', '', '-parallel', 'horovod', '-machine', 'jureca', '-merging', 'batch', '-aug_gt', 'orient', '-updateFraction', '0.1', '-steplr', '1000', '1', '-print_minibatch', '10', '-dfname', '598frame']
seed =  0
path were main.py is located= ../../
opt.wandb =  
93
file to frame csv ../../csv/598frame.csv
93
file to frame csv ../../csv/598frame.csv
lframe's length after laoding =  5283
lframe's length after laoding =  5283
lframe's length after laoding =  5283
lframe's length after laoding =  5283
lframe len after excluding all exceptions= 5200
lframe len after excluding all exceptions= 5200
lframe len after excluding all exceptions= 5200
lframe len after excluding all exceptions= 5200
len train =  4160
len train =  4160
len train =  4160
len train =  4160
train consists of 277 full batches with 15 tensors with 3 views
the last batch has size of 5 tensors with 3 views
val consists of 69 full batches with 15 tensors with 3 views
the last batch has size of 5 tensors with 3 views
Epoch 0/299
----------
Mon Jan 31 00:14:52 2022
batch 0, train loss = 0.52, mean loss = 0.52
Mon Jan 31 00:15:02 2022
batch 10, train loss = 75.09, mean loss = 67.01
Mon Jan 31 00:15:57 2022
batch 20, train loss = 12.79, mean loss = 40.93
Mon Jan 31 00:16:52 2022
batch 30, train loss = 16.94, mean loss = 32.95
Mon Jan 31 00:17:46 2022
batch 40, train loss = 26.50, mean loss = 31.28
Mon Jan 31 00:18:40 2022
batch 50, train loss = 14.21, mean loss = 27.92
Mon Jan 31 00:19:33 2022
batch 60, train loss = 2.39, mean loss = 23.74
Mon Jan 31 00:20:27 2022
train Loss: 21.49

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 00:21:18 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 00:22:12 2022
val Loss: 0.03

epoch 0 was done for 477.334878 seconds
Epoch 1/299
----------
Mon Jan 31 00:22:50 2022
batch 0, train loss = 6.44, mean loss = 6.44
Mon Jan 31 00:22:55 2022
batch 10, train loss = 8.97, mean loss = 8.64
Mon Jan 31 00:23:48 2022
batch 20, train loss = 2.98, mean loss = 6.03
Mon Jan 31 00:24:42 2022
batch 30, train loss = 1.83, mean loss = 4.73
Mon Jan 31 00:25:35 2022
batch 40, train loss = 4.68, mean loss = 4.76
Mon Jan 31 00:26:28 2022
batch 50, train loss = 3.67, mean loss = 4.60
Mon Jan 31 00:27:21 2022
batch 60, train loss = 1.35, mean loss = 4.08
Mon Jan 31 00:28:14 2022
train Loss: 3.76

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:29:04 2022
batch 10, val loss = 0.03, mean loss = 0.02
Mon Jan 31 00:29:57 2022
val Loss: 0.02

epoch 1 was done for 465.012058 seconds
Epoch 2/299
----------
Mon Jan 31 00:30:35 2022
batch 0, train loss = 1.46, mean loss = 1.46
Mon Jan 31 00:30:40 2022
batch 10, train loss = 2.65, mean loss = 2.72
Mon Jan 31 00:31:34 2022
batch 20, train loss = 2.25, mean loss = 2.73
Mon Jan 31 00:32:29 2022
batch 30, train loss = 1.72, mean loss = 2.38
Mon Jan 31 00:33:23 2022
batch 40, train loss = 0.99, mean loss = 2.08
Mon Jan 31 00:34:17 2022
batch 50, train loss = 2.09, mean loss = 2.09
Mon Jan 31 00:35:12 2022
batch 60, train loss = 2.01, mean loss = 2.11
Mon Jan 31 00:36:06 2022
train Loss: 2.04

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:36:57 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:37:52 2022
val Loss: 0.02

epoch 2 was done for 475.007885 seconds
Epoch 3/299
----------
Mon Jan 31 00:38:30 2022
batch 0, train loss = 1.42, mean loss = 1.42
Mon Jan 31 00:38:35 2022
batch 10, train loss = 1.06, mean loss = 1.08
Mon Jan 31 00:39:29 2022
batch 20, train loss = 1.25, mean loss = 1.24
Mon Jan 31 00:40:23 2022
batch 30, train loss = 1.86, mean loss = 1.41
Mon Jan 31 00:41:17 2022
batch 40, train loss = 1.40, mean loss = 1.40
Mon Jan 31 00:42:11 2022
batch 50, train loss = 0.94, mean loss = 1.31
Mon Jan 31 00:43:05 2022
batch 60, train loss = 1.03, mean loss = 1.27
Mon Jan 31 00:43:59 2022
train Loss: 1.29

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:44:49 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:45:44 2022
val Loss: 0.02

epoch 3 was done for 471.993147 seconds
Epoch 4/299
----------
Mon Jan 31 00:46:22 2022
batch 0, train loss = 1.52, mean loss = 1.52
Mon Jan 31 00:46:27 2022
batch 10, train loss = 1.16, mean loss = 1.14
Mon Jan 31 00:47:21 2022
batch 20, train loss = 0.77, mean loss = 0.96
Mon Jan 31 00:48:15 2022
batch 30, train loss = 0.81, mean loss = 0.92
Mon Jan 31 00:49:09 2022
batch 40, train loss = 1.12, mean loss = 0.97
Mon Jan 31 00:50:03 2022
batch 50, train loss = 0.93, mean loss = 0.96
Mon Jan 31 00:50:57 2022
batch 60, train loss = 0.61, mean loss = 0.90
Mon Jan 31 00:51:51 2022
train Loss: 0.86

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:52:42 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:53:37 2022
val Loss: 0.02

epoch 4 was done for 472.761973 seconds
Epoch 5/299
----------
Mon Jan 31 00:54:14 2022
batch 0, train loss = 0.57, mean loss = 0.57
Mon Jan 31 00:54:20 2022
batch 10, train loss = 0.83, mean loss = 0.83
Mon Jan 31 00:55:14 2022
batch 20, train loss = 0.83, mean loss = 0.84
Mon Jan 31 00:56:09 2022
batch 30, train loss = 0.59, mean loss = 0.75
Mon Jan 31 00:57:03 2022
batch 40, train loss = 0.40, mean loss = 0.68
Mon Jan 31 00:57:58 2022
batch 50, train loss = 0.57, mean loss = 0.67
Mon Jan 31 00:58:52 2022
batch 60, train loss = 0.69, mean loss = 0.67
Mon Jan 31 00:59:47 2022
train Loss: 0.65

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:00:38 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:01:33 2022
val Loss: 0.02

epoch 5 was done for 476.471945 seconds
Epoch 6/299
----------
Mon Jan 31 01:02:11 2022
batch 0, train loss = 0.38, mean loss = 0.38
Mon Jan 31 01:02:16 2022
batch 10, train loss = 0.35, mean loss = 0.40
Mon Jan 31 01:03:10 2022
batch 20, train loss = 0.46, mean loss = 0.44
Mon Jan 31 01:04:04 2022
batch 30, train loss = 0.52, mean loss = 0.48
Mon Jan 31 01:04:59 2022
batch 40, train loss = 0.39, mean loss = 0.48
Mon Jan 31 01:05:53 2022
batch 50, train loss = 0.36, mean loss = 0.47
Mon Jan 31 01:06:47 2022
batch 60, train loss = 0.34, mean loss = 0.46
Mon Jan 31 01:07:41 2022
train Loss: 0.46

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:08:31 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:09:26 2022
val Loss: 0.02

epoch 6 was done for 472.877961 seconds
Epoch 7/299
----------
Mon Jan 31 01:10:04 2022
batch 0, train loss = 0.33, mean loss = 0.33
Mon Jan 31 01:10:09 2022
batch 10, train loss = 0.38, mean loss = 0.44
Mon Jan 31 01:11:03 2022
batch 20, train loss = 0.38, mean loss = 0.43
Mon Jan 31 01:11:57 2022
batch 30, train loss = 0.28, mean loss = 0.41
Mon Jan 31 01:12:51 2022
batch 40, train loss = 0.29, mean loss = 0.41
Mon Jan 31 01:13:45 2022
batch 50, train loss = 0.39, mean loss = 0.41
Mon Jan 31 01:14:39 2022
batch 60, train loss = 0.33, mean loss = 0.41
Mon Jan 31 01:15:33 2022
train Loss: 0.41

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:16:23 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:17:18 2022
val Loss: 0.02

epoch 7 was done for 471.179254 seconds
Epoch 8/299
----------
Mon Jan 31 01:17:55 2022
batch 0, train loss = 0.27, mean loss = 0.27
Mon Jan 31 01:18:00 2022
batch 10, train loss = 0.29, mean loss = 0.36
Mon Jan 31 01:18:54 2022
batch 20, train loss = 0.37, mean loss = 0.39
Mon Jan 31 01:19:47 2022
batch 30, train loss = 0.32, mean loss = 0.39
Mon Jan 31 01:20:41 2022
batch 40, train loss = 0.26, mean loss = 0.38
Mon Jan 31 01:21:34 2022
batch 50, train loss = 0.30, mean loss = 0.38
Mon Jan 31 01:22:28 2022
batch 60, train loss = 0.34, mean loss = 0.38
Mon Jan 31 01:23:21 2022
train Loss: 0.39

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:24:11 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:25:05 2022
val Loss: 0.02

epoch 8 was done for 468.185926 seconds
Epoch 9/299
----------
Mon Jan 31 01:25:43 2022
batch 0, train loss = 0.31, mean loss = 0.31
Mon Jan 31 01:25:48 2022
batch 10, train loss = 0.28, mean loss = 0.35
Mon Jan 31 01:26:42 2022
batch 20, train loss = 0.31, mean loss = 0.36
Mon Jan 31 01:27:36 2022
batch 30, train loss = 0.32, mean loss = 0.37
Mon Jan 31 01:28:30 2022
batch 40, train loss = 0.29, mean loss = 0.37
Mon Jan 31 01:29:24 2022
batch 50, train loss = 0.26, mean loss = 0.36
Mon Jan 31 01:30:18 2022
batch 60, train loss = 0.28, mean loss = 0.36
Mon Jan 31 01:31:12 2022
train Loss: 0.37

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:32:02 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:32:56 2022
val Loss: 0.02

epoch 9 was done for 470.798420 seconds
Epoch 10/299
----------
Mon Jan 31 01:33:34 2022
batch 0, train loss = 0.30, mean loss = 0.30
Mon Jan 31 01:33:39 2022
batch 10, train loss = 0.30, mean loss = 0.36
Mon Jan 31 01:34:33 2022
batch 20, train loss = 0.30, mean loss = 0.36
Mon Jan 31 01:35:27 2022
batch 30, train loss = 0.27, mean loss = 0.36
Mon Jan 31 01:36:22 2022
batch 40, train loss = 0.29, mean loss = 0.36
Mon Jan 31 01:37:16 2022
batch 50, train loss = 0.27, mean loss = 0.36
Mon Jan 31 01:38:10 2022
batch 60, train loss = 0.26, mean loss = 0.36
Mon Jan 31 01:39:04 2022
train Loss: 0.36

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:39:54 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:40:49 2022
val Loss: 0.02

epoch 10 was done for 473.163211 seconds
Epoch 11/299
----------
Mon Jan 31 01:41:27 2022
batch 0, train loss = 0.26, mean loss = 0.26
Mon Jan 31 01:41:32 2022
batch 10, train loss = 0.30, mean loss = 0.36
Mon Jan 31 01:42:26 2022
batch 20, train loss = 0.30, mean loss = 0.36
Mon Jan 31 01:43:19 2022
batch 30, train loss = 0.25, mean loss = 0.36
Mon Jan 31 01:44:13 2022
batch 40, train loss = 0.26, mean loss = 0.35
Mon Jan 31 01:45:06 2022
batch 50, train loss = 0.27, mean loss = 0.35
Mon Jan 31 01:45:59 2022
batch 60, train loss = 0.25, mean loss = 0.35
Mon Jan 31 01:46:53 2022
train Loss: 0.35

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:47:43 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:48:37 2022
val Loss: 0.02

epoch 11 was done for 467.136264 seconds
Epoch 12/299
----------
Mon Jan 31 01:49:14 2022
batch 0, train loss = 0.24, mean loss = 0.24
Mon Jan 31 01:49:20 2022
batch 10, train loss = 0.27, mean loss = 0.35
Mon Jan 31 01:50:14 2022
batch 20, train loss = 0.30, mean loss = 0.35
Mon Jan 31 01:51:08 2022
batch 30, train loss = 0.24, mean loss = 0.35
Mon Jan 31 01:52:02 2022
batch 40, train loss = 0.24, mean loss = 0.35
Mon Jan 31 01:52:56 2022
batch 50, train loss = 0.26, mean loss = 0.35
Mon Jan 31 01:53:50 2022
batch 60, train loss = 0.25, mean loss = 0.34
Mon Jan 31 01:54:44 2022
train Loss: 0.35

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:55:35 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:56:29 2022
val Loss: 0.02

epoch 12 was done for 472.771211 seconds
Epoch 13/299
----------
Mon Jan 31 01:57:07 2022
batch 0, train loss = 0.23, mean loss = 0.23
Mon Jan 31 01:57:12 2022
batch 10, train loss = 0.25, mean loss = 0.34
Mon Jan 31 01:58:07 2022
batch 20, train loss = 0.29, mean loss = 0.35
Mon Jan 31 01:59:02 2022
batch 30, train loss = 0.24, mean loss = 0.34
Mon Jan 31 01:59:56 2022
batch 40, train loss = 0.23, mean loss = 0.34
Mon Jan 31 02:00:51 2022
batch 50, train loss = 0.25, mean loss = 0.34
Mon Jan 31 02:01:45 2022
batch 60, train loss = 0.24, mean loss = 0.34
Mon Jan 31 02:02:40 2022
train Loss: 0.34

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:03:31 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:04:26 2022
val Loss: 0.02

epoch 13 was done for 477.260159 seconds
Epoch 14/299
----------
Mon Jan 31 02:05:04 2022
batch 0, train loss = 0.22, mean loss = 0.22
Mon Jan 31 02:05:10 2022
batch 10, train loss = 0.24, mean loss = 0.33
Mon Jan 31 02:06:04 2022
batch 20, train loss = 0.28, mean loss = 0.34
Mon Jan 31 02:06:58 2022
batch 30, train loss = 0.23, mean loss = 0.34
Mon Jan 31 02:07:52 2022
batch 40, train loss = 0.22, mean loss = 0.34
Mon Jan 31 02:08:46 2022
batch 50, train loss = 0.24, mean loss = 0.34
Mon Jan 31 02:09:40 2022
batch 60, train loss = 0.23, mean loss = 0.34
Mon Jan 31 02:10:35 2022
train Loss: 0.34

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:11:25 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:12:20 2022
val Loss: 0.02

epoch 14 was done for 473.251033 seconds
Epoch 15/299
----------
Mon Jan 31 02:12:57 2022
batch 0, train loss = 0.21, mean loss = 0.21
Mon Jan 31 02:13:03 2022
batch 10, train loss = 0.24, mean loss = 0.33
Mon Jan 31 02:13:57 2022
batch 20, train loss = 0.27, mean loss = 0.34
Mon Jan 31 02:14:51 2022
batch 30, train loss = 0.23, mean loss = 0.34
Mon Jan 31 02:15:45 2022
batch 40, train loss = 0.22, mean loss = 0.34
Mon Jan 31 02:16:39 2022
batch 50, train loss = 0.24, mean loss = 0.34
Mon Jan 31 02:17:34 2022
batch 60, train loss = 0.23, mean loss = 0.34
Mon Jan 31 02:18:31 2022
train Loss: 0.34

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:19:22 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:20:17 2022
val Loss: 0.02

epoch 15 was done for 477.033347 seconds
Epoch 16/299
----------
Mon Jan 31 02:20:54 2022
batch 0, train loss = 0.21, mean loss = 0.21
Mon Jan 31 02:21:00 2022
batch 10, train loss = 0.23, mean loss = 0.33
Mon Jan 31 02:21:55 2022
batch 20, train loss = 0.26, mean loss = 0.34
Mon Jan 31 02:22:51 2022
batch 30, train loss = 0.22, mean loss = 0.34
Mon Jan 31 02:23:46 2022
batch 40, train loss = 0.21, mean loss = 0.34
Mon Jan 31 02:24:41 2022
batch 50, train loss = 0.23, mean loss = 0.34
Mon Jan 31 02:25:36 2022
batch 60, train loss = 0.22, mean loss = 0.34
Mon Jan 31 02:26:32 2022
train Loss: 0.34

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:27:23 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:28:19 2022
val Loss: 0.02

epoch 16 was done for 482.993032 seconds
Epoch 17/299
----------
Mon Jan 31 02:28:57 2022
batch 0, train loss = 0.20, mean loss = 0.20
Mon Jan 31 02:29:03 2022
batch 10, train loss = 0.23, mean loss = 0.33
Mon Jan 31 02:29:57 2022
batch 20, train loss = 0.26, mean loss = 0.34
Mon Jan 31 02:30:51 2022
batch 30, train loss = 0.22, mean loss = 0.34
Mon Jan 31 02:31:45 2022
batch 40, train loss = 0.21, mean loss = 0.34
Mon Jan 31 02:32:39 2022
batch 50, train loss = 0.23, mean loss = 0.34
Mon Jan 31 02:33:33 2022
batch 60, train loss = 0.21, mean loss = 0.34
Mon Jan 31 02:34:26 2022
train Loss: 0.34

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:35:16 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:36:11 2022
val Loss: 0.02

epoch 17 was done for 470.637869 seconds
Epoch 18/299
----------
Mon Jan 31 02:36:48 2022
batch 0, train loss = 0.20, mean loss = 0.20
Mon Jan 31 02:36:54 2022
batch 10, train loss = 0.22, mean loss = 0.33
Mon Jan 31 02:37:48 2022
batch 20, train loss = 0.25, mean loss = 0.34
Mon Jan 31 02:38:42 2022
batch 30, train loss = 0.21, mean loss = 0.34
Mon Jan 31 02:39:35 2022
batch 40, train loss = 0.21, mean loss = 0.34
Mon Jan 31 02:40:29 2022
batch 50, train loss = 0.22, mean loss = 0.34
Mon Jan 31 02:41:23 2022
batch 60, train loss = 0.20, mean loss = 0.34
Mon Jan 31 02:42:17 2022
train Loss: 0.34

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:43:07 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:44:02 2022
val Loss: 0.02

epoch 18 was done for 471.222954 seconds
Epoch 19/299
----------
Mon Jan 31 02:44:39 2022
batch 0, train loss = 0.20, mean loss = 0.20
Mon Jan 31 02:44:45 2022
batch 10, train loss = 0.22, mean loss = 0.33
Mon Jan 31 02:45:39 2022
batch 20, train loss = 0.25, mean loss = 0.34
Mon Jan 31 02:46:34 2022
batch 30, train loss = 0.21, mean loss = 0.34
Mon Jan 31 02:47:28 2022
batch 40, train loss = 0.20, mean loss = 0.34
Mon Jan 31 02:48:23 2022
batch 50, train loss = 0.22, mean loss = 0.34
Mon Jan 31 02:49:17 2022
batch 60, train loss = 0.20, mean loss = 0.34
Mon Jan 31 02:50:12 2022
train Loss: 0.34

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:51:03 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:51:59 2022
val Loss: 0.02

epoch 19 was done for 477.148401 seconds
Epoch 20/299
----------
Mon Jan 31 02:52:36 2022
batch 0, train loss = 0.19, mean loss = 0.19
Mon Jan 31 02:52:42 2022
batch 10, train loss = 0.21, mean loss = 0.33
Mon Jan 31 02:53:36 2022
batch 20, train loss = 0.24, mean loss = 0.34
Mon Jan 31 02:54:30 2022
batch 30, train loss = 0.20, mean loss = 0.34
Mon Jan 31 02:55:24 2022
batch 40, train loss = 0.20, mean loss = 0.34
Mon Jan 31 02:56:18 2022
batch 50, train loss = 0.21, mean loss = 0.34
Mon Jan 31 02:57:13 2022
batch 60, train loss = 0.19, mean loss = 0.34
Mon Jan 31 02:58:07 2022
train Loss: 0.34

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:58:57 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:59:52 2022
val Loss: 0.02

epoch 20 was done for 472.720905 seconds
Epoch 21/299
----------
Mon Jan 31 03:00:29 2022
batch 0, train loss = 0.19, mean loss = 0.19
Mon Jan 31 03:00:35 2022
batch 10, train loss = 0.21, mean loss = 0.33
Mon Jan 31 03:01:29 2022
batch 20, train loss = 0.23, mean loss = 0.34
Mon Jan 31 03:02:23 2022
batch 30, train loss = 0.20, mean loss = 0.34
Mon Jan 31 03:03:17 2022
batch 40, train loss = 0.19, mean loss = 0.34
Mon Jan 31 03:04:12 2022
batch 50, train loss = 0.21, mean loss = 0.34
Mon Jan 31 03:05:06 2022
batch 60, train loss = 0.19, mean loss = 0.34
Mon Jan 31 03:06:00 2022
train Loss: 0.34

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:06:51 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:07:46 2022
val Loss: 0.02

epoch 21 was done for 474.272693 seconds
Epoch 22/299
----------
Mon Jan 31 03:08:23 2022
batch 0, train loss = 0.19, mean loss = 0.19
Mon Jan 31 03:08:29 2022
batch 10, train loss = 0.21, mean loss = 0.33
Mon Jan 31 03:09:23 2022
batch 20, train loss = 0.23, mean loss = 0.34
Mon Jan 31 03:10:17 2022
batch 30, train loss = 0.19, mean loss = 0.34
Mon Jan 31 03:11:11 2022
batch 40, train loss = 0.19, mean loss = 0.34
Mon Jan 31 03:12:04 2022
batch 50, train loss = 0.21, mean loss = 0.34
Mon Jan 31 03:12:58 2022
batch 60, train loss = 0.18, mean loss = 0.34
Mon Jan 31 03:13:52 2022
train Loss: 0.34

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:14:42 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:15:38 2022
val Loss: 0.02

epoch 22 was done for 472.030558 seconds
Epoch 23/299
----------
Mon Jan 31 03:16:15 2022
batch 0, train loss = 0.18, mean loss = 0.18
Mon Jan 31 03:16:21 2022
batch 10, train loss = 0.20, mean loss = 0.33
Mon Jan 31 03:17:15 2022
batch 20, train loss = 0.22, mean loss = 0.34
Mon Jan 31 03:18:10 2022
batch 30, train loss = 0.19, mean loss = 0.34
Mon Jan 31 03:19:04 2022
batch 40, train loss = 0.19, mean loss = 0.34
Mon Jan 31 03:19:59 2022
batch 50, train loss = 0.20, mean loss = 0.34
Mon Jan 31 03:20:53 2022
batch 60, train loss = 0.18, mean loss = 0.34
Mon Jan 31 03:21:47 2022
train Loss: 0.34

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:22:38 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:23:33 2022
val Loss: 0.02

epoch 23 was done for 475.523641 seconds
Epoch 24/299
----------
Mon Jan 31 03:24:11 2022
batch 0, train loss = 0.18, mean loss = 0.18
Mon Jan 31 03:24:17 2022
batch 10, train loss = 0.20, mean loss = 0.33
Mon Jan 31 03:25:11 2022
batch 20, train loss = 0.22, mean loss = 0.34
Mon Jan 31 03:26:06 2022
batch 30, train loss = 0.18, mean loss = 0.34
Mon Jan 31 03:27:01 2022
batch 40, train loss = 0.18, mean loss = 0.34
Mon Jan 31 03:27:56 2022
batch 50, train loss = 0.20, mean loss = 0.34
Mon Jan 31 03:28:51 2022
batch 60, train loss = 0.17, mean loss = 0.34
Mon Jan 31 03:29:46 2022
train Loss: 0.34

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:30:37 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:31:32 2022
val Loss: 0.02

epoch 24 was done for 479.150313 seconds
Epoch 25/299
----------
Mon Jan 31 03:32:10 2022
batch 0, train loss = 0.17, mean loss = 0.17
Mon Jan 31 03:32:16 2022
batch 10, train loss = 0.19, mean loss = 0.33
Mon Jan 31 03:33:10 2022
batch 20, train loss = 0.21, mean loss = 0.34
Mon Jan 31 03:34:05 2022
batch 30, train loss = 0.18, mean loss = 0.34
Mon Jan 31 03:34:59 2022
batch 40, train loss = 0.18, mean loss = 0.34
Mon Jan 31 03:35:53 2022
batch 50, train loss = 0.19, mean loss = 0.34
Mon Jan 31 03:36:48 2022
batch 60, train loss = 0.16, mean loss = 0.34
Mon Jan 31 03:37:42 2022
train Loss: 0.35

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:38:33 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:39:28 2022
val Loss: 0.02

epoch 25 was done for 474.686007 seconds
Epoch 26/299
----------
Mon Jan 31 03:40:05 2022
batch 0, train loss = 0.17, mean loss = 0.17
Mon Jan 31 03:40:10 2022
batch 10, train loss = 0.19, mean loss = 0.33
Mon Jan 31 03:41:04 2022
batch 20, train loss = 0.21, mean loss = 0.34
Mon Jan 31 03:41:59 2022
batch 30, train loss = 0.17, mean loss = 0.34
Mon Jan 31 03:42:53 2022
batch 40, train loss = 0.18, mean loss = 0.34
Mon Jan 31 03:43:47 2022
batch 50, train loss = 0.19, mean loss = 0.34
Mon Jan 31 03:44:41 2022
batch 60, train loss = 0.16, mean loss = 0.34
Mon Jan 31 03:45:35 2022
train Loss: 0.35

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:46:25 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:47:20 2022
val Loss: 0.02

epoch 26 was done for 472.881142 seconds
Epoch 27/299
----------
Mon Jan 31 03:47:58 2022
batch 0, train loss = 0.17, mean loss = 0.17
Mon Jan 31 03:48:03 2022
batch 10, train loss = 0.18, mean loss = 0.33
Mon Jan 31 03:48:58 2022
batch 20, train loss = 0.20, mean loss = 0.34
Mon Jan 31 03:49:52 2022
batch 30, train loss = 0.17, mean loss = 0.34
Mon Jan 31 03:50:47 2022
batch 40, train loss = 0.17, mean loss = 0.34
Mon Jan 31 03:51:41 2022
batch 50, train loss = 0.18, mean loss = 0.34
Mon Jan 31 03:52:36 2022
batch 60, train loss = 0.15, mean loss = 0.34
Mon Jan 31 03:53:30 2022
train Loss: 0.35

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:54:21 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:55:16 2022
val Loss: 0.02

epoch 27 was done for 475.896465 seconds
Epoch 28/299
----------
Mon Jan 31 03:55:54 2022
batch 0, train loss = 0.16, mean loss = 0.16
Mon Jan 31 03:55:59 2022
batch 10, train loss = 0.18, mean loss = 0.33
Mon Jan 31 03:56:53 2022
batch 20, train loss = 0.20, mean loss = 0.34
Mon Jan 31 03:57:47 2022
batch 30, train loss = 0.17, mean loss = 0.34
Mon Jan 31 03:58:40 2022
batch 40, train loss = 0.17, mean loss = 0.35
Mon Jan 31 03:59:34 2022
batch 50, train loss = 0.18, mean loss = 0.35
Mon Jan 31 04:00:28 2022
batch 60, train loss = 0.15, mean loss = 0.34
Mon Jan 31 04:01:21 2022
train Loss: 0.35

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:02:11 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:03:06 2022
val Loss: 0.02

epoch 28 was done for 469.273183 seconds
Epoch 29/299
----------
Mon Jan 31 04:03:43 2022
batch 0, train loss = 0.16, mean loss = 0.16
Mon Jan 31 04:03:48 2022
batch 10, train loss = 0.18, mean loss = 0.33
Mon Jan 31 04:04:42 2022
batch 20, train loss = 0.19, mean loss = 0.34
Mon Jan 31 04:05:36 2022
batch 30, train loss = 0.16, mean loss = 0.34
Mon Jan 31 04:06:30 2022
batch 40, train loss = 0.16, mean loss = 0.35
Mon Jan 31 04:07:24 2022
batch 50, train loss = 0.17, mean loss = 0.35
Mon Jan 31 04:08:18 2022
batch 60, train loss = 0.14, mean loss = 0.34
Mon Jan 31 04:09:12 2022
train Loss: 0.35

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:10:02 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:10:57 2022
val Loss: 0.02

epoch 29 was done for 471.568690 seconds
Epoch 30/299
----------
Mon Jan 31 04:11:34 2022
batch 0, train loss = 0.16, mean loss = 0.16
Mon Jan 31 04:11:40 2022
batch 10, train loss = 0.17, mean loss = 0.33
Mon Jan 31 04:12:35 2022
batch 20, train loss = 0.19, mean loss = 0.34
Mon Jan 31 04:13:29 2022
batch 30, train loss = 0.16, mean loss = 0.34
Mon Jan 31 04:14:24 2022
batch 40, train loss = 0.16, mean loss = 0.35
Mon Jan 31 04:15:18 2022
batch 50, train loss = 0.17, mean loss = 0.35
Mon Jan 31 04:16:12 2022
batch 60, train loss = 0.14, mean loss = 0.35
Mon Jan 31 04:17:07 2022
train Loss: 0.35

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:17:58 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:18:53 2022
val Loss: 0.02

epoch 30 was done for 475.967328 seconds
Epoch 31/299
----------
Mon Jan 31 04:19:30 2022
batch 0, train loss = 0.15, mean loss = 0.15
Mon Jan 31 04:19:36 2022
batch 10, train loss = 0.17, mean loss = 0.33
Mon Jan 31 04:20:30 2022
batch 20, train loss = 0.18, mean loss = 0.34
Mon Jan 31 04:21:24 2022
batch 30, train loss = 0.15, mean loss = 0.35
Mon Jan 31 04:22:19 2022
batch 40, train loss = 0.16, mean loss = 0.35
Mon Jan 31 04:23:13 2022
batch 50, train loss = 0.17, mean loss = 0.35
Mon Jan 31 04:24:07 2022
batch 60, train loss = 0.14, mean loss = 0.35
Mon Jan 31 04:25:01 2022
train Loss: 0.35

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:25:52 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:26:47 2022
val Loss: 0.02

epoch 31 was done for 473.680958 seconds
Epoch 32/299
----------
Mon Jan 31 04:27:24 2022
batch 0, train loss = 0.15, mean loss = 0.15
Mon Jan 31 04:27:30 2022
batch 10, train loss = 0.16, mean loss = 0.33
Mon Jan 31 04:28:24 2022
batch 20, train loss = 0.18, mean loss = 0.34
Mon Jan 31 04:29:18 2022
batch 30, train loss = 0.15, mean loss = 0.35
Mon Jan 31 04:30:12 2022
batch 40, train loss = 0.15, mean loss = 0.35
Mon Jan 31 04:31:06 2022
batch 50, train loss = 0.16, mean loss = 0.35
Mon Jan 31 04:32:01 2022
batch 60, train loss = 0.13, mean loss = 0.35
Mon Jan 31 04:32:55 2022
train Loss: 0.35

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:33:46 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:34:41 2022
val Loss: 0.02

epoch 32 was done for 473.902795 seconds
Epoch 33/299
----------
Mon Jan 31 04:35:18 2022
batch 0, train loss = 0.14, mean loss = 0.14
Mon Jan 31 04:35:24 2022
batch 10, train loss = 0.16, mean loss = 0.33
Mon Jan 31 04:36:18 2022
batch 20, train loss = 0.17, mean loss = 0.35
Mon Jan 31 04:37:12 2022
batch 30, train loss = 0.14, mean loss = 0.35
Mon Jan 31 04:38:07 2022
batch 40, train loss = 0.15, mean loss = 0.35
Mon Jan 31 04:39:01 2022
batch 50, train loss = 0.16, mean loss = 0.35
Mon Jan 31 04:39:56 2022
batch 60, train loss = 0.13, mean loss = 0.35
Mon Jan 31 04:40:50 2022
train Loss: 0.35

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:41:41 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:42:36 2022
val Loss: 0.02

epoch 33 was done for 475.684318 seconds
Epoch 34/299
----------
Mon Jan 31 04:43:14 2022
batch 0, train loss = 0.14, mean loss = 0.14
Mon Jan 31 04:43:19 2022
batch 10, train loss = 0.15, mean loss = 0.33
Mon Jan 31 04:44:13 2022
batch 20, train loss = 0.17, mean loss = 0.35
Mon Jan 31 04:45:07 2022
batch 30, train loss = 0.14, mean loss = 0.35
Mon Jan 31 04:46:02 2022
batch 40, train loss = 0.14, mean loss = 0.35
Mon Jan 31 04:46:56 2022
batch 50, train loss = 0.15, mean loss = 0.35
Mon Jan 31 04:47:50 2022
batch 60, train loss = 0.12, mean loss = 0.35
Mon Jan 31 04:48:44 2022
train Loss: 0.35

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:49:34 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:50:29 2022
val Loss: 0.02

epoch 34 was done for 473.312495 seconds
Epoch 35/299
----------
Mon Jan 31 04:51:07 2022
batch 0, train loss = 0.14, mean loss = 0.14
Mon Jan 31 04:51:12 2022
batch 10, train loss = 0.15, mean loss = 0.33
Mon Jan 31 04:52:07 2022
batch 20, train loss = 0.16, mean loss = 0.35
Mon Jan 31 04:53:01 2022
batch 30, train loss = 0.14, mean loss = 0.35
Mon Jan 31 04:53:55 2022
batch 40, train loss = 0.14, mean loss = 0.35
Mon Jan 31 04:54:49 2022
batch 50, train loss = 0.15, mean loss = 0.35
Mon Jan 31 04:55:43 2022
batch 60, train loss = 0.12, mean loss = 0.35
Mon Jan 31 04:56:37 2022
train Loss: 0.36

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:57:28 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:58:22 2022
val Loss: 0.02

epoch 35 was done for 472.819787 seconds
Epoch 36/299
----------
Mon Jan 31 04:59:00 2022
batch 0, train loss = 0.13, mean loss = 0.13
Mon Jan 31 04:59:05 2022
batch 10, train loss = 0.14, mean loss = 0.34
Mon Jan 31 04:59:59 2022
batch 20, train loss = 0.16, mean loss = 0.35
Mon Jan 31 05:00:52 2022
batch 30, train loss = 0.13, mean loss = 0.35
Mon Jan 31 05:01:46 2022
batch 40, train loss = 0.13, mean loss = 0.35
Mon Jan 31 05:02:39 2022
batch 50, train loss = 0.15, mean loss = 0.35
Mon Jan 31 05:03:33 2022
batch 60, train loss = 0.11, mean loss = 0.35
Mon Jan 31 05:04:27 2022
train Loss: 0.36

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:05:17 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:06:12 2022
val Loss: 0.02

epoch 36 was done for 470.172375 seconds
Epoch 37/299
----------
Mon Jan 31 05:06:50 2022
batch 0, train loss = 0.13, mean loss = 0.13
Mon Jan 31 05:06:55 2022
batch 10, train loss = 0.14, mean loss = 0.34
Mon Jan 31 05:07:49 2022
batch 20, train loss = 0.15, mean loss = 0.35
Mon Jan 31 05:08:43 2022
batch 30, train loss = 0.13, mean loss = 0.35
Mon Jan 31 05:09:38 2022
batch 40, train loss = 0.13, mean loss = 0.35
Mon Jan 31 05:10:31 2022
batch 50, train loss = 0.14, mean loss = 0.36
Mon Jan 31 05:11:25 2022
batch 60, train loss = 0.11, mean loss = 0.35
Mon Jan 31 05:12:19 2022
train Loss: 0.36

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:13:10 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:14:05 2022
val Loss: 0.02

epoch 37 was done for 472.064500 seconds
Epoch 38/299
----------
Mon Jan 31 05:14:42 2022
batch 0, train loss = 0.12, mean loss = 0.12
Mon Jan 31 05:14:48 2022
batch 10, train loss = 0.14, mean loss = 0.34
Mon Jan 31 05:15:42 2022
batch 20, train loss = 0.15, mean loss = 0.35
Mon Jan 31 05:16:36 2022
batch 30, train loss = 0.12, mean loss = 0.35
Mon Jan 31 05:17:30 2022
batch 40, train loss = 0.13, mean loss = 0.36
Mon Jan 31 05:18:24 2022
batch 50, train loss = 0.14, mean loss = 0.36
Mon Jan 31 05:19:19 2022
batch 60, train loss = 0.10, mean loss = 0.35
Mon Jan 31 05:20:13 2022
train Loss: 0.36

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:21:03 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:21:58 2022
val Loss: 0.02

epoch 38 was done for 473.884913 seconds
Epoch 39/299
----------
Mon Jan 31 05:22:36 2022
batch 0, train loss = 0.12, mean loss = 0.12
Mon Jan 31 05:22:41 2022
batch 10, train loss = 0.13, mean loss = 0.34
Mon Jan 31 05:23:35 2022
batch 20, train loss = 0.14, mean loss = 0.35
Mon Jan 31 05:24:29 2022
batch 30, train loss = 0.12, mean loss = 0.35
Mon Jan 31 05:25:22 2022
batch 40, train loss = 0.12, mean loss = 0.36
Mon Jan 31 05:26:16 2022
batch 50, train loss = 0.14, mean loss = 0.36
Mon Jan 31 05:27:09 2022
batch 60, train loss = 0.10, mean loss = 0.36
Mon Jan 31 05:28:03 2022
train Loss: 0.36

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:28:53 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:29:48 2022
val Loss: 0.02

epoch 39 was done for 469.344733 seconds
Epoch 40/299
----------
Mon Jan 31 05:30:25 2022
batch 0, train loss = 0.12, mean loss = 0.12
Mon Jan 31 05:30:31 2022
batch 10, train loss = 0.13, mean loss = 0.34
Mon Jan 31 05:31:25 2022
batch 20, train loss = 0.14, mean loss = 0.35
Mon Jan 31 05:32:19 2022
batch 30, train loss = 0.11, mean loss = 0.36
Mon Jan 31 05:33:13 2022
batch 40, train loss = 0.12, mean loss = 0.36
Mon Jan 31 05:34:07 2022
batch 50, train loss = 0.13, mean loss = 0.36
Mon Jan 31 05:35:01 2022
batch 60, train loss = 0.10, mean loss = 0.36
Mon Jan 31 05:35:56 2022
train Loss: 0.36

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:36:46 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:37:41 2022
val Loss: 0.02

epoch 40 was done for 473.242817 seconds
Epoch 41/299
----------
Mon Jan 31 05:38:19 2022
batch 0, train loss = 0.11, mean loss = 0.11
Mon Jan 31 05:38:24 2022
batch 10, train loss = 0.12, mean loss = 0.34
Mon Jan 31 05:39:18 2022
batch 20, train loss = 0.13, mean loss = 0.36
Mon Jan 31 05:40:13 2022
batch 30, train loss = 0.11, mean loss = 0.36
Mon Jan 31 05:41:08 2022
batch 40, train loss = 0.12, mean loss = 0.36
Mon Jan 31 05:42:02 2022
batch 50, train loss = 0.13, mean loss = 0.36
Mon Jan 31 05:42:57 2022
batch 60, train loss = 0.09, mean loss = 0.36
Mon Jan 31 05:43:51 2022
train Loss: 0.37

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:44:42 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:45:38 2022
val Loss: 0.02

epoch 41 was done for 476.953530 seconds
Epoch 42/299
----------
Mon Jan 31 05:46:15 2022
batch 0, train loss = 0.11, mean loss = 0.11
Mon Jan 31 05:46:21 2022
batch 10, train loss = 0.12, mean loss = 0.34
Mon Jan 31 05:47:15 2022
batch 20, train loss = 0.13, mean loss = 0.36
Mon Jan 31 05:48:09 2022
batch 30, train loss = 0.11, mean loss = 0.36
Mon Jan 31 05:49:03 2022
batch 40, train loss = 0.11, mean loss = 0.36
Mon Jan 31 05:49:57 2022
batch 50, train loss = 0.13, mean loss = 0.36
Mon Jan 31 05:50:50 2022
batch 60, train loss = 0.09, mean loss = 0.36
Mon Jan 31 05:51:44 2022
train Loss: 0.37

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:52:35 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:53:29 2022
val Loss: 0.02

epoch 42 was done for 470.955724 seconds
Epoch 43/299
----------
Mon Jan 31 05:54:06 2022
batch 0, train loss = 0.10, mean loss = 0.10
Mon Jan 31 05:54:12 2022
batch 10, train loss = 0.12, mean loss = 0.35
Mon Jan 31 05:55:06 2022
batch 20, train loss = 0.12, mean loss = 0.36
Mon Jan 31 05:56:01 2022
batch 30, train loss = 0.10, mean loss = 0.36
Mon Jan 31 05:56:55 2022
batch 40, train loss = 0.11, mean loss = 0.36
Mon Jan 31 05:57:49 2022
batch 50, train loss = 0.12, mean loss = 0.37
Mon Jan 31 05:58:43 2022
batch 60, train loss = 0.08, mean loss = 0.36
Mon Jan 31 05:59:38 2022
train Loss: 0.37

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:00:29 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:01:24 2022
val Loss: 0.02

epoch 43 was done for 475.388634 seconds
Epoch 44/299
----------
Mon Jan 31 06:02:02 2022
batch 0, train loss = 0.10, mean loss = 0.10
Mon Jan 31 06:02:07 2022
batch 10, train loss = 0.11, mean loss = 0.35
Mon Jan 31 06:03:02 2022
batch 20, train loss = 0.12, mean loss = 0.36
Mon Jan 31 06:03:58 2022
batch 30, train loss = 0.10, mean loss = 0.36
Mon Jan 31 06:04:53 2022
batch 40, train loss = 0.10, mean loss = 0.37
Mon Jan 31 06:05:48 2022
batch 50, train loss = 0.12, mean loss = 0.37
Mon Jan 31 06:06:43 2022
batch 60, train loss = 0.08, mean loss = 0.37
Mon Jan 31 06:07:38 2022
train Loss: 0.37

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:08:29 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:09:25 2022
val Loss: 0.02

epoch 44 was done for 480.770806 seconds
Epoch 45/299
----------
Mon Jan 31 06:10:03 2022
batch 0, train loss = 0.10, mean loss = 0.10
Mon Jan 31 06:10:08 2022
batch 10, train loss = 0.11, mean loss = 0.35
Mon Jan 31 06:11:02 2022
batch 20, train loss = 0.12, mean loss = 0.36
Mon Jan 31 06:11:56 2022
batch 30, train loss = 0.09, mean loss = 0.36
Mon Jan 31 06:12:50 2022
batch 40, train loss = 0.10, mean loss = 0.37
Mon Jan 31 06:13:44 2022
batch 50, train loss = 0.12, mean loss = 0.37
Mon Jan 31 06:14:38 2022
batch 60, train loss = 0.08, mean loss = 0.37
Mon Jan 31 06:15:33 2022
train Loss: 0.37

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:16:23 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:17:19 2022
val Loss: 0.02

epoch 45 was done for 473.548194 seconds
Epoch 46/299
----------
Mon Jan 31 06:17:56 2022
batch 0, train loss = 0.09, mean loss = 0.09
Mon Jan 31 06:18:02 2022
batch 10, train loss = 0.10, mean loss = 0.35
Mon Jan 31 06:18:56 2022
batch 20, train loss = 0.11, mean loss = 0.37
Mon Jan 31 06:19:51 2022
batch 30, train loss = 0.09, mean loss = 0.37
Mon Jan 31 06:20:46 2022
batch 40, train loss = 0.10, mean loss = 0.37
Mon Jan 31 06:21:40 2022
batch 50, train loss = 0.12, mean loss = 0.37
Mon Jan 31 06:22:35 2022
batch 60, train loss = 0.07, mean loss = 0.37
Mon Jan 31 06:23:30 2022
train Loss: 0.38

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:24:21 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:25:16 2022
val Loss: 0.02

epoch 46 was done for 477.631157 seconds
Epoch 47/299
----------
Mon Jan 31 06:25:54 2022
batch 0, train loss = 0.09, mean loss = 0.09
Mon Jan 31 06:25:59 2022
batch 10, train loss = 0.10, mean loss = 0.36
Mon Jan 31 06:26:55 2022
batch 20, train loss = 0.11, mean loss = 0.37
Mon Jan 31 06:27:50 2022
batch 30, train loss = 0.09, mean loss = 0.37
Mon Jan 31 06:28:45 2022
batch 40, train loss = 0.09, mean loss = 0.37
Mon Jan 31 06:29:40 2022
batch 50, train loss = 0.11, mean loss = 0.38
Mon Jan 31 06:30:35 2022
batch 60, train loss = 0.07, mean loss = 0.37
Mon Jan 31 06:31:31 2022
train Loss: 0.38

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:32:22 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:33:18 2022
val Loss: 0.02

epoch 47 was done for 481.909040 seconds
Epoch 48/299
----------
Mon Jan 31 06:33:56 2022
batch 0, train loss = 0.09, mean loss = 0.09
Mon Jan 31 06:34:01 2022
batch 10, train loss = 0.10, mean loss = 0.36
Mon Jan 31 06:34:56 2022
batch 20, train loss = 0.10, mean loss = 0.37
Mon Jan 31 06:35:50 2022
batch 30, train loss = 0.08, mean loss = 0.37
Mon Jan 31 06:36:44 2022
batch 40, train loss = 0.09, mean loss = 0.38
Mon Jan 31 06:37:38 2022
batch 50, train loss = 0.11, mean loss = 0.38
Mon Jan 31 06:38:32 2022
batch 60, train loss = 0.07, mean loss = 0.38
Mon Jan 31 06:39:26 2022
train Loss: 0.38

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:40:17 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:41:11 2022
val Loss: 0.02

epoch 48 was done for 473.042689 seconds
Epoch 49/299
----------
Mon Jan 31 06:41:49 2022
batch 0, train loss = 0.08, mean loss = 0.08
Mon Jan 31 06:41:54 2022
batch 10, train loss = 0.10, mean loss = 0.36
Mon Jan 31 06:42:48 2022
batch 20, train loss = 0.10, mean loss = 0.38
Mon Jan 31 06:43:43 2022
batch 30, train loss = 0.08, mean loss = 0.38
Mon Jan 31 06:44:37 2022
batch 40, train loss = 0.09, mean loss = 0.38
Mon Jan 31 06:45:32 2022
batch 50, train loss = 0.11, mean loss = 0.38
Mon Jan 31 06:46:26 2022
batch 60, train loss = 0.07, mean loss = 0.38
Mon Jan 31 06:47:20 2022
train Loss: 0.39

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:48:10 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:49:05 2022
val Loss: 0.02

epoch 49 was done for 474.216122 seconds
Epoch 50/299
----------
Mon Jan 31 06:49:43 2022
batch 0, train loss = 0.08, mean loss = 0.08
Mon Jan 31 06:49:48 2022
batch 10, train loss = 0.09, mean loss = 0.37
Mon Jan 31 06:50:42 2022
batch 20, train loss = 0.09, mean loss = 0.38
Mon Jan 31 06:51:36 2022
batch 30, train loss = 0.07, mean loss = 0.38
Mon Jan 31 06:52:29 2022
batch 40, train loss = 0.08, mean loss = 0.38
Mon Jan 31 06:53:23 2022
batch 50, train loss = 0.11, mean loss = 0.39
Mon Jan 31 06:54:17 2022
batch 60, train loss = 0.06, mean loss = 0.38
Mon Jan 31 06:55:10 2022
train Loss: 0.39

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:56:01 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:56:57 2022
val Loss: 0.02

epoch 50 was done for 471.740578 seconds
Epoch 51/299
----------
Mon Jan 31 06:57:35 2022
batch 0, train loss = 0.08, mean loss = 0.08
Mon Jan 31 06:57:40 2022
batch 10, train loss = 0.09, mean loss = 0.37
Mon Jan 31 06:58:34 2022
batch 20, train loss = 0.09, mean loss = 0.39
Mon Jan 31 06:59:28 2022
batch 30, train loss = 0.07, mean loss = 0.38
Mon Jan 31 07:00:22 2022
batch 40, train loss = 0.08, mean loss = 0.39
Mon Jan 31 07:01:16 2022
batch 50, train loss = 0.11, mean loss = 0.39
Mon Jan 31 07:02:10 2022
batch 60, train loss = 0.06, mean loss = 0.39
Mon Jan 31 07:03:04 2022
train Loss: 0.39

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:03:54 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:04:49 2022
val Loss: 0.02

epoch 51 was done for 471.680739 seconds
Epoch 52/299
----------
Mon Jan 31 07:05:26 2022
batch 0, train loss = 0.07, mean loss = 0.07
Mon Jan 31 07:05:32 2022
batch 10, train loss = 0.09, mean loss = 0.38
Mon Jan 31 07:06:26 2022
batch 20, train loss = 0.09, mean loss = 0.39
Mon Jan 31 07:07:21 2022
batch 30, train loss = 0.07, mean loss = 0.39
Mon Jan 31 07:08:15 2022
batch 40, train loss = 0.08, mean loss = 0.39
Mon Jan 31 07:09:10 2022
batch 50, train loss = 0.11, mean loss = 0.40
Mon Jan 31 07:10:04 2022
batch 60, train loss = 0.06, mean loss = 0.39
Mon Jan 31 07:10:59 2022
train Loss: 0.40

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:11:50 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:12:45 2022
val Loss: 0.02

epoch 52 was done for 476.256575 seconds
Epoch 53/299
----------
Mon Jan 31 07:13:23 2022
batch 0, train loss = 0.07, mean loss = 0.07
Mon Jan 31 07:13:28 2022
batch 10, train loss = 0.09, mean loss = 0.38
Mon Jan 31 07:14:22 2022
batch 20, train loss = 0.08, mean loss = 0.40
Mon Jan 31 07:15:16 2022
batch 30, train loss = 0.07, mean loss = 0.39
Mon Jan 31 07:16:09 2022
batch 40, train loss = 0.08, mean loss = 0.39
Mon Jan 31 07:17:03 2022
batch 50, train loss = 0.11, mean loss = 0.40
Mon Jan 31 07:17:56 2022
batch 60, train loss = 0.06, mean loss = 0.40
Mon Jan 31 07:18:50 2022
train Loss: 0.40

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:19:40 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:20:35 2022
val Loss: 0.02

epoch 53 was done for 469.896509 seconds
Epoch 54/299
----------
Mon Jan 31 07:21:13 2022
batch 0, train loss = 0.07, mean loss = 0.07
Mon Jan 31 07:21:18 2022
batch 10, train loss = 0.08, mean loss = 0.39
Mon Jan 31 07:22:12 2022
batch 20, train loss = 0.08, mean loss = 0.40
Mon Jan 31 07:23:06 2022
batch 30, train loss = 0.06, mean loss = 0.40
Mon Jan 31 07:24:00 2022
batch 40, train loss = 0.07, mean loss = 0.40
Mon Jan 31 07:24:54 2022
batch 50, train loss = 0.11, mean loss = 0.41
Mon Jan 31 07:25:48 2022
batch 60, train loss = 0.06, mean loss = 0.40
Mon Jan 31 07:26:42 2022
train Loss: 0.41

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:27:33 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:28:28 2022
val Loss: 0.02

epoch 54 was done for 472.869836 seconds
Epoch 55/299
----------
Mon Jan 31 07:29:05 2022
batch 0, train loss = 0.07, mean loss = 0.07
Mon Jan 31 07:29:11 2022
batch 10, train loss = 0.08, mean loss = 0.39
Mon Jan 31 07:30:05 2022
batch 20, train loss = 0.08, mean loss = 0.41
Mon Jan 31 07:31:00 2022
batch 30, train loss = 0.06, mean loss = 0.40
Mon Jan 31 07:31:56 2022
batch 40, train loss = 0.07, mean loss = 0.41
Mon Jan 31 07:32:50 2022
batch 50, train loss = 0.11, mean loss = 0.41
Mon Jan 31 07:33:45 2022
batch 60, train loss = 0.06, mean loss = 0.41
Mon Jan 31 07:34:39 2022
train Loss: 0.42

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:35:30 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:36:25 2022
val Loss: 0.02

epoch 55 was done for 477.611774 seconds
Epoch 56/299
----------
Mon Jan 31 07:37:03 2022
batch 0, train loss = 0.06, mean loss = 0.06
Mon Jan 31 07:37:08 2022
batch 10, train loss = 0.08, mean loss = 0.40
Mon Jan 31 07:38:03 2022
batch 20, train loss = 0.08, mean loss = 0.42
Mon Jan 31 07:38:57 2022
batch 30, train loss = 0.06, mean loss = 0.41
Mon Jan 31 07:39:51 2022
batch 40, train loss = 0.07, mean loss = 0.41
Mon Jan 31 07:40:50 2022
batch 50, train loss = 0.11, mean loss = 0.42
Mon Jan 31 07:41:44 2022
batch 60, train loss = 0.06, mean loss = 0.42
Mon Jan 31 07:42:38 2022
train Loss: 0.42

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:43:29 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:44:24 2022
val Loss: 0.02

epoch 56 was done for 478.937230 seconds
Epoch 57/299
----------
Mon Jan 31 07:45:02 2022
batch 0, train loss = 0.06, mean loss = 0.06
Mon Jan 31 07:45:07 2022
batch 10, train loss = 0.08, mean loss = 0.41
Mon Jan 31 07:46:07 2022
batch 20, train loss = 0.08, mean loss = 0.43
Mon Jan 31 07:47:01 2022
batch 30, train loss = 0.06, mean loss = 0.42
Mon Jan 31 07:47:57 2022
batch 40, train loss = 0.07, mean loss = 0.42
Mon Jan 31 07:48:51 2022
batch 50, train loss = 0.11, mean loss = 0.43
Mon Jan 31 07:49:45 2022
batch 60, train loss = 0.06, mean loss = 0.43
Mon Jan 31 07:50:39 2022
train Loss: 0.43

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:51:30 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:52:25 2022
val Loss: 0.02

epoch 57 was done for 479.982648 seconds
Epoch 58/299
----------
Mon Jan 31 07:53:02 2022
batch 0, train loss = 0.06, mean loss = 0.06
Mon Jan 31 07:53:07 2022
batch 10, train loss = 0.08, mean loss = 0.42
Mon Jan 31 07:54:02 2022
batch 20, train loss = 0.08, mean loss = 0.44
Mon Jan 31 07:54:56 2022
batch 30, train loss = 0.06, mean loss = 0.43
Mon Jan 31 07:55:51 2022
batch 40, train loss = 0.07, mean loss = 0.43
Mon Jan 31 07:56:46 2022
batch 50, train loss = 0.12, mean loss = 0.44
Mon Jan 31 07:57:40 2022
batch 60, train loss = 0.06, mean loss = 0.44
Mon Jan 31 07:58:35 2022
train Loss: 0.44

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:59:26 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:00:22 2022
val Loss: 0.02

epoch 58 was done for 477.612803 seconds
Epoch 59/299
----------
Mon Jan 31 08:01:00 2022
batch 0, train loss = 0.06, mean loss = 0.06
Mon Jan 31 08:01:05 2022
batch 10, train loss = 0.08, mean loss = 0.43
Mon Jan 31 08:01:59 2022
batch 20, train loss = 0.08, mean loss = 0.46
Mon Jan 31 08:02:55 2022
batch 30, train loss = 0.06, mean loss = 0.44
Mon Jan 31 08:03:49 2022
batch 40, train loss = 0.07, mean loss = 0.44
Mon Jan 31 08:04:44 2022
batch 50, train loss = 0.12, mean loss = 0.46
Mon Jan 31 08:05:39 2022
batch 60, train loss = 0.07, mean loss = 0.45
Mon Jan 31 08:06:33 2022
train Loss: 0.45

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:07:24 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:08:19 2022
val Loss: 0.02

epoch 59 was done for 477.723220 seconds
Epoch 60/299
----------
Mon Jan 31 08:08:57 2022
batch 0, train loss = 0.06, mean loss = 0.06
Mon Jan 31 08:09:03 2022
batch 10, train loss = 0.08, mean loss = 0.44
Mon Jan 31 08:09:57 2022
batch 20, train loss = 0.09, mean loss = 0.47
Mon Jan 31 08:10:52 2022
batch 30, train loss = 0.06, mean loss = 0.45
Mon Jan 31 08:11:46 2022
batch 40, train loss = 0.06, mean loss = 0.45
Mon Jan 31 08:12:41 2022
batch 50, train loss = 0.13, mean loss = 0.47
Mon Jan 31 08:13:35 2022
batch 60, train loss = 0.08, mean loss = 0.47
Mon Jan 31 08:14:30 2022
train Loss: 0.47

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:15:21 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:16:16 2022
val Loss: 0.02

epoch 60 was done for 476.484196 seconds
Epoch 61/299
----------
Mon Jan 31 08:16:54 2022
batch 0, train loss = 0.06, mean loss = 0.06
Mon Jan 31 08:16:59 2022
batch 10, train loss = 0.09, mean loss = 0.46
Mon Jan 31 08:17:54 2022
batch 20, train loss = 0.09, mean loss = 0.49
Mon Jan 31 08:18:49 2022
batch 30, train loss = 0.07, mean loss = 0.47
Mon Jan 31 08:19:44 2022
batch 40, train loss = 0.06, mean loss = 0.47
Mon Jan 31 08:20:39 2022
batch 50, train loss = 0.13, mean loss = 0.48
Mon Jan 31 08:21:34 2022
batch 60, train loss = 0.08, mean loss = 0.48
Mon Jan 31 08:22:29 2022
train Loss: 0.48

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:23:20 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:24:15 2022
val Loss: 0.02

epoch 61 was done for 479.316201 seconds
Epoch 62/299
----------
Mon Jan 31 08:24:53 2022
batch 0, train loss = 0.06, mean loss = 0.06
Mon Jan 31 08:24:59 2022
batch 10, train loss = 0.09, mean loss = 0.47
Mon Jan 31 08:25:53 2022
batch 20, train loss = 0.10, mean loss = 0.52
Mon Jan 31 08:26:47 2022
batch 30, train loss = 0.08, mean loss = 0.50
Mon Jan 31 08:27:46 2022
batch 40, train loss = 0.07, mean loss = 0.49
Mon Jan 31 08:28:43 2022
batch 50, train loss = 0.15, mean loss = 0.51
Mon Jan 31 08:29:38 2022
batch 60, train loss = 0.09, mean loss = 0.51
Mon Jan 31 08:30:34 2022
train Loss: 0.51

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:31:25 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:32:26 2022
val Loss: 0.02

epoch 62 was done for 490.665372 seconds
Epoch 63/299
----------
Mon Jan 31 08:33:04 2022
batch 0, train loss = 0.06, mean loss = 0.06
Mon Jan 31 08:33:09 2022
batch 10, train loss = 0.09, mean loss = 0.49
Mon Jan 31 08:34:09 2022
batch 20, train loss = 0.11, mean loss = 0.54
Mon Jan 31 08:35:11 2022
batch 30, train loss = 0.09, mean loss = 0.51
Mon Jan 31 08:36:16 2022
batch 40, train loss = 0.06, mean loss = 0.50
Mon Jan 31 08:37:19 2022
batch 50, train loss = 0.16, mean loss = 0.53
Mon Jan 31 08:38:22 2022
batch 60, train loss = 0.12, mean loss = 0.53
Mon Jan 31 08:39:26 2022
train Loss: 0.53

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:40:25 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:41:28 2022
val Loss: 0.02

epoch 63 was done for 546.860101 seconds
Epoch 64/299
----------
Mon Jan 31 08:42:11 2022
batch 0, train loss = 0.07, mean loss = 0.07
Mon Jan 31 08:42:17 2022
batch 10, train loss = 0.10, mean loss = 0.52
Mon Jan 31 08:43:21 2022
batch 20, train loss = 0.13, mean loss = 0.58
Mon Jan 31 08:44:26 2022
batch 30, train loss = 0.11, mean loss = 0.56
Mon Jan 31 08:45:29 2022
batch 40, train loss = 0.07, mean loss = 0.54
Mon Jan 31 08:46:33 2022
batch 50, train loss = 0.18, mean loss = 0.57
Mon Jan 31 08:47:37 2022
batch 60, train loss = 0.14, mean loss = 0.57
Mon Jan 31 08:48:42 2022
train Loss: 0.57

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 08:49:42 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:50:51 2022
val Loss: 0.02

epoch 64 was done for 566.425555 seconds
Epoch 65/299
----------
Mon Jan 31 08:51:37 2022
batch 0, train loss = 0.08, mean loss = 0.08
Mon Jan 31 08:51:44 2022
batch 10, train loss = 0.10, mean loss = 0.54
Mon Jan 31 08:52:49 2022
batch 20, train loss = 0.15, mean loss = 0.62
Mon Jan 31 08:53:55 2022
batch 30, train loss = 0.13, mean loss = 0.59
Mon Jan 31 08:55:02 2022
batch 40, train loss = 0.07, mean loss = 0.57
Mon Jan 31 08:56:08 2022
batch 50, train loss = 0.20, mean loss = 0.60
Mon Jan 31 08:57:15 2022
batch 60, train loss = 0.17, mean loss = 0.61
Mon Jan 31 08:58:22 2022
train Loss: 0.60

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 08:59:25 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:00:34 2022
val Loss: 0.02

epoch 65 was done for 583.266420 seconds
Epoch 66/299
----------
Mon Jan 31 09:01:20 2022
batch 0, train loss = 0.11, mean loss = 0.11
Mon Jan 31 09:01:27 2022
batch 10, train loss = 0.12, mean loss = 0.57
Mon Jan 31 09:02:35 2022
batch 20, train loss = 0.18, mean loss = 0.66
Mon Jan 31 09:03:43 2022
batch 30, train loss = 0.17, mean loss = 0.65
Mon Jan 31 09:04:51 2022
batch 40, train loss = 0.08, mean loss = 0.61
Mon Jan 31 09:05:59 2022
batch 50, train loss = 0.23, mean loss = 0.64
Mon Jan 31 09:07:07 2022
batch 60, train loss = 0.23, mean loss = 0.66
Mon Jan 31 09:08:21 2022
train Loss: 0.66

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 09:09:25 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:10:34 2022
val Loss: 0.02

epoch 66 was done for 599.936298 seconds
Epoch 67/299
----------
Mon Jan 31 09:11:20 2022
batch 0, train loss = 0.14, mean loss = 0.14
Mon Jan 31 09:11:27 2022
batch 10, train loss = 0.12, mean loss = 0.60
Mon Jan 31 09:12:35 2022
batch 20, train loss = 0.22, mean loss = 0.73
Mon Jan 31 09:13:43 2022
batch 30, train loss = 0.25, mean loss = 0.73
Mon Jan 31 09:14:52 2022
batch 40, train loss = 0.10, mean loss = 0.68
Mon Jan 31 09:16:00 2022
batch 50, train loss = 0.29, mean loss = 0.72
Mon Jan 31 09:17:08 2022
batch 60, train loss = 0.28, mean loss = 0.75
Mon Jan 31 09:18:16 2022
train Loss: 0.73

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 09:19:20 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:20:29 2022
val Loss: 0.02

epoch 67 was done for 595.731345 seconds
Epoch 68/299
----------
Mon Jan 31 09:21:16 2022
batch 0, train loss = 0.17, mean loss = 0.17
Mon Jan 31 09:21:23 2022
batch 10, train loss = 0.11, mean loss = 0.61
Mon Jan 31 09:22:31 2022
batch 20, train loss = 0.25, mean loss = 0.76
Mon Jan 31 09:23:38 2022
batch 30, train loss = 0.23, mean loss = 0.75
Mon Jan 31 09:24:47 2022
batch 40, train loss = 0.12, mean loss = 0.70
Mon Jan 31 09:25:56 2022
batch 50, train loss = 0.34, mean loss = 0.75
Mon Jan 31 09:27:04 2022
batch 60, train loss = 0.30, mean loss = 0.78
Mon Jan 31 09:28:13 2022
train Loss: 0.78

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 09:29:17 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 09:30:25 2022
val Loss: 0.03

epoch 68 was done for 595.889667 seconds
Epoch 69/299
----------
Mon Jan 31 09:31:12 2022
batch 0, train loss = 0.24, mean loss = 0.24
Mon Jan 31 09:31:19 2022
batch 10, train loss = 0.15, mean loss = 0.66
Mon Jan 31 09:32:26 2022
batch 20, train loss = 0.28, mean loss = 0.85
Mon Jan 31 09:33:35 2022
batch 30, train loss = 0.36, mean loss = 0.87
Mon Jan 31 09:34:43 2022
batch 40, train loss = 0.20, mean loss = 0.82
Mon Jan 31 09:35:51 2022
batch 50, train loss = 0.32, mean loss = 0.85
Mon Jan 31 09:37:00 2022
batch 60, train loss = 0.41, mean loss = 0.90
Mon Jan 31 09:38:09 2022
train Loss: 0.91

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:39:12 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:40:21 2022
val Loss: 0.02

epoch 69 was done for 595.569720 seconds
Epoch 70/299
----------
Mon Jan 31 09:41:07 2022
batch 0, train loss = 0.41, mean loss = 0.41
Mon Jan 31 09:41:14 2022
batch 10, train loss = 0.16, mean loss = 0.67
Mon Jan 31 09:42:23 2022
batch 20, train loss = 0.40, mean loss = 0.94
Mon Jan 31 09:43:31 2022
batch 30, train loss = 0.51, mean loss = 1.01
Mon Jan 31 09:44:39 2022
batch 40, train loss = 0.32, mean loss = 0.96
Mon Jan 31 09:45:48 2022
batch 50, train loss = 0.27, mean loss = 0.95
Mon Jan 31 09:46:58 2022
batch 60, train loss = 0.55, mean loss = 1.02
Mon Jan 31 09:48:07 2022
train Loss: 1.04

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:49:11 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:50:23 2022
val Loss: 0.02

epoch 70 was done for 602.140043 seconds
Epoch 71/299
----------
Mon Jan 31 09:51:10 2022
batch 0, train loss = 0.58, mean loss = 0.58
Mon Jan 31 09:51:16 2022
batch 10, train loss = 0.29, mean loss = 0.76
Mon Jan 31 09:52:25 2022
batch 20, train loss = 0.51, mean loss = 1.07
Mon Jan 31 09:53:33 2022
batch 30, train loss = 0.83, mean loss = 1.21
Mon Jan 31 09:54:41 2022
batch 40, train loss = 0.56, mean loss = 1.17
Mon Jan 31 09:55:49 2022
batch 50, train loss = 0.39, mean loss = 1.14
Mon Jan 31 09:56:57 2022
batch 60, train loss = 0.87, mean loss = 1.25
Mon Jan 31 09:58:05 2022
train Loss: 1.28

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 09:59:10 2022
batch 10, val loss = 0.03, mean loss = 0.02
Mon Jan 31 10:00:20 2022
val Loss: 0.02

epoch 71 was done for 598.085476 seconds
Epoch 72/299
----------
Mon Jan 31 10:01:08 2022
batch 0, train loss = 0.83, mean loss = 0.83
Mon Jan 31 10:01:14 2022
batch 10, train loss = 0.59, mean loss = 1.06
Mon Jan 31 10:02:23 2022
batch 20, train loss = 0.43, mean loss = 1.14
Mon Jan 31 10:03:31 2022
batch 30, train loss = 1.26, mean loss = 1.44
Mon Jan 31 10:04:40 2022
batch 40, train loss = 0.69, mean loss = 1.38
Mon Jan 31 10:05:47 2022
batch 50, train loss = 0.61, mean loss = 1.35
Mon Jan 31 10:06:56 2022
batch 60, train loss = 0.85, mean loss = 1.42
Mon Jan 31 10:08:04 2022
train Loss: 1.51

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:09:08 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:10:16 2022
val Loss: 0.02

epoch 72 was done for 595.283116 seconds
Epoch 73/299
----------
Mon Jan 31 10:11:03 2022
batch 0, train loss = 1.48, mean loss = 1.48
Mon Jan 31 10:11:10 2022
batch 10, train loss = 0.55, mean loss = 1.08
Mon Jan 31 10:12:18 2022
batch 20, train loss = 0.52, mean loss = 1.16
Mon Jan 31 10:13:27 2022
batch 30, train loss = 0.94, mean loss = 1.36
Mon Jan 31 10:14:36 2022
batch 40, train loss = 1.03, mean loss = 1.41
Mon Jan 31 10:15:44 2022
batch 50, train loss = 0.42, mean loss = 1.32
Mon Jan 31 10:16:53 2022
batch 60, train loss = 0.80, mean loss = 1.38
Mon Jan 31 10:18:02 2022
train Loss: 1.45

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:19:06 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:20:15 2022
val Loss: 0.02

epoch 73 was done for 598.995067 seconds
Epoch 74/299
----------
Mon Jan 31 10:21:02 2022
batch 0, train loss = 1.24, mean loss = 1.24
Mon Jan 31 10:21:09 2022
batch 10, train loss = 0.61, mean loss = 1.16
Mon Jan 31 10:22:19 2022
batch 20, train loss = 0.30, mean loss = 1.05
Mon Jan 31 10:23:28 2022
batch 30, train loss = 0.74, mean loss = 1.23
Mon Jan 31 10:24:37 2022
batch 40, train loss = 0.98, mean loss = 1.32
Mon Jan 31 10:25:44 2022
batch 50, train loss = 0.39, mean loss = 1.23
Mon Jan 31 10:26:53 2022
batch 60, train loss = 0.36, mean loss = 1.21
Mon Jan 31 10:28:01 2022
train Loss: 1.34

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:29:05 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:30:14 2022
val Loss: 0.02

epoch 74 was done for 599.194443 seconds
Epoch 75/299
----------
Mon Jan 31 10:31:01 2022
batch 0, train loss = 1.40, mean loss = 1.40
Mon Jan 31 10:31:08 2022
batch 10, train loss = 1.15, mean loss = 1.65
Mon Jan 31 10:32:17 2022
batch 20, train loss = 0.33, mean loss = 1.25
Mon Jan 31 10:33:26 2022
batch 30, train loss = 0.74, mean loss = 1.35
Mon Jan 31 10:34:35 2022
batch 40, train loss = 1.60, mean loss = 1.58
Mon Jan 31 10:35:43 2022
batch 50, train loss = 1.23, mean loss = 1.62
Mon Jan 31 10:36:52 2022
batch 60, train loss = 0.42, mean loss = 1.51
Mon Jan 31 10:38:01 2022
train Loss: 1.59

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:39:05 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:40:15 2022
val Loss: 0.02

epoch 75 was done for 600.162013 seconds
Epoch 76/299
----------
Mon Jan 31 10:41:01 2022
batch 0, train loss = 1.36, mean loss = 1.36
Mon Jan 31 10:41:08 2022
batch 10, train loss = 2.29, mean loss = 2.85
Mon Jan 31 10:42:17 2022
batch 20, train loss = 1.13, mean loss = 2.23
Mon Jan 31 10:43:25 2022
batch 30, train loss = 0.42, mean loss = 1.82
Mon Jan 31 10:44:34 2022
batch 40, train loss = 1.87, mean loss = 2.05
Mon Jan 31 10:45:43 2022
batch 50, train loss = 2.37, mean loss = 2.25
Mon Jan 31 10:46:52 2022
batch 60, train loss = 0.81, mean loss = 2.05
Mon Jan 31 10:48:02 2022
train Loss: 1.98

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 10:49:07 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 10:50:17 2022
val Loss: 0.02

epoch 76 was done for 603.485761 seconds
Epoch 77/299
----------
Mon Jan 31 10:51:05 2022
batch 0, train loss = 0.84, mean loss = 0.84
Mon Jan 31 10:51:11 2022
