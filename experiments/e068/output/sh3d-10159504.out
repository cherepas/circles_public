The activation script must be sourced, otherwise the virtual environment will not work.
Setting vars
iscuda= True
PyTorch Version:  1.8.1
Torchvision Version:  0.9.0a0
opt:
 Namespace(ampl=441, aug_gt='orient', batch_output=2, bs=15, cencrop=700, center=False, chidden_dim=[96, 128, 256, 256, 256], classicnorm=False, cmscrop=0, conTrain='', criterion='L2', csvname='598csv9', datapath='/p/project/delia-mp/cherepashkin1/phenoseed/', dfname='598frame', downsample=1, epoch=300, expdescr='', expnum='e068', feature_extract=False, framelim=6000, gradient_predivide_factor=1.0, gttype='single_file', haf=True, hidden_dim=[9], inputt='img', kernel_sizes=[7, 3, 3, 3, 3, 3], lb='orient', loadh5=False, localexp='', lr=1e-06, machine='jureca', maintain=False, maintain_line=False, man_dist=False, measure_time=False, merging='batch', minmax=False, minmax3dimage=False, minmax_f=True, minmax_fn='', model_name='', netname=['cnet'], ngpu=4, noise_input=False, noise_output=False, normalize=False, num_input_images=3, num_sam_points=500, num_workers=0, outputt='orient', parallel='horovod', pin_memory=False, print_minibatch=10, pscale=100, rand_angle=False, rescale=500, rot_dirs=False, rotate_output=False, save_output=True, single_folder=False, specie='598', standardize=255, steplr=[1000.0, 1.0], transappendix='_image', ufmodel=100000, updateFraction=0.25, use_adasum=False, use_cuda=True, use_existing_csv=True, use_pretrained=False, use_sep_csv=True, view_sep=False, wandb='', weight_decay=0, zero_angle=True)
sys.argv:
 ['../../main.py', '-datapath', '/p/project/delia-mp/cherepashkin1/phenoseed/', '-epoch', '300', '-bs', '15', '-num_input_images', '3', '-framelim', '6000', '-criterion', 'L2', '-localexp', '', '-lr', '1e-6', '-expnum', 'e068', '-hidden_dim', '9', '-inputt', 'img', '-outputt', 'orient', '-lb', 'orient', '-no_loadh5', '-minmax_fn', '', '-parallel', 'horovod', '-machine', 'jureca', '-merging', 'batch', '-aug_gt', 'orient', '-updateFraction', '0.25', '-steplr', '1000', '1', '-print_minibatch', '10', '-dfname', '598frame']
seed =  0
path were main.py is located= ../../
opt.wandb =  
93
file to frame csv ../../csv/598frame.csv
iscuda= True
iscuda= True
iscuda= True
opt.wandb =  
93
file to frame csv ../../csv/598frame.csv
opt.wandb =  
opt.wandb =  
93
file to frame csv ../../csv/598frame.csv
93
file to frame csv ../../csv/598frame.csv
lframe's length after laoding =  5283
lframe's length after laoding =  5283
lframe's length after laoding =  5283
lframe's length after laoding =  5283
lframe len after excluding all exceptions= 5200
lframe len after excluding all exceptions= 5200
lframe len after excluding all exceptions= 5200
lframe len after excluding all exceptions= 5200
len train =  4160
len train =  4160
len train =  4160
len train =  4160
train consists of 277 full batches with 15 tensors with 3 views
the last batch has size of 5 tensors with 3 views
val consists of 69 full batches with 15 tensors with 3 views
the last batch has size of 5 tensors with 3 views
Epoch 0/299
----------
Sun Jan 30 23:49:57 2022
batch 0, train loss = 0.52, mean loss = 0.52
Sun Jan 30 23:50:10 2022
batch 10, train loss = 0.40, mean loss = 0.42
Sun Jan 30 23:51:21 2022
batch 20, train loss = 0.42, mean loss = 0.41
Sun Jan 30 23:52:31 2022
batch 30, train loss = 0.38, mean loss = 0.41
Sun Jan 30 23:53:41 2022
batch 40, train loss = 0.32, mean loss = 0.40
Sun Jan 30 23:54:50 2022
batch 50, train loss = 0.39, mean loss = 0.40
Sun Jan 30 23:55:59 2022
batch 60, train loss = 0.37, mean loss = 0.40
Sun Jan 30 23:57:09 2022
train Loss: 0.39

batch 0, val loss = 0.03, mean loss = 0.03
Sun Jan 30 23:58:13 2022
batch 10, val loss = 0.03, mean loss = 0.03
Sun Jan 30 23:59:23 2022
val Loss: 0.03

epoch 0 was done for 612.630543 seconds
Epoch 1/299
----------
Mon Jan 31 00:00:10 2022
batch 0, train loss = 0.35, mean loss = 0.35
Mon Jan 31 00:00:15 2022
batch 10, train loss = 0.35, mean loss = 0.35
Mon Jan 31 00:01:09 2022
batch 20, train loss = 0.36, mean loss = 0.36
Mon Jan 31 00:02:04 2022
batch 30, train loss = 0.35, mean loss = 0.36
Mon Jan 31 00:02:58 2022
batch 40, train loss = 0.30, mean loss = 0.35
Mon Jan 31 00:03:52 2022
batch 50, train loss = 0.37, mean loss = 0.36
Mon Jan 31 00:04:45 2022
batch 60, train loss = 0.34, mean loss = 0.35
Mon Jan 31 00:05:39 2022
train Loss: 0.35

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 00:06:29 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 00:07:23 2022
val Loss: 0.03

epoch 1 was done for 470.295671 seconds
Epoch 2/299
----------
Mon Jan 31 00:08:00 2022
batch 0, train loss = 0.33, mean loss = 0.33
Mon Jan 31 00:08:06 2022
batch 10, train loss = 0.35, mean loss = 0.34
Mon Jan 31 00:09:00 2022
batch 20, train loss = 0.34, mean loss = 0.34
Mon Jan 31 00:09:55 2022
batch 30, train loss = 0.34, mean loss = 0.34
Mon Jan 31 00:10:49 2022
batch 40, train loss = 0.28, mean loss = 0.34
Mon Jan 31 00:11:44 2022
batch 50, train loss = 0.36, mean loss = 0.34
Mon Jan 31 00:12:38 2022
batch 60, train loss = 0.32, mean loss = 0.34
Mon Jan 31 00:13:33 2022
train Loss: 0.34

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 00:14:24 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 00:15:19 2022
val Loss: 0.03

epoch 2 was done for 476.410562 seconds
Epoch 3/299
----------
Mon Jan 31 00:15:57 2022
batch 0, train loss = 0.31, mean loss = 0.31
Mon Jan 31 00:16:02 2022
batch 10, train loss = 0.35, mean loss = 0.33
Mon Jan 31 00:16:56 2022
batch 20, train loss = 0.32, mean loss = 0.33
Mon Jan 31 00:17:50 2022
batch 30, train loss = 0.34, mean loss = 0.34
Mon Jan 31 00:18:44 2022
batch 40, train loss = 0.28, mean loss = 0.33
Mon Jan 31 00:19:39 2022
batch 50, train loss = 0.36, mean loss = 0.34
Mon Jan 31 00:20:33 2022
batch 60, train loss = 0.31, mean loss = 0.33
Mon Jan 31 00:21:27 2022
train Loss: 0.34

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 00:22:17 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 00:23:12 2022
val Loss: 0.03

epoch 3 was done for 472.781594 seconds
Epoch 4/299
----------
Mon Jan 31 00:23:49 2022
batch 0, train loss = 0.30, mean loss = 0.30
Mon Jan 31 00:23:55 2022
batch 10, train loss = 0.34, mean loss = 0.33
Mon Jan 31 00:24:49 2022
batch 20, train loss = 0.31, mean loss = 0.33
Mon Jan 31 00:25:43 2022
batch 30, train loss = 0.34, mean loss = 0.33
Mon Jan 31 00:26:37 2022
batch 40, train loss = 0.27, mean loss = 0.33
Mon Jan 31 00:27:31 2022
batch 50, train loss = 0.35, mean loss = 0.33
Mon Jan 31 00:28:25 2022
batch 60, train loss = 0.30, mean loss = 0.33
Mon Jan 31 00:29:19 2022
train Loss: 0.33

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 00:30:10 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 00:31:04 2022
val Loss: 0.03

epoch 4 was done for 472.485717 seconds
Epoch 5/299
----------
Mon Jan 31 00:31:42 2022
batch 0, train loss = 0.29, mean loss = 0.29
Mon Jan 31 00:31:48 2022
batch 10, train loss = 0.34, mean loss = 0.33
Mon Jan 31 00:32:42 2022
batch 20, train loss = 0.30, mean loss = 0.33
Mon Jan 31 00:33:37 2022
batch 30, train loss = 0.34, mean loss = 0.33
Mon Jan 31 00:34:31 2022
batch 40, train loss = 0.26, mean loss = 0.33
Mon Jan 31 00:35:26 2022
batch 50, train loss = 0.35, mean loss = 0.33
Mon Jan 31 00:36:20 2022
batch 60, train loss = 0.29, mean loss = 0.33
Mon Jan 31 00:37:15 2022
train Loss: 0.33

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 00:38:06 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 00:39:01 2022
val Loss: 0.03

epoch 5 was done for 477.353778 seconds
Epoch 6/299
----------
Mon Jan 31 00:39:39 2022
batch 0, train loss = 0.28, mean loss = 0.28
Mon Jan 31 00:39:45 2022
batch 10, train loss = 0.34, mean loss = 0.32
Mon Jan 31 00:40:39 2022
batch 20, train loss = 0.29, mean loss = 0.32
Mon Jan 31 00:41:33 2022
batch 30, train loss = 0.34, mean loss = 0.32
Mon Jan 31 00:42:27 2022
batch 40, train loss = 0.26, mean loss = 0.32
Mon Jan 31 00:43:21 2022
batch 50, train loss = 0.35, mean loss = 0.32
Mon Jan 31 00:44:15 2022
batch 60, train loss = 0.28, mean loss = 0.32
Mon Jan 31 00:45:09 2022
train Loss: 0.32

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 00:46:00 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 00:46:55 2022
val Loss: 0.03

epoch 6 was done for 472.876354 seconds
Epoch 7/299
----------
Mon Jan 31 00:47:32 2022
batch 0, train loss = 0.27, mean loss = 0.27
Mon Jan 31 00:47:38 2022
batch 10, train loss = 0.33, mean loss = 0.32
Mon Jan 31 00:48:32 2022
batch 20, train loss = 0.29, mean loss = 0.32
Mon Jan 31 00:49:26 2022
batch 30, train loss = 0.34, mean loss = 0.32
Mon Jan 31 00:50:20 2022
batch 40, train loss = 0.25, mean loss = 0.32
Mon Jan 31 00:51:14 2022
batch 50, train loss = 0.34, mean loss = 0.32
Mon Jan 31 00:52:08 2022
batch 60, train loss = 0.27, mean loss = 0.32
Mon Jan 31 00:53:02 2022
train Loss: 0.32

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 00:53:53 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 00:54:48 2022
val Loss: 0.03

epoch 7 was done for 473.227150 seconds
Epoch 8/299
----------
Mon Jan 31 00:55:25 2022
batch 0, train loss = 0.27, mean loss = 0.27
Mon Jan 31 00:55:31 2022
batch 10, train loss = 0.33, mean loss = 0.32
Mon Jan 31 00:56:25 2022
batch 20, train loss = 0.28, mean loss = 0.32
Mon Jan 31 00:57:19 2022
batch 30, train loss = 0.34, mean loss = 0.32
Mon Jan 31 00:58:13 2022
batch 40, train loss = 0.24, mean loss = 0.32
Mon Jan 31 00:59:08 2022
batch 50, train loss = 0.34, mean loss = 0.32
Mon Jan 31 01:00:02 2022
batch 60, train loss = 0.26, mean loss = 0.32
Mon Jan 31 01:00:57 2022
train Loss: 0.32

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 01:01:48 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 01:02:43 2022
val Loss: 0.03

epoch 8 was done for 475.191497 seconds
Epoch 9/299
----------
Mon Jan 31 01:03:21 2022
batch 0, train loss = 0.26, mean loss = 0.26
Mon Jan 31 01:03:26 2022
batch 10, train loss = 0.33, mean loss = 0.32
Mon Jan 31 01:04:20 2022
batch 20, train loss = 0.27, mean loss = 0.32
Mon Jan 31 01:05:15 2022
batch 30, train loss = 0.33, mean loss = 0.32
Mon Jan 31 01:06:09 2022
batch 40, train loss = 0.23, mean loss = 0.32
Mon Jan 31 01:07:03 2022
batch 50, train loss = 0.34, mean loss = 0.32
Mon Jan 31 01:07:57 2022
batch 60, train loss = 0.25, mean loss = 0.32
Mon Jan 31 01:08:51 2022
train Loss: 0.32

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 01:09:42 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 01:10:37 2022
val Loss: 0.03

epoch 9 was done for 474.642158 seconds
Epoch 10/299
----------
Mon Jan 31 01:11:15 2022
batch 0, train loss = 0.25, mean loss = 0.25
Mon Jan 31 01:11:21 2022
batch 10, train loss = 0.33, mean loss = 0.32
Mon Jan 31 01:12:15 2022
batch 20, train loss = 0.27, mean loss = 0.31
Mon Jan 31 01:13:09 2022
batch 30, train loss = 0.33, mean loss = 0.32
Mon Jan 31 01:14:04 2022
batch 40, train loss = 0.23, mean loss = 0.31
Mon Jan 31 01:14:58 2022
batch 50, train loss = 0.34, mean loss = 0.32
Mon Jan 31 01:15:52 2022
batch 60, train loss = 0.25, mean loss = 0.31
Mon Jan 31 01:16:47 2022
train Loss: 0.32

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 01:17:38 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 01:18:33 2022
val Loss: 0.03

epoch 10 was done for 475.249910 seconds
Epoch 11/299
----------
Mon Jan 31 01:19:10 2022
batch 0, train loss = 0.25, mean loss = 0.25
Mon Jan 31 01:19:16 2022
batch 10, train loss = 0.33, mean loss = 0.31
Mon Jan 31 01:20:10 2022
batch 20, train loss = 0.26, mean loss = 0.31
Mon Jan 31 01:21:04 2022
batch 30, train loss = 0.33, mean loss = 0.32
Mon Jan 31 01:21:59 2022
batch 40, train loss = 0.22, mean loss = 0.31
Mon Jan 31 01:22:53 2022
batch 50, train loss = 0.34, mean loss = 0.32
Mon Jan 31 01:23:48 2022
batch 60, train loss = 0.24, mean loss = 0.31
Mon Jan 31 01:24:43 2022
train Loss: 0.32

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 01:25:34 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 01:26:30 2022
val Loss: 0.03

epoch 11 was done for 476.888004 seconds
Epoch 12/299
----------
Mon Jan 31 01:27:07 2022
batch 0, train loss = 0.24, mean loss = 0.24
Mon Jan 31 01:27:13 2022
batch 10, train loss = 0.32, mean loss = 0.31
Mon Jan 31 01:28:07 2022
batch 20, train loss = 0.26, mean loss = 0.31
Mon Jan 31 01:29:01 2022
batch 30, train loss = 0.33, mean loss = 0.31
Mon Jan 31 01:29:56 2022
batch 40, train loss = 0.22, mean loss = 0.31
Mon Jan 31 01:30:51 2022
batch 50, train loss = 0.34, mean loss = 0.31
Mon Jan 31 01:31:46 2022
batch 60, train loss = 0.23, mean loss = 0.31
Mon Jan 31 01:32:40 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 01:33:31 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 01:34:26 2022
val Loss: 0.03

epoch 12 was done for 476.041326 seconds
Epoch 13/299
----------
Mon Jan 31 01:35:03 2022
batch 0, train loss = 0.24, mean loss = 0.24
Mon Jan 31 01:35:09 2022
batch 10, train loss = 0.32, mean loss = 0.31
Mon Jan 31 01:36:03 2022
batch 20, train loss = 0.25, mean loss = 0.31
Mon Jan 31 01:36:58 2022
batch 30, train loss = 0.33, mean loss = 0.31
Mon Jan 31 01:37:52 2022
batch 40, train loss = 0.21, mean loss = 0.31
Mon Jan 31 01:38:46 2022
batch 50, train loss = 0.34, mean loss = 0.31
Mon Jan 31 01:39:41 2022
batch 60, train loss = 0.23, mean loss = 0.31
Mon Jan 31 01:40:35 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 01:41:26 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 01:42:21 2022
val Loss: 0.03

epoch 13 was done for 475.029542 seconds
Epoch 14/299
----------
Mon Jan 31 01:42:58 2022
batch 0, train loss = 0.23, mean loss = 0.23
Mon Jan 31 01:43:04 2022
batch 10, train loss = 0.32, mean loss = 0.31
Mon Jan 31 01:43:58 2022
batch 20, train loss = 0.24, mean loss = 0.31
Mon Jan 31 01:44:53 2022
batch 30, train loss = 0.34, mean loss = 0.31
Mon Jan 31 01:45:47 2022
batch 40, train loss = 0.21, mean loss = 0.31
Mon Jan 31 01:46:41 2022
batch 50, train loss = 0.34, mean loss = 0.31
Mon Jan 31 01:47:35 2022
batch 60, train loss = 0.22, mean loss = 0.31
Mon Jan 31 01:48:29 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 01:49:21 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 01:50:17 2022
val Loss: 0.03

epoch 14 was done for 476.449847 seconds
Epoch 15/299
----------
Mon Jan 31 01:50:55 2022
batch 0, train loss = 0.22, mean loss = 0.22
Mon Jan 31 01:51:00 2022
batch 10, train loss = 0.32, mean loss = 0.31
Mon Jan 31 01:51:56 2022
batch 20, train loss = 0.24, mean loss = 0.31
Mon Jan 31 01:52:50 2022
batch 30, train loss = 0.34, mean loss = 0.31
Mon Jan 31 01:53:45 2022
batch 40, train loss = 0.20, mean loss = 0.31
Mon Jan 31 01:54:40 2022
batch 50, train loss = 0.34, mean loss = 0.31
Mon Jan 31 01:55:34 2022
batch 60, train loss = 0.21, mean loss = 0.31
Mon Jan 31 01:56:29 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 01:57:20 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 01:58:15 2022
val Loss: 0.03

epoch 15 was done for 478.203278 seconds
Epoch 16/299
----------
Mon Jan 31 01:58:53 2022
batch 0, train loss = 0.22, mean loss = 0.22
Mon Jan 31 01:58:59 2022
batch 10, train loss = 0.32, mean loss = 0.31
Mon Jan 31 01:59:53 2022
batch 20, train loss = 0.23, mean loss = 0.31
Mon Jan 31 02:00:48 2022
batch 30, train loss = 0.34, mean loss = 0.31
Mon Jan 31 02:01:42 2022
batch 40, train loss = 0.19, mean loss = 0.31
Mon Jan 31 02:02:37 2022
batch 50, train loss = 0.34, mean loss = 0.31
Mon Jan 31 02:03:31 2022
batch 60, train loss = 0.21, mean loss = 0.31
Mon Jan 31 02:04:26 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 02:05:17 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 02:06:12 2022
val Loss: 0.03

epoch 16 was done for 476.972269 seconds
Epoch 17/299
----------
Mon Jan 31 02:06:50 2022
batch 0, train loss = 0.21, mean loss = 0.21
Mon Jan 31 02:06:56 2022
batch 10, train loss = 0.32, mean loss = 0.31
Mon Jan 31 02:07:50 2022
batch 20, train loss = 0.23, mean loss = 0.30
Mon Jan 31 02:08:45 2022
batch 30, train loss = 0.34, mean loss = 0.31
Mon Jan 31 02:09:39 2022
batch 40, train loss = 0.19, mean loss = 0.31
Mon Jan 31 02:10:34 2022
batch 50, train loss = 0.34, mean loss = 0.31
Mon Jan 31 02:11:29 2022
batch 60, train loss = 0.20, mean loss = 0.31
Mon Jan 31 02:12:24 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 02:13:16 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 02:14:12 2022
val Loss: 0.03

epoch 17 was done for 479.988611 seconds
Epoch 18/299
----------
Mon Jan 31 02:14:50 2022
batch 0, train loss = 0.20, mean loss = 0.20
Mon Jan 31 02:14:56 2022
batch 10, train loss = 0.32, mean loss = 0.30
Mon Jan 31 02:15:50 2022
batch 20, train loss = 0.22, mean loss = 0.30
Mon Jan 31 02:16:45 2022
batch 30, train loss = 0.34, mean loss = 0.31
Mon Jan 31 02:17:41 2022
batch 40, train loss = 0.18, mean loss = 0.31
Mon Jan 31 02:18:39 2022
batch 50, train loss = 0.34, mean loss = 0.31
Mon Jan 31 02:19:34 2022
batch 60, train loss = 0.19, mean loss = 0.30
Mon Jan 31 02:20:29 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 02:21:20 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 02:22:15 2022
val Loss: 0.03

epoch 18 was done for 482.558185 seconds
Epoch 19/299
----------
Mon Jan 31 02:22:53 2022
batch 0, train loss = 0.20, mean loss = 0.20
Mon Jan 31 02:22:58 2022
batch 10, train loss = 0.32, mean loss = 0.30
Mon Jan 31 02:23:53 2022
batch 20, train loss = 0.22, mean loss = 0.30
Mon Jan 31 02:24:47 2022
batch 30, train loss = 0.34, mean loss = 0.31
Mon Jan 31 02:25:42 2022
batch 40, train loss = 0.18, mean loss = 0.30
Mon Jan 31 02:26:36 2022
batch 50, train loss = 0.34, mean loss = 0.31
Mon Jan 31 02:27:31 2022
batch 60, train loss = 0.19, mean loss = 0.30
Mon Jan 31 02:28:25 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 02:29:17 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 02:30:12 2022
val Loss: 0.03

epoch 19 was done for 476.883430 seconds
Epoch 20/299
----------
Mon Jan 31 02:30:49 2022
batch 0, train loss = 0.19, mean loss = 0.19
Mon Jan 31 02:30:55 2022
batch 10, train loss = 0.32, mean loss = 0.30
Mon Jan 31 02:31:49 2022
batch 20, train loss = 0.21, mean loss = 0.30
Mon Jan 31 02:32:44 2022
batch 30, train loss = 0.34, mean loss = 0.31
Mon Jan 31 02:33:38 2022
batch 40, train loss = 0.17, mean loss = 0.30
Mon Jan 31 02:34:33 2022
batch 50, train loss = 0.34, mean loss = 0.31
Mon Jan 31 02:35:27 2022
batch 60, train loss = 0.18, mean loss = 0.30
Mon Jan 31 02:36:23 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 02:37:14 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 02:38:10 2022
val Loss: 0.03

epoch 20 was done for 478.444416 seconds
Epoch 21/299
----------
Mon Jan 31 02:38:48 2022
batch 0, train loss = 0.19, mean loss = 0.19
Mon Jan 31 02:38:53 2022
batch 10, train loss = 0.32, mean loss = 0.30
Mon Jan 31 02:39:48 2022
batch 20, train loss = 0.21, mean loss = 0.30
Mon Jan 31 02:40:43 2022
batch 30, train loss = 0.34, mean loss = 0.31
Mon Jan 31 02:41:37 2022
batch 40, train loss = 0.17, mean loss = 0.30
Mon Jan 31 02:42:32 2022
batch 50, train loss = 0.34, mean loss = 0.31
Mon Jan 31 02:43:26 2022
batch 60, train loss = 0.18, mean loss = 0.30
Mon Jan 31 02:44:21 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 02:45:12 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 02:46:07 2022
val Loss: 0.03

epoch 21 was done for 476.592458 seconds
Epoch 22/299
----------
Mon Jan 31 02:46:45 2022
batch 0, train loss = 0.18, mean loss = 0.18
Mon Jan 31 02:46:50 2022
batch 10, train loss = 0.32, mean loss = 0.30
Mon Jan 31 02:47:44 2022
batch 20, train loss = 0.20, mean loss = 0.30
Mon Jan 31 02:48:39 2022
batch 30, train loss = 0.34, mean loss = 0.30
Mon Jan 31 02:49:33 2022
batch 40, train loss = 0.16, mean loss = 0.30
Mon Jan 31 02:50:28 2022
batch 50, train loss = 0.34, mean loss = 0.31
Mon Jan 31 02:51:22 2022
batch 60, train loss = 0.17, mean loss = 0.30
Mon Jan 31 02:52:17 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 02:53:09 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 02:54:04 2022
val Loss: 0.03

epoch 22 was done for 476.681743 seconds
Epoch 23/299
----------
Mon Jan 31 02:54:41 2022
batch 0, train loss = 0.18, mean loss = 0.18
Mon Jan 31 02:54:47 2022
batch 10, train loss = 0.32, mean loss = 0.30
Mon Jan 31 02:55:43 2022
batch 20, train loss = 0.19, mean loss = 0.30
Mon Jan 31 02:56:37 2022
batch 30, train loss = 0.34, mean loss = 0.30
Mon Jan 31 02:57:32 2022
batch 40, train loss = 0.16, mean loss = 0.30
Mon Jan 31 02:58:26 2022
batch 50, train loss = 0.34, mean loss = 0.30
Mon Jan 31 02:59:20 2022
batch 60, train loss = 0.17, mean loss = 0.30
Mon Jan 31 03:00:16 2022
train Loss: 0.30

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 03:01:07 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 03:02:03 2022
val Loss: 0.03

epoch 23 was done for 479.581140 seconds
Epoch 24/299
----------
Mon Jan 31 03:02:41 2022
batch 0, train loss = 0.17, mean loss = 0.17
Mon Jan 31 03:02:46 2022
batch 10, train loss = 0.33, mean loss = 0.30
Mon Jan 31 03:03:41 2022
batch 20, train loss = 0.19, mean loss = 0.30
Mon Jan 31 03:04:35 2022
batch 30, train loss = 0.34, mean loss = 0.30
Mon Jan 31 03:05:29 2022
batch 40, train loss = 0.15, mean loss = 0.30
Mon Jan 31 03:06:24 2022
batch 50, train loss = 0.34, mean loss = 0.30
Mon Jan 31 03:07:18 2022
batch 60, train loss = 0.16, mean loss = 0.30
Mon Jan 31 03:08:13 2022
train Loss: 0.30

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 03:09:04 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 03:09:59 2022
val Loss: 0.03

epoch 24 was done for 475.404588 seconds
Epoch 25/299
----------
Mon Jan 31 03:10:36 2022
batch 0, train loss = 0.17, mean loss = 0.17
Mon Jan 31 03:10:42 2022
batch 10, train loss = 0.33, mean loss = 0.30
Mon Jan 31 03:11:36 2022
batch 20, train loss = 0.18, mean loss = 0.30
Mon Jan 31 03:12:31 2022
batch 30, train loss = 0.34, mean loss = 0.30
Mon Jan 31 03:13:25 2022
batch 40, train loss = 0.15, mean loss = 0.30
Mon Jan 31 03:14:20 2022
batch 50, train loss = 0.34, mean loss = 0.30
Mon Jan 31 03:15:15 2022
batch 60, train loss = 0.15, mean loss = 0.30
Mon Jan 31 03:16:10 2022
train Loss: 0.30

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 03:17:02 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 03:17:57 2022
val Loss: 0.03

epoch 25 was done for 479.041675 seconds
Epoch 26/299
----------
Mon Jan 31 03:18:35 2022
batch 0, train loss = 0.16, mean loss = 0.16
Mon Jan 31 03:18:41 2022
batch 10, train loss = 0.33, mean loss = 0.30
Mon Jan 31 03:19:36 2022
batch 20, train loss = 0.18, mean loss = 0.30
Mon Jan 31 03:20:31 2022
batch 30, train loss = 0.34, mean loss = 0.30
Mon Jan 31 03:21:26 2022
batch 40, train loss = 0.14, mean loss = 0.30
Mon Jan 31 03:22:21 2022
batch 50, train loss = 0.34, mean loss = 0.30
Mon Jan 31 03:23:16 2022
batch 60, train loss = 0.15, mean loss = 0.30
Mon Jan 31 03:24:11 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:25:03 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 03:25:59 2022
val Loss: 0.03

epoch 26 was done for 481.599017 seconds
Epoch 27/299
----------
Mon Jan 31 03:26:37 2022
batch 0, train loss = 0.16, mean loss = 0.16
Mon Jan 31 03:26:42 2022
batch 10, train loss = 0.33, mean loss = 0.30
Mon Jan 31 03:27:37 2022
batch 20, train loss = 0.17, mean loss = 0.30
Mon Jan 31 03:28:32 2022
batch 30, train loss = 0.35, mean loss = 0.30
Mon Jan 31 03:29:27 2022
batch 40, train loss = 0.14, mean loss = 0.30
Mon Jan 31 03:30:22 2022
batch 50, train loss = 0.34, mean loss = 0.30
Mon Jan 31 03:31:17 2022
batch 60, train loss = 0.14, mean loss = 0.30
Mon Jan 31 03:32:12 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:33:03 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 03:33:57 2022
val Loss: 0.03

epoch 27 was done for 478.379862 seconds
Epoch 28/299
----------
Mon Jan 31 03:34:35 2022
batch 0, train loss = 0.15, mean loss = 0.15
Mon Jan 31 03:34:41 2022
batch 10, train loss = 0.33, mean loss = 0.30
Mon Jan 31 03:35:35 2022
batch 20, train loss = 0.17, mean loss = 0.30
Mon Jan 31 03:36:29 2022
batch 30, train loss = 0.35, mean loss = 0.30
Mon Jan 31 03:37:23 2022
batch 40, train loss = 0.13, mean loss = 0.30
Mon Jan 31 03:38:18 2022
batch 50, train loss = 0.34, mean loss = 0.30
Mon Jan 31 03:39:12 2022
batch 60, train loss = 0.14, mean loss = 0.30
Mon Jan 31 03:40:06 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:40:57 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 03:41:52 2022
val Loss: 0.03

epoch 28 was done for 474.291748 seconds
Epoch 29/299
----------
Mon Jan 31 03:42:30 2022
batch 0, train loss = 0.15, mean loss = 0.15
Mon Jan 31 03:42:35 2022
batch 10, train loss = 0.33, mean loss = 0.29
Mon Jan 31 03:43:29 2022
batch 20, train loss = 0.16, mean loss = 0.30
Mon Jan 31 03:44:23 2022
batch 30, train loss = 0.35, mean loss = 0.30
Mon Jan 31 03:45:18 2022
batch 40, train loss = 0.13, mean loss = 0.30
Mon Jan 31 03:46:12 2022
batch 50, train loss = 0.34, mean loss = 0.30
Mon Jan 31 03:47:06 2022
batch 60, train loss = 0.13, mean loss = 0.30
Mon Jan 31 03:48:01 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:48:51 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 03:49:47 2022
val Loss: 0.03

epoch 29 was done for 475.373064 seconds
Epoch 30/299
----------
Mon Jan 31 03:50:25 2022
batch 0, train loss = 0.14, mean loss = 0.14
Mon Jan 31 03:50:30 2022
batch 10, train loss = 0.33, mean loss = 0.29
Mon Jan 31 03:51:25 2022
batch 20, train loss = 0.16, mean loss = 0.29
Mon Jan 31 03:52:19 2022
batch 30, train loss = 0.35, mean loss = 0.30
Mon Jan 31 03:53:14 2022
batch 40, train loss = 0.12, mean loss = 0.30
Mon Jan 31 03:54:08 2022
batch 50, train loss = 0.34, mean loss = 0.30
Mon Jan 31 03:55:03 2022
batch 60, train loss = 0.13, mean loss = 0.30
Mon Jan 31 03:55:58 2022
train Loss: 0.30

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 03:56:49 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 03:57:44 2022
val Loss: 0.03

epoch 30 was done for 477.116786 seconds
Epoch 31/299
----------
Mon Jan 31 03:58:22 2022
batch 0, train loss = 0.14, mean loss = 0.14
Mon Jan 31 03:58:28 2022
batch 10, train loss = 0.33, mean loss = 0.29
Mon Jan 31 03:59:22 2022
batch 20, train loss = 0.15, mean loss = 0.29
Mon Jan 31 04:00:17 2022
batch 30, train loss = 0.35, mean loss = 0.30
Mon Jan 31 04:01:12 2022
batch 40, train loss = 0.12, mean loss = 0.30
Mon Jan 31 04:02:06 2022
batch 50, train loss = 0.34, mean loss = 0.30
Mon Jan 31 04:03:01 2022
batch 60, train loss = 0.13, mean loss = 0.30
Mon Jan 31 04:03:55 2022
train Loss: 0.30

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 04:04:46 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 04:05:41 2022
val Loss: 0.03

epoch 31 was done for 476.456811 seconds
Epoch 32/299
----------
Mon Jan 31 04:06:18 2022
batch 0, train loss = 0.13, mean loss = 0.13
Mon Jan 31 04:06:24 2022
batch 10, train loss = 0.33, mean loss = 0.29
Mon Jan 31 04:07:18 2022
batch 20, train loss = 0.15, mean loss = 0.29
Mon Jan 31 04:08:12 2022
batch 30, train loss = 0.35, mean loss = 0.30
Mon Jan 31 04:09:06 2022
batch 40, train loss = 0.12, mean loss = 0.30
Mon Jan 31 04:10:00 2022
batch 50, train loss = 0.34, mean loss = 0.30
Mon Jan 31 04:10:54 2022
batch 60, train loss = 0.12, mean loss = 0.30
Mon Jan 31 04:11:48 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:12:38 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 04:13:34 2022
val Loss: 0.03

epoch 32 was done for 472.946177 seconds
Epoch 33/299
----------
Mon Jan 31 04:14:11 2022
batch 0, train loss = 0.13, mean loss = 0.13
Mon Jan 31 04:14:17 2022
batch 10, train loss = 0.33, mean loss = 0.29
Mon Jan 31 04:15:11 2022
batch 20, train loss = 0.14, mean loss = 0.29
Mon Jan 31 04:16:05 2022
batch 30, train loss = 0.35, mean loss = 0.30
Mon Jan 31 04:16:59 2022
batch 40, train loss = 0.11, mean loss = 0.30
Mon Jan 31 04:17:53 2022
batch 50, train loss = 0.35, mean loss = 0.30
Mon Jan 31 04:18:47 2022
batch 60, train loss = 0.12, mean loss = 0.30
Mon Jan 31 04:19:42 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:20:32 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 04:21:27 2022
val Loss: 0.03

epoch 33 was done for 473.384406 seconds
Epoch 34/299
----------
Mon Jan 31 04:22:05 2022
batch 0, train loss = 0.12, mean loss = 0.12
Mon Jan 31 04:22:10 2022
batch 10, train loss = 0.33, mean loss = 0.29
Mon Jan 31 04:23:04 2022
batch 20, train loss = 0.14, mean loss = 0.29
Mon Jan 31 04:23:59 2022
batch 30, train loss = 0.35, mean loss = 0.30
Mon Jan 31 04:24:53 2022
batch 40, train loss = 0.11, mean loss = 0.30
Mon Jan 31 04:25:48 2022
batch 50, train loss = 0.35, mean loss = 0.30
Mon Jan 31 04:26:42 2022
batch 60, train loss = 0.11, mean loss = 0.30
Mon Jan 31 04:27:36 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:28:27 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 04:29:22 2022
val Loss: 0.03

epoch 34 was done for 474.611514 seconds
Epoch 35/299
----------
Mon Jan 31 04:29:59 2022
batch 0, train loss = 0.12, mean loss = 0.12
Mon Jan 31 04:30:05 2022
batch 10, train loss = 0.33, mean loss = 0.29
Mon Jan 31 04:30:59 2022
batch 20, train loss = 0.13, mean loss = 0.29
Mon Jan 31 04:31:53 2022
batch 30, train loss = 0.35, mean loss = 0.30
Mon Jan 31 04:32:47 2022
batch 40, train loss = 0.10, mean loss = 0.30
Mon Jan 31 04:33:41 2022
batch 50, train loss = 0.35, mean loss = 0.30
Mon Jan 31 04:34:35 2022
batch 60, train loss = 0.11, mean loss = 0.30
Mon Jan 31 04:35:29 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:36:20 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 04:37:16 2022
val Loss: 0.03

epoch 35 was done for 474.417979 seconds
Epoch 36/299
----------
Mon Jan 31 04:37:54 2022
batch 0, train loss = 0.11, mean loss = 0.11
Mon Jan 31 04:37:59 2022
batch 10, train loss = 0.34, mean loss = 0.29
Mon Jan 31 04:38:53 2022
batch 20, train loss = 0.13, mean loss = 0.29
Mon Jan 31 04:39:48 2022
batch 30, train loss = 0.35, mean loss = 0.30
Mon Jan 31 04:40:42 2022
batch 40, train loss = 0.10, mean loss = 0.30
Mon Jan 31 04:41:36 2022
batch 50, train loss = 0.35, mean loss = 0.30
Mon Jan 31 04:42:30 2022
batch 60, train loss = 0.10, mean loss = 0.30
Mon Jan 31 04:43:24 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:44:15 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 04:45:10 2022
val Loss: 0.03

epoch 36 was done for 473.427308 seconds
Epoch 37/299
----------
Mon Jan 31 04:45:47 2022
batch 0, train loss = 0.11, mean loss = 0.11
Mon Jan 31 04:45:53 2022
batch 10, train loss = 0.34, mean loss = 0.29
Mon Jan 31 04:46:47 2022
batch 20, train loss = 0.12, mean loss = 0.29
Mon Jan 31 04:47:41 2022
batch 30, train loss = 0.36, mean loss = 0.30
Mon Jan 31 04:48:35 2022
batch 40, train loss = 0.10, mean loss = 0.30
Mon Jan 31 04:49:29 2022
batch 50, train loss = 0.35, mean loss = 0.30
Mon Jan 31 04:50:24 2022
batch 60, train loss = 0.10, mean loss = 0.30
Mon Jan 31 04:51:18 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:52:09 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 04:53:04 2022
val Loss: 0.03

epoch 37 was done for 473.957705 seconds
Epoch 38/299
----------
Mon Jan 31 04:53:41 2022
batch 0, train loss = 0.11, mean loss = 0.11
Mon Jan 31 04:53:47 2022
batch 10, train loss = 0.34, mean loss = 0.29
Mon Jan 31 04:54:41 2022
batch 20, train loss = 0.12, mean loss = 0.29
Mon Jan 31 04:55:35 2022
batch 30, train loss = 0.36, mean loss = 0.30
Mon Jan 31 04:56:29 2022
batch 40, train loss = 0.09, mean loss = 0.30
Mon Jan 31 04:57:23 2022
batch 50, train loss = 0.35, mean loss = 0.30
Mon Jan 31 04:58:17 2022
batch 60, train loss = 0.10, mean loss = 0.30
Mon Jan 31 04:59:11 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:00:02 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 05:00:57 2022
val Loss: 0.03

epoch 38 was done for 474.105318 seconds
Epoch 39/299
----------
Mon Jan 31 05:01:35 2022
batch 0, train loss = 0.10, mean loss = 0.10
Mon Jan 31 05:01:41 2022
batch 10, train loss = 0.34, mean loss = 0.29
Mon Jan 31 05:02:35 2022
batch 20, train loss = 0.11, mean loss = 0.29
Mon Jan 31 05:03:29 2022
batch 30, train loss = 0.36, mean loss = 0.30
Mon Jan 31 05:04:24 2022
batch 40, train loss = 0.09, mean loss = 0.30
Mon Jan 31 05:05:18 2022
batch 50, train loss = 0.35, mean loss = 0.30
Mon Jan 31 05:06:12 2022
batch 60, train loss = 0.09, mean loss = 0.30
Mon Jan 31 05:07:06 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:07:57 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 05:08:52 2022
val Loss: 0.03

epoch 39 was done for 474.347338 seconds
Epoch 40/299
----------
Mon Jan 31 05:09:30 2022
batch 0, train loss = 0.10, mean loss = 0.10
Mon Jan 31 05:09:35 2022
batch 10, train loss = 0.34, mean loss = 0.29
Mon Jan 31 05:10:29 2022
batch 20, train loss = 0.11, mean loss = 0.29
Mon Jan 31 05:11:24 2022
batch 30, train loss = 0.36, mean loss = 0.30
Mon Jan 31 05:12:18 2022
batch 40, train loss = 0.09, mean loss = 0.30
Mon Jan 31 05:13:12 2022
batch 50, train loss = 0.35, mean loss = 0.30
Mon Jan 31 05:14:07 2022
batch 60, train loss = 0.09, mean loss = 0.30
Mon Jan 31 05:15:01 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:15:52 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 05:16:47 2022
val Loss: 0.03

epoch 40 was done for 475.063878 seconds
Epoch 41/299
----------
Mon Jan 31 05:17:25 2022
batch 0, train loss = 0.09, mean loss = 0.09
Mon Jan 31 05:17:30 2022
batch 10, train loss = 0.34, mean loss = 0.29
Mon Jan 31 05:18:24 2022
batch 20, train loss = 0.11, mean loss = 0.29
Mon Jan 31 05:19:18 2022
batch 30, train loss = 0.36, mean loss = 0.30
Mon Jan 31 05:20:13 2022
batch 40, train loss = 0.08, mean loss = 0.30
Mon Jan 31 05:21:07 2022
batch 50, train loss = 0.35, mean loss = 0.30
Mon Jan 31 05:22:01 2022
batch 60, train loss = 0.09, mean loss = 0.30
Mon Jan 31 05:22:55 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:23:46 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 05:24:42 2022
val Loss: 0.03

epoch 41 was done for 474.995235 seconds
Epoch 42/299
----------
Mon Jan 31 05:25:20 2022
batch 0, train loss = 0.09, mean loss = 0.09
Mon Jan 31 05:25:25 2022
batch 10, train loss = 0.34, mean loss = 0.29
Mon Jan 31 05:26:19 2022
batch 20, train loss = 0.10, mean loss = 0.29
Mon Jan 31 05:27:14 2022
batch 30, train loss = 0.36, mean loss = 0.30
Mon Jan 31 05:28:08 2022
batch 40, train loss = 0.08, mean loss = 0.30
Mon Jan 31 05:29:02 2022
batch 50, train loss = 0.35, mean loss = 0.30
Mon Jan 31 05:29:57 2022
batch 60, train loss = 0.08, mean loss = 0.30
Mon Jan 31 05:30:51 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:31:42 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 05:32:37 2022
val Loss: 0.03

epoch 42 was done for 474.970978 seconds
Epoch 43/299
----------
Mon Jan 31 05:33:15 2022
batch 0, train loss = 0.09, mean loss = 0.09
Mon Jan 31 05:33:20 2022
batch 10, train loss = 0.34, mean loss = 0.29
Mon Jan 31 05:34:14 2022
batch 20, train loss = 0.10, mean loss = 0.29
Mon Jan 31 05:35:09 2022
batch 30, train loss = 0.36, mean loss = 0.30
Mon Jan 31 05:36:03 2022
batch 40, train loss = 0.08, mean loss = 0.30
Mon Jan 31 05:36:57 2022
batch 50, train loss = 0.35, mean loss = 0.30
Mon Jan 31 05:37:51 2022
batch 60, train loss = 0.08, mean loss = 0.30
Mon Jan 31 05:38:46 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:39:37 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 05:40:31 2022
val Loss: 0.03

epoch 43 was done for 474.250297 seconds
Epoch 44/299
----------
Mon Jan 31 05:41:09 2022
batch 0, train loss = 0.08, mean loss = 0.08
Mon Jan 31 05:41:14 2022
batch 10, train loss = 0.34, mean loss = 0.29
Mon Jan 31 05:42:08 2022
batch 20, train loss = 0.09, mean loss = 0.29
Mon Jan 31 05:43:02 2022
batch 30, train loss = 0.36, mean loss = 0.30
Mon Jan 31 05:43:56 2022
batch 40, train loss = 0.07, mean loss = 0.30
Mon Jan 31 05:44:51 2022
batch 50, train loss = 0.35, mean loss = 0.30
Mon Jan 31 05:45:44 2022
batch 60, train loss = 0.08, mean loss = 0.30
Mon Jan 31 05:46:39 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:47:30 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 05:48:25 2022
val Loss: 0.03

epoch 44 was done for 473.558869 seconds
Epoch 45/299
----------
Mon Jan 31 05:49:03 2022
batch 0, train loss = 0.08, mean loss = 0.08
Mon Jan 31 05:49:08 2022
batch 10, train loss = 0.35, mean loss = 0.29
Mon Jan 31 05:50:02 2022
batch 20, train loss = 0.09, mean loss = 0.29
Mon Jan 31 05:50:56 2022
batch 30, train loss = 0.37, mean loss = 0.30
Mon Jan 31 05:51:50 2022
batch 40, train loss = 0.07, mean loss = 0.30
Mon Jan 31 05:52:45 2022
batch 50, train loss = 0.36, mean loss = 0.30
Mon Jan 31 05:53:39 2022
batch 60, train loss = 0.07, mean loss = 0.30
Mon Jan 31 05:54:33 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:55:24 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 05:56:18 2022
val Loss: 0.03

epoch 45 was done for 473.573590 seconds
Epoch 46/299
----------
Mon Jan 31 05:56:56 2022
batch 0, train loss = 0.08, mean loss = 0.08
Mon Jan 31 05:57:02 2022
batch 10, train loss = 0.35, mean loss = 0.29
Mon Jan 31 05:57:55 2022
batch 20, train loss = 0.09, mean loss = 0.29
Mon Jan 31 05:58:50 2022
batch 30, train loss = 0.37, mean loss = 0.30
Mon Jan 31 05:59:44 2022
batch 40, train loss = 0.07, mean loss = 0.30
Mon Jan 31 06:00:38 2022
batch 50, train loss = 0.36, mean loss = 0.30
Mon Jan 31 06:01:32 2022
batch 60, train loss = 0.07, mean loss = 0.30
Mon Jan 31 06:02:26 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:03:17 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 06:04:12 2022
val Loss: 0.03

epoch 46 was done for 473.154806 seconds
Epoch 47/299
----------
Mon Jan 31 06:04:49 2022
batch 0, train loss = 0.07, mean loss = 0.07
Mon Jan 31 06:04:55 2022
batch 10, train loss = 0.35, mean loss = 0.29
Mon Jan 31 06:05:49 2022
batch 20, train loss = 0.08, mean loss = 0.29
Mon Jan 31 06:06:43 2022
batch 30, train loss = 0.37, mean loss = 0.30
Mon Jan 31 06:07:37 2022
batch 40, train loss = 0.07, mean loss = 0.30
Mon Jan 31 06:08:31 2022
batch 50, train loss = 0.36, mean loss = 0.30
Mon Jan 31 06:09:25 2022
batch 60, train loss = 0.07, mean loss = 0.30
Mon Jan 31 06:10:19 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:11:11 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 06:12:06 2022
val Loss: 0.03

epoch 47 was done for 474.661397 seconds
Epoch 48/299
----------
Mon Jan 31 06:12:44 2022
batch 0, train loss = 0.07, mean loss = 0.07
Mon Jan 31 06:12:49 2022
batch 10, train loss = 0.35, mean loss = 0.29
Mon Jan 31 06:13:43 2022
batch 20, train loss = 0.08, mean loss = 0.29
Mon Jan 31 06:14:38 2022
batch 30, train loss = 0.37, mean loss = 0.30
Mon Jan 31 06:15:32 2022
batch 40, train loss = 0.06, mean loss = 0.30
Mon Jan 31 06:16:26 2022
batch 50, train loss = 0.36, mean loss = 0.30
Mon Jan 31 06:17:20 2022
batch 60, train loss = 0.06, mean loss = 0.30
Mon Jan 31 06:18:14 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:19:05 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 06:19:59 2022
val Loss: 0.03

epoch 48 was done for 472.962306 seconds
Epoch 49/299
----------
Mon Jan 31 06:20:37 2022
batch 0, train loss = 0.07, mean loss = 0.07
Mon Jan 31 06:20:42 2022
batch 10, train loss = 0.35, mean loss = 0.29
Mon Jan 31 06:21:37 2022
batch 20, train loss = 0.08, mean loss = 0.29
Mon Jan 31 06:22:31 2022
batch 30, train loss = 0.38, mean loss = 0.30
Mon Jan 31 06:23:25 2022
batch 40, train loss = 0.06, mean loss = 0.30
Mon Jan 31 06:24:20 2022
batch 50, train loss = 0.37, mean loss = 0.30
Mon Jan 31 06:25:14 2022
batch 60, train loss = 0.06, mean loss = 0.30
Mon Jan 31 06:26:08 2022
train Loss: 0.30

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 06:26:59 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 06:27:54 2022
val Loss: 0.03

epoch 49 was done for 474.884808 seconds
Epoch 50/299
----------
Mon Jan 31 06:28:32 2022
batch 0, train loss = 0.07, mean loss = 0.07
Mon Jan 31 06:28:37 2022
batch 10, train loss = 0.35, mean loss = 0.29
Mon Jan 31 06:29:31 2022
batch 20, train loss = 0.08, mean loss = 0.29
Mon Jan 31 06:30:25 2022
batch 30, train loss = 0.38, mean loss = 0.30
Mon Jan 31 06:31:19 2022
batch 40, train loss = 0.07, mean loss = 0.30
Mon Jan 31 06:32:14 2022
batch 50, train loss = 0.36, mean loss = 0.30
Mon Jan 31 06:33:08 2022
batch 60, train loss = 0.07, mean loss = 0.30
Mon Jan 31 06:34:02 2022
train Loss: 0.30

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 06:34:53 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 06:35:49 2022
val Loss: 0.03

epoch 50 was done for 475.295720 seconds
Epoch 51/299
----------
Mon Jan 31 06:36:27 2022
batch 0, train loss = 0.07, mean loss = 0.07
Mon Jan 31 06:36:32 2022
batch 10, train loss = 0.35, mean loss = 0.29
Mon Jan 31 06:37:27 2022
batch 20, train loss = 0.07, mean loss = 0.29
Mon Jan 31 06:38:21 2022
batch 30, train loss = 0.37, mean loss = 0.30
Mon Jan 31 06:39:15 2022
batch 40, train loss = 0.06, mean loss = 0.30
Mon Jan 31 06:40:09 2022
batch 50, train loss = 0.36, mean loss = 0.31
Mon Jan 31 06:41:03 2022
batch 60, train loss = 0.06, mean loss = 0.30
Mon Jan 31 06:41:57 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:42:48 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 06:43:43 2022
val Loss: 0.03

epoch 51 was done for 472.950285 seconds
Epoch 52/299
----------
Mon Jan 31 06:44:20 2022
batch 0, train loss = 0.06, mean loss = 0.06
Mon Jan 31 06:44:25 2022
batch 10, train loss = 0.35, mean loss = 0.29
Mon Jan 31 06:45:20 2022
batch 20, train loss = 0.07, mean loss = 0.29
Mon Jan 31 06:46:14 2022
batch 30, train loss = 0.37, mean loss = 0.30
Mon Jan 31 06:47:08 2022
batch 40, train loss = 0.05, mean loss = 0.30
Mon Jan 31 06:48:03 2022
batch 50, train loss = 0.36, mean loss = 0.30
Mon Jan 31 06:48:57 2022
batch 60, train loss = 0.05, mean loss = 0.30
Mon Jan 31 06:49:51 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:50:42 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 06:51:37 2022
val Loss: 0.03

epoch 52 was done for 474.268357 seconds
Epoch 53/299
----------
Mon Jan 31 06:52:14 2022
batch 0, train loss = 0.06, mean loss = 0.06
Mon Jan 31 06:52:20 2022
batch 10, train loss = 0.35, mean loss = 0.29
Mon Jan 31 06:53:14 2022
batch 20, train loss = 0.06, mean loss = 0.29
Mon Jan 31 06:54:08 2022
batch 30, train loss = 0.37, mean loss = 0.30
Mon Jan 31 06:55:02 2022
batch 40, train loss = 0.05, mean loss = 0.30
Mon Jan 31 06:55:56 2022
batch 50, train loss = 0.36, mean loss = 0.30
Mon Jan 31 06:56:50 2022
batch 60, train loss = 0.05, mean loss = 0.30
Mon Jan 31 06:57:45 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:58:36 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 06:59:31 2022
val Loss: 0.03

epoch 53 was done for 474.474340 seconds
Epoch 54/299
----------
Mon Jan 31 07:00:09 2022
batch 0, train loss = 0.06, mean loss = 0.06
Mon Jan 31 07:00:14 2022
batch 10, train loss = 0.35, mean loss = 0.29
Mon Jan 31 07:01:08 2022
batch 20, train loss = 0.06, mean loss = 0.29
Mon Jan 31 07:02:03 2022
batch 30, train loss = 0.37, mean loss = 0.30
Mon Jan 31 07:02:57 2022
batch 40, train loss = 0.05, mean loss = 0.30
Mon Jan 31 07:03:51 2022
batch 50, train loss = 0.36, mean loss = 0.30
Mon Jan 31 07:04:46 2022
batch 60, train loss = 0.05, mean loss = 0.30
Mon Jan 31 07:05:40 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:06:31 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 07:07:26 2022
val Loss: 0.03

epoch 54 was done for 475.162873 seconds
Epoch 55/299
----------
Mon Jan 31 07:08:04 2022
batch 0, train loss = 0.05, mean loss = 0.05
Mon Jan 31 07:08:09 2022
batch 10, train loss = 0.35, mean loss = 0.29
Mon Jan 31 07:09:04 2022
batch 20, train loss = 0.06, mean loss = 0.29
Mon Jan 31 07:09:59 2022
batch 30, train loss = 0.37, mean loss = 0.30
Mon Jan 31 07:10:53 2022
batch 40, train loss = 0.05, mean loss = 0.30
Mon Jan 31 07:11:48 2022
batch 50, train loss = 0.36, mean loss = 0.30
Mon Jan 31 07:12:42 2022
batch 60, train loss = 0.05, mean loss = 0.30
Mon Jan 31 07:13:37 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:14:28 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 07:15:23 2022
val Loss: 0.03

epoch 55 was done for 476.280611 seconds
Epoch 56/299
----------
Mon Jan 31 07:16:00 2022
batch 0, train loss = 0.05, mean loss = 0.05
Mon Jan 31 07:16:06 2022
batch 10, train loss = 0.35, mean loss = 0.29
Mon Jan 31 07:17:00 2022
batch 20, train loss = 0.06, mean loss = 0.29
Mon Jan 31 07:17:54 2022
batch 30, train loss = 0.37, mean loss = 0.30
Mon Jan 31 07:18:48 2022
batch 40, train loss = 0.05, mean loss = 0.30
Mon Jan 31 07:19:42 2022
batch 50, train loss = 0.36, mean loss = 0.30
Mon Jan 31 07:20:36 2022
batch 60, train loss = 0.05, mean loss = 0.30
Mon Jan 31 07:21:30 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:22:21 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 07:23:17 2022
val Loss: 0.03

epoch 56 was done for 474.591347 seconds
Epoch 57/299
----------
Mon Jan 31 07:23:55 2022
batch 0, train loss = 0.05, mean loss = 0.05
Mon Jan 31 07:24:00 2022
batch 10, train loss = 0.35, mean loss = 0.29
Mon Jan 31 07:24:54 2022
batch 20, train loss = 0.05, mean loss = 0.29
Mon Jan 31 07:25:49 2022
batch 30, train loss = 0.38, mean loss = 0.30
Mon Jan 31 07:26:43 2022
batch 40, train loss = 0.04, mean loss = 0.30
Mon Jan 31 07:27:38 2022
batch 50, train loss = 0.37, mean loss = 0.30
Mon Jan 31 07:28:32 2022
batch 60, train loss = 0.04, mean loss = 0.30
Mon Jan 31 07:29:27 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:30:18 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 07:31:13 2022
val Loss: 0.03

epoch 57 was done for 478.387623 seconds
Epoch 58/299
----------
Mon Jan 31 07:31:53 2022
batch 0, train loss = 0.05, mean loss = 0.05
Mon Jan 31 07:31:59 2022
batch 10, train loss = 0.35, mean loss = 0.29
Mon Jan 31 07:32:53 2022
batch 20, train loss = 0.05, mean loss = 0.29
Mon Jan 31 07:33:48 2022
batch 30, train loss = 0.38, mean loss = 0.30
Mon Jan 31 07:34:42 2022
batch 40, train loss = 0.04, mean loss = 0.30
Mon Jan 31 07:35:36 2022
batch 50, train loss = 0.37, mean loss = 0.30
Mon Jan 31 07:36:31 2022
batch 60, train loss = 0.04, mean loss = 0.30
Mon Jan 31 07:37:25 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:38:16 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 07:39:11 2022
val Loss: 0.03

epoch 58 was done for 475.637301 seconds
Epoch 59/299
----------
Mon Jan 31 07:39:49 2022
batch 0, train loss = 0.05, mean loss = 0.05
Mon Jan 31 07:39:54 2022
batch 10, train loss = 0.35, mean loss = 0.29
Mon Jan 31 07:40:57 2022
batch 20, train loss = 0.05, mean loss = 0.29
Mon Jan 31 07:41:52 2022
batch 30, train loss = 0.38, mean loss = 0.30
Mon Jan 31 07:42:46 2022
batch 40, train loss = 0.04, mean loss = 0.30
Mon Jan 31 07:43:41 2022
batch 50, train loss = 0.37, mean loss = 0.30
Mon Jan 31 07:44:35 2022
batch 60, train loss = 0.04, mean loss = 0.30
Mon Jan 31 07:45:34 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:46:25 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 07:47:23 2022
val Loss: 0.03

epoch 59 was done for 492.103547 seconds
Epoch 60/299
----------
Mon Jan 31 07:48:01 2022
batch 0, train loss = 0.04, mean loss = 0.04
Mon Jan 31 07:48:06 2022
batch 10, train loss = 0.36, mean loss = 0.29
Mon Jan 31 07:49:01 2022
batch 20, train loss = 0.05, mean loss = 0.29
Mon Jan 31 07:49:55 2022
batch 30, train loss = 0.38, mean loss = 0.30
Mon Jan 31 07:50:50 2022
batch 40, train loss = 0.04, mean loss = 0.30
Mon Jan 31 07:51:44 2022
batch 50, train loss = 0.37, mean loss = 0.30
Mon Jan 31 07:52:39 2022
batch 60, train loss = 0.04, mean loss = 0.30
Mon Jan 31 07:53:33 2022
train Loss: 0.30

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 07:54:24 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 07:55:21 2022
val Loss: 0.03

epoch 60 was done for 477.321329 seconds
Epoch 61/299
----------
Mon Jan 31 07:55:58 2022
batch 0, train loss = 0.04, mean loss = 0.04
Mon Jan 31 07:56:04 2022
batch 10, train loss = 0.36, mean loss = 0.29
Mon Jan 31 07:56:58 2022
batch 20, train loss = 0.05, mean loss = 0.29
Mon Jan 31 07:57:53 2022
batch 30, train loss = 0.38, mean loss = 0.30
Mon Jan 31 07:58:47 2022
batch 40, train loss = 0.04, mean loss = 0.30
Mon Jan 31 07:59:42 2022
batch 50, train loss = 0.37, mean loss = 0.30
Mon Jan 31 08:00:36 2022
batch 60, train loss = 0.04, mean loss = 0.30
Mon Jan 31 08:01:31 2022
train Loss: 0.30

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 08:02:23 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 08:03:18 2022
val Loss: 0.03

epoch 61 was done for 478.240133 seconds
Epoch 62/299
----------
Mon Jan 31 08:03:56 2022
batch 0, train loss = 0.04, mean loss = 0.04
Mon Jan 31 08:04:02 2022
batch 10, train loss = 0.36, mean loss = 0.29
Mon Jan 31 08:04:57 2022
batch 20, train loss = 0.05, mean loss = 0.29
Mon Jan 31 08:05:52 2022
batch 30, train loss = 0.38, mean loss = 0.30
Mon Jan 31 08:06:46 2022
batch 40, train loss = 0.04, mean loss = 0.30
Mon Jan 31 08:07:41 2022
batch 50, train loss = 0.37, mean loss = 0.30
Mon Jan 31 08:08:35 2022
batch 60, train loss = 0.04, mean loss = 0.30
Mon Jan 31 08:09:30 2022
train Loss: 0.30

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 08:10:21 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 08:11:17 2022
val Loss: 0.03

epoch 62 was done for 478.154269 seconds
Epoch 63/299
----------
Mon Jan 31 08:11:55 2022
batch 0, train loss = 0.04, mean loss = 0.04
Mon Jan 31 08:12:00 2022
batch 10, train loss = 0.35, mean loss = 0.29
Mon Jan 31 08:12:54 2022
batch 20, train loss = 0.05, mean loss = 0.29
Mon Jan 31 08:13:49 2022
batch 30, train loss = 0.38, mean loss = 0.30
Mon Jan 31 08:14:43 2022
batch 40, train loss = 0.04, mean loss = 0.30
Mon Jan 31 08:15:38 2022
batch 50, train loss = 0.37, mean loss = 0.31
Mon Jan 31 08:16:32 2022
batch 60, train loss = 0.04, mean loss = 0.30
Mon Jan 31 08:17:27 2022
train Loss: 0.30

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 08:18:18 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 08:19:12 2022
val Loss: 0.03

epoch 63 was done for 475.247658 seconds
Epoch 64/299
----------
Mon Jan 31 08:19:50 2022
batch 0, train loss = 0.04, mean loss = 0.04
Mon Jan 31 08:19:55 2022
batch 10, train loss = 0.36, mean loss = 0.29
Mon Jan 31 08:20:50 2022
batch 20, train loss = 0.04, mean loss = 0.29
Mon Jan 31 08:21:45 2022
batch 30, train loss = 0.38, mean loss = 0.30
Mon Jan 31 08:22:39 2022
batch 40, train loss = 0.04, mean loss = 0.30
Mon Jan 31 08:23:34 2022
batch 50, train loss = 0.37, mean loss = 0.31
Mon Jan 31 08:24:28 2022
batch 60, train loss = 0.04, mean loss = 0.30
Mon Jan 31 08:25:23 2022
train Loss: 0.30

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 08:26:14 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 08:27:08 2022
val Loss: 0.03

epoch 64 was done for 479.989443 seconds
Epoch 65/299
----------
Mon Jan 31 08:27:50 2022
batch 0, train loss = 0.04, mean loss = 0.04
Mon Jan 31 08:27:57 2022
batch 10, train loss = 0.36, mean loss = 0.30
Mon Jan 31 08:28:51 2022
batch 20, train loss = 0.04, mean loss = 0.29
Mon Jan 31 08:29:48 2022
batch 30, train loss = 0.38, mean loss = 0.30
Mon Jan 31 08:30:42 2022
batch 40, train loss = 0.04, mean loss = 0.30
Mon Jan 31 08:31:40 2022
batch 50, train loss = 0.37, mean loss = 0.31
Mon Jan 31 08:32:37 2022
batch 60, train loss = 0.04, mean loss = 0.30
Mon Jan 31 08:33:32 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 08:34:29 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 08:35:29 2022
val Loss: 0.03

epoch 65 was done for 499.189961 seconds
Epoch 66/299
----------
Mon Jan 31 08:36:09 2022
batch 0, train loss = 0.04, mean loss = 0.04
Mon Jan 31 08:36:15 2022
batch 10, train loss = 0.37, mean loss = 0.30
Mon Jan 31 08:37:11 2022
batch 20, train loss = 0.04, mean loss = 0.30
Mon Jan 31 08:38:08 2022
batch 30, train loss = 0.39, mean loss = 0.31
Mon Jan 31 08:39:04 2022
batch 40, train loss = 0.03, mean loss = 0.30
Mon Jan 31 08:39:59 2022
batch 50, train loss = 0.38, mean loss = 0.31
Mon Jan 31 08:40:54 2022
batch 60, train loss = 0.03, mean loss = 0.30
Mon Jan 31 08:41:50 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:42:43 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 08:43:39 2022
val Loss: 0.03

epoch 66 was done for 487.833824 seconds
Epoch 67/299
----------
Mon Jan 31 08:44:17 2022
batch 0, train loss = 0.05, mean loss = 0.05
Mon Jan 31 08:44:22 2022
batch 10, train loss = 0.37, mean loss = 0.29
Mon Jan 31 08:45:17 2022
batch 20, train loss = 0.05, mean loss = 0.30
Mon Jan 31 08:46:13 2022
batch 30, train loss = 0.38, mean loss = 0.30
Mon Jan 31 08:47:08 2022
batch 40, train loss = 0.04, mean loss = 0.30
Mon Jan 31 08:48:03 2022
batch 50, train loss = 0.37, mean loss = 0.31
Mon Jan 31 08:48:59 2022
batch 60, train loss = 0.04, mean loss = 0.30
Mon Jan 31 08:49:55 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:50:50 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 08:51:46 2022
val Loss: 0.03

epoch 67 was done for 487.131963 seconds
Epoch 68/299
----------
Mon Jan 31 08:52:24 2022
batch 0, train loss = 0.04, mean loss = 0.04
Mon Jan 31 08:52:29 2022
batch 10, train loss = 0.36, mean loss = 0.29
Mon Jan 31 08:53:25 2022
batch 20, train loss = 0.04, mean loss = 0.29
Mon Jan 31 08:54:20 2022
batch 30, train loss = 0.38, mean loss = 0.30
Mon Jan 31 08:55:15 2022
batch 40, train loss = 0.03, mean loss = 0.30
Mon Jan 31 08:56:09 2022
batch 50, train loss = 0.37, mean loss = 0.30
Mon Jan 31 08:57:15 2022
batch 60, train loss = 0.03, mean loss = 0.30
Mon Jan 31 08:58:22 2022
train Loss: 0.30

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 08:59:25 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 09:00:34 2022
val Loss: 0.03

epoch 68 was done for 536.436471 seconds
Epoch 69/299
----------
Mon Jan 31 09:01:20 2022
batch 0, train loss = 0.03, mean loss = 0.03
Mon Jan 31 09:01:27 2022
batch 10, train loss = 0.36, mean loss = 0.29
Mon Jan 31 09:02:35 2022
batch 20, train loss = 0.03, mean loss = 0.29
Mon Jan 31 09:03:43 2022
batch 30, train loss = 0.38, mean loss = 0.30
Mon Jan 31 09:04:51 2022
batch 40, train loss = 0.03, mean loss = 0.30
Mon Jan 31 09:05:59 2022
batch 50, train loss = 0.37, mean loss = 0.30
Mon Jan 31 09:07:07 2022
batch 60, train loss = 0.03, mean loss = 0.30
Mon Jan 31 09:08:21 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:09:25 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 09:10:34 2022
val Loss: 0.03

epoch 69 was done for 600.100107 seconds
Epoch 70/299
----------
Mon Jan 31 09:11:21 2022
batch 0, train loss = 0.03, mean loss = 0.03
Mon Jan 31 09:11:27 2022
batch 10, train loss = 0.36, mean loss = 0.29
Mon Jan 31 09:12:35 2022
batch 20, train loss = 0.03, mean loss = 0.29
Mon Jan 31 09:13:43 2022
batch 30, train loss = 0.38, mean loss = 0.30
Mon Jan 31 09:14:52 2022
batch 40, train loss = 0.03, mean loss = 0.30
Mon Jan 31 09:16:00 2022
batch 50, train loss = 0.37, mean loss = 0.30
Mon Jan 31 09:17:08 2022
batch 60, train loss = 0.03, mean loss = 0.30
Mon Jan 31 09:18:16 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:19:20 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 09:20:29 2022
val Loss: 0.03

epoch 70 was done for 595.553750 seconds
Epoch 71/299
----------
Mon Jan 31 09:21:16 2022
batch 0, train loss = 0.03, mean loss = 0.03
Mon Jan 31 09:21:23 2022
batch 10, train loss = 0.36, mean loss = 0.29
Mon Jan 31 09:22:31 2022
batch 20, train loss = 0.03, mean loss = 0.29
Mon Jan 31 09:23:38 2022
batch 30, train loss = 0.38, mean loss = 0.30
Mon Jan 31 09:24:46 2022
batch 40, train loss = 0.03, mean loss = 0.30
Mon Jan 31 09:25:56 2022
batch 50, train loss = 0.38, mean loss = 0.30
Mon Jan 31 09:27:04 2022
batch 60, train loss = 0.03, mean loss = 0.30
Mon Jan 31 09:28:13 2022
train Loss: 0.30

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:29:17 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 09:30:25 2022
val Loss: 0.03

epoch 71 was done for 595.921800 seconds
Epoch 72/299
----------
Mon Jan 31 09:31:12 2022
batch 0, train loss = 0.03, mean loss = 0.03
Mon Jan 31 09:31:19 2022
batch 10, train loss = 0.36, mean loss = 0.29
Mon Jan 31 09:32:26 2022
batch 20, train loss = 0.03, mean loss = 0.29
Mon Jan 31 09:33:35 2022
batch 30, train loss = 0.38, mean loss = 0.30
Mon Jan 31 09:34:43 2022
batch 40, train loss = 0.03, mean loss = 0.30
Mon Jan 31 09:35:51 2022
batch 50, train loss = 0.38, mean loss = 0.30
Mon Jan 31 09:37:00 2022
batch 60, train loss = 0.02, mean loss = 0.30
Mon Jan 31 09:38:09 2022
train Loss: 0.30

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 09:39:12 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 09:40:21 2022
val Loss: 0.03

epoch 72 was done for 595.724558 seconds
Epoch 73/299
----------
Mon Jan 31 09:41:08 2022
batch 0, train loss = 0.03, mean loss = 0.03
Mon Jan 31 09:41:14 2022
batch 10, train loss = 0.36, mean loss = 0.29
Mon Jan 31 09:42:23 2022
batch 20, train loss = 0.03, mean loss = 0.29
Mon Jan 31 09:43:31 2022
batch 30, train loss = 0.38, mean loss = 0.30
Mon Jan 31 09:44:39 2022
batch 40, train loss = 0.03, mean loss = 0.30
Mon Jan 31 09:45:48 2022
batch 50, train loss = 0.38, mean loss = 0.30
Mon Jan 31 09:46:58 2022
batch 60, train loss = 0.02, mean loss = 0.30
Mon Jan 31 09:48:07 2022
train Loss: 0.30

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 09:49:11 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 09:50:23 2022
val Loss: 0.03

epoch 73 was done for 601.952669 seconds
Epoch 74/299
----------
Mon Jan 31 09:51:10 2022
batch 0, train loss = 0.03, mean loss = 0.03
Mon Jan 31 09:51:16 2022
batch 10, train loss = 0.36, mean loss = 0.29
Mon Jan 31 09:52:25 2022
batch 20, train loss = 0.03, mean loss = 0.29
Mon Jan 31 09:53:33 2022
batch 30, train loss = 0.38, mean loss = 0.30
Mon Jan 31 09:54:41 2022
batch 40, train loss = 0.03, mean loss = 0.30
Mon Jan 31 09:55:49 2022
batch 50, train loss = 0.38, mean loss = 0.31
Mon Jan 31 09:56:57 2022
batch 60, train loss = 0.02, mean loss = 0.30
Mon Jan 31 09:58:05 2022
train Loss: 0.30

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 09:59:10 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 10:00:20 2022
val Loss: 0.03

epoch 74 was done for 597.974438 seconds
Epoch 75/299
----------
Mon Jan 31 10:01:08 2022
batch 0, train loss = 0.03, mean loss = 0.03
Mon Jan 31 10:01:14 2022
batch 10, train loss = 0.36, mean loss = 0.29
Mon Jan 31 10:02:23 2022
batch 20, train loss = 0.03, mean loss = 0.30
Mon Jan 31 10:03:31 2022
batch 30, train loss = 0.39, mean loss = 0.30
Mon Jan 31 10:04:39 2022
batch 40, train loss = 0.03, mean loss = 0.30
Mon Jan 31 10:05:47 2022
batch 50, train loss = 0.38, mean loss = 0.31
Mon Jan 31 10:06:56 2022
batch 60, train loss = 0.02, mean loss = 0.30
Mon Jan 31 10:08:04 2022
train Loss: 0.30

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 10:09:08 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 10:10:16 2022
val Loss: 0.03

epoch 75 was done for 595.413135 seconds
Epoch 76/299
----------
Mon Jan 31 10:11:03 2022
batch 0, train loss = 0.02, mean loss = 0.02
Mon Jan 31 10:11:10 2022
batch 10, train loss = 0.37, mean loss = 0.29
Mon Jan 31 10:12:18 2022
batch 20, train loss = 0.03, mean loss = 0.29
Mon Jan 31 10:13:27 2022
batch 30, train loss = 0.39, mean loss = 0.30
Mon Jan 31 10:14:36 2022
batch 40, train loss = 0.02, mean loss = 0.30
Mon Jan 31 10:15:44 2022
batch 50, train loss = 0.38, mean loss = 0.31
Mon Jan 31 10:16:53 2022
batch 60, train loss = 0.02, mean loss = 0.30
Mon Jan 31 10:18:02 2022
train Loss: 0.30

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 10:19:06 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 10:20:15 2022
val Loss: 0.03

epoch 76 was done for 598.897308 seconds
Epoch 77/299
----------
Mon Jan 31 10:21:02 2022
batch 0, train loss = 0.02, mean loss = 0.02
Mon Jan 31 10:21:09 2022
batch 10, train loss = 0.37, mean loss = 0.29
Mon Jan 31 10:22:18 2022
batch 20, train loss = 0.03, mean loss = 0.29
Mon Jan 31 10:23:28 2022
batch 30, train loss = 0.39, mean loss = 0.30
Mon Jan 31 10:24:37 2022
batch 40, train loss = 0.02, mean loss = 0.30
Mon Jan 31 10:25:44 2022
batch 50, train loss = 0.38, mean loss = 0.31
Mon Jan 31 10:26:52 2022
batch 60, train loss = 0.02, mean loss = 0.30
Mon Jan 31 10:28:01 2022
train Loss: 0.30

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 10:29:05 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 10:30:14 2022
val Loss: 0.03

epoch 77 was done for 599.213721 seconds
Epoch 78/299
----------
Mon Jan 31 10:31:01 2022
batch 0, train loss = 0.02, mean loss = 0.02
Mon Jan 31 10:31:08 2022
batch 10, train loss = 0.37, mean loss = 0.29
Mon Jan 31 10:32:16 2022
batch 20, train loss = 0.03, mean loss = 0.29
Mon Jan 31 10:33:26 2022
batch 30, train loss = 0.39, mean loss = 0.30
Mon Jan 31 10:34:35 2022
batch 40, train loss = 0.02, mean loss = 0.30
Mon Jan 31 10:35:43 2022
batch 50, train loss = 0.38, mean loss = 0.31
Mon Jan 31 10:36:52 2022
batch 60, train loss = 0.02, mean loss = 0.30
Mon Jan 31 10:38:01 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 10:39:05 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 10:40:15 2022
val Loss: 0.03

epoch 78 was done for 600.281557 seconds
Epoch 79/299
----------
Mon Jan 31 10:41:01 2022
batch 0, train loss = 0.02, mean loss = 0.02
Mon Jan 31 10:41:08 2022
batch 10, train loss = 0.37, mean loss = 0.29
Mon Jan 31 10:42:17 2022
batch 20, train loss = 0.03, mean loss = 0.30
Mon Jan 31 10:43:25 2022
batch 30, train loss = 0.39, mean loss = 0.31
Mon Jan 31 10:44:34 2022
batch 40, train loss = 0.02, mean loss = 0.30
Mon Jan 31 10:45:43 2022
batch 50, train loss = 0.38, mean loss = 0.31
Mon Jan 31 10:46:52 2022
batch 60, train loss = 0.02, mean loss = 0.30
Mon Jan 31 10:48:02 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:49:07 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 10:50:17 2022
val Loss: 0.03

epoch 79 was done for 603.362936 seconds
Epoch 80/299
----------
Mon Jan 31 10:51:05 2022
batch 0, train loss = 0.02, mean loss = 0.02
Mon Jan 31 10:51:11 2022
batch 10, train loss = 0.38, mean loss = 0.30
Mon Jan 31 10:52:20 2022
batch 20, train loss = 0.02, mean loss = 0.30
Mon Jan 31 10:53:30 2022
batch 30, train loss = 0.40, mean loss = 0.31
Mon Jan 31 10:54:39 2022
batch 40, train loss = 0.02, mean loss = 0.31
Mon Jan 31 10:55:48 2022
batch 50, train loss = 0.40, mean loss = 0.31
Mon Jan 31 10:56:58 2022
batch 60, train loss = 0.02, mean loss = 0.31
Mon Jan 31 10:58:08 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 10:59:13 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 11:00:22 2022
val Loss: 0.03

epoch 80 was done for 607.230634 seconds
Epoch 81/299
----------
Mon Jan 31 11:01:12 2022
batch 0, train loss = 0.03, mean loss = 0.03
Mon Jan 31 11:01:18 2022
batch 10, train loss = 0.39, mean loss = 0.30
Mon Jan 31 11:02:29 2022
batch 20, train loss = 0.03, mean loss = 0.30
Mon Jan 31 11:03:38 2022
batch 30, train loss = 0.42, mean loss = 0.31
Mon Jan 31 11:04:47 2022
batch 40, train loss = 0.03, mean loss = 0.31
Mon Jan 31 11:05:57 2022
batch 50, train loss = 0.41, mean loss = 0.32
Mon Jan 31 11:07:06 2022
batch 60, train loss = 0.03, mean loss = 0.31
Mon Jan 31 11:08:15 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 11:09:21 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 11:10:30 2022
val Loss: 0.03

epoch 81 was done for 604.763651 seconds
Epoch 82/299
----------
Mon Jan 31 11:11:17 2022
batch 0, train loss = 0.04, mean loss = 0.04
Mon Jan 31 11:11:23 2022
batch 10, train loss = 0.37, mean loss = 0.30
Mon Jan 31 11:12:31 2022
batch 20, train loss = 0.03, mean loss = 0.30
Mon Jan 31 11:13:39 2022
batch 30, train loss = 0.40, mean loss = 0.31
Mon Jan 31 11:14:50 2022
batch 40, train loss = 0.03, mean loss = 0.31
Mon Jan 31 11:15:58 2022
batch 50, train loss = 0.39, mean loss = 0.31
Mon Jan 31 11:17:07 2022
batch 60, train loss = 0.02, mean loss = 0.31
Mon Jan 31 11:18:17 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 11:19:22 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 11:20:32 2022
val Loss: 0.03

epoch 82 was done for 601.365263 seconds
Epoch 83/299
----------
Mon Jan 31 11:21:18 2022
batch 0, train loss = 0.02, mean loss = 0.02
Mon Jan 31 11:21:25 2022
batch 10, train loss = 0.37, mean loss = 0.30
Mon Jan 31 11:22:33 2022
batch 20, train loss = 0.02, mean loss = 0.29
Mon Jan 31 11:23:42 2022
batch 30, train loss = 0.39, mean loss = 0.30
Mon Jan 31 11:24:50 2022
batch 40, train loss = 0.02, mean loss = 0.30
Mon Jan 31 11:25:59 2022
batch 50, train loss = 0.39, mean loss = 0.31
Mon Jan 31 11:27:07 2022
batch 60, train loss = 0.02, mean loss = 0.30
Mon Jan 31 11:28:17 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 11:29:21 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 11:30:30 2022
val Loss: 0.03

epoch 83 was done for 597.680602 seconds
Epoch 84/299
----------
Mon Jan 31 11:31:16 2022
batch 0, train loss = 0.02, mean loss = 0.02
Mon Jan 31 11:31:23 2022
batch 10, train loss = 0.37, mean loss = 0.30
Mon Jan 31 11:32:32 2022
batch 20, train loss = 0.02, mean loss = 0.29
Mon Jan 31 11:33:41 2022
batch 30, train loss = 0.39, mean loss = 0.30
Mon Jan 31 11:34:50 2022
batch 40, train loss = 0.02, mean loss = 0.30
Mon Jan 31 11:35:59 2022
batch 50, train loss = 0.38, mean loss = 0.31
Mon Jan 31 11:37:07 2022
batch 60, train loss = 0.02, mean loss = 0.30
Mon Jan 31 11:38:16 2022
train Loss: 0.30

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 11:39:20 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 11:40:30 2022
val Loss: 0.03

epoch 84 was done for 600.231778 seconds
Epoch 85/299
----------
Mon Jan 31 11:41:16 2022
batch 0, train loss = 0.02, mean loss = 0.02
Mon Jan 31 11:41:23 2022
batch 10, train loss = 0.37, mean loss = 0.29
Mon Jan 31 11:42:35 2022
batch 20, train loss = 0.02, mean loss = 0.29
Mon Jan 31 11:43:43 2022
batch 30, train loss = 0.39, mean loss = 0.30
Mon Jan 31 11:44:52 2022
batch 40, train loss = 0.02, mean loss = 0.30
Mon Jan 31 11:46:00 2022
batch 50, train loss = 0.38, mean loss = 0.31
Mon Jan 31 11:47:10 2022
batch 60, train loss = 0.02, mean loss = 0.30
Mon Jan 31 11:48:20 2022
train Loss: 0.30

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 11:49:24 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 11:50:33 2022
val Loss: 0.03

epoch 85 was done for 603.634601 seconds
Epoch 86/299
----------
Mon Jan 31 11:51:20 2022
batch 0, train loss = 0.02, mean loss = 0.02
Mon Jan 31 11:51:27 2022
batch 10, train loss = 0.37, mean loss = 0.29
Mon Jan 31 11:52:36 2022
batch 20, train loss = 0.02, mean loss = 0.29
Mon Jan 31 11:53:46 2022
batch 30, train loss = 0.39, mean loss = 0.30
Mon Jan 31 11:54:57 2022
batch 40, train loss = 0.02, mean loss = 0.30
Mon Jan 31 11:56:07 2022
batch 50, train loss = 0.39, mean loss = 0.31
Mon Jan 31 11:57:17 2022
batch 60, train loss = 0.02, mean loss = 0.30
Mon Jan 31 11:58:27 2022
train Loss: 0.30

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 11:59:32 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 12:00:41 2022
val Loss: 0.03

epoch 86 was done for 608.736679 seconds
Epoch 87/299
----------
Mon Jan 31 12:01:28 2022
batch 0, train loss = 0.02, mean loss = 0.02
Mon Jan 31 12:01:35 2022
batch 10, train loss = 0.37, mean loss = 0.29
Mon Jan 31 12:02:45 2022
batch 20, train loss = 0.02, mean loss = 0.29
Mon Jan 31 12:03:54 2022
batch 30, train loss = 0.39, mean loss = 0.30
Mon Jan 31 12:05:14 2022
batch 40, train loss = 0.02, mean loss = 0.30
Mon Jan 31 12:06:25 2022
batch 50, train loss = 0.39, mean loss = 0.31
Mon Jan 31 12:07:34 2022
batch 60, train loss = 0.01, mean loss = 0.30
Mon Jan 31 12:08:43 2022
train Loss: 0.30

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 12:09:48 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 12:10:58 2022
val Loss: 0.03

epoch 87 was done for 615.585242 seconds
Epoch 88/299
----------
Mon Jan 31 12:11:44 2022
batch 0, train loss = 0.02, mean loss = 0.02
Mon Jan 31 12:11:51 2022
batch 10, train loss = 0.37, mean loss = 0.29
Mon Jan 31 12:12:59 2022
batch 20, train loss = 0.02, mean loss = 0.29
Mon Jan 31 12:14:08 2022
batch 30, train loss = 0.39, mean loss = 0.30
Mon Jan 31 12:15:16 2022
batch 40, train loss = 0.02, mean loss = 0.30
Mon Jan 31 12:16:26 2022
batch 50, train loss = 0.39, mean loss = 0.31
Mon Jan 31 12:17:35 2022
batch 60, train loss = 0.01, mean loss = 0.30
Mon Jan 31 12:18:45 2022
train Loss: 0.30

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 12:19:49 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 12:20:59 2022
val Loss: 0.03

epoch 88 was done for 601.052844 seconds
Epoch 89/299
----------
Mon Jan 31 12:21:45 2022
batch 0, train loss = 0.01, mean loss = 0.01
Mon Jan 31 12:21:52 2022
batch 10, train loss = 0.37, mean loss = 0.29
Mon Jan 31 12:23:01 2022
batch 20, train loss = 0.02, mean loss = 0.29
Mon Jan 31 12:24:10 2022
batch 30, train loss = 0.39, mean loss = 0.30
Mon Jan 31 12:25:20 2022
batch 40, train loss = 0.02, mean loss = 0.30
Mon Jan 31 12:26:28 2022
batch 50, train loss = 0.39, mean loss = 0.31
Mon Jan 31 12:27:37 2022
batch 60, train loss = 0.01, mean loss = 0.30
Mon Jan 31 12:28:46 2022
train Loss: 0.30

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 12:29:50 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 12:30:59 2022
val Loss: 0.03

epoch 89 was done for 603.076465 seconds
Epoch 90/299
----------
Mon Jan 31 12:31:48 2022
batch 0, train loss = 0.01, mean loss = 0.01
Mon Jan 31 12:31:55 2022
batch 10, train loss = 0.37, mean loss = 0.29
Mon Jan 31 12:33:03 2022
batch 20, train loss = 0.02, mean loss = 0.29
Mon Jan 31 12:34:12 2022
batch 30, train loss = 0.39, mean loss = 0.30
Mon Jan 31 12:35:20 2022
batch 40, train loss = 0.01, mean loss = 0.30
Mon Jan 31 12:36:29 2022
batch 50, train loss = 0.39, mean loss = 0.31
Mon Jan 31 12:37:37 2022
batch 60, train loss = 0.01, mean loss = 0.30
Mon Jan 31 12:38:47 2022
train Loss: 0.30

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 12:39:57 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 12:41:06 2022
val Loss: 0.03

epoch 90 was done for 604.170635 seconds
Epoch 91/299
----------
Mon Jan 31 12:41:52 2022
batch 0, train loss = 0.01, mean loss = 0.01
Mon Jan 31 12:41:59 2022
batch 10, train loss = 0.37, mean loss = 0.29
Mon Jan 31 12:43:07 2022
batch 20, train loss = 0.01, mean loss = 0.29
Mon Jan 31 12:44:15 2022
batch 30, train loss = 0.39, mean loss = 0.30
Mon Jan 31 12:45:25 2022
batch 40, train loss = 0.01, mean loss = 0.30
Mon Jan 31 12:46:33 2022
batch 50, train loss = 0.39, mean loss = 0.31
Mon Jan 31 12:47:42 2022
batch 60, train loss = 0.01, mean loss = 0.30
Mon Jan 31 12:48:51 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 12:49:57 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 12:51:05 2022
val Loss: 0.03

epoch 91 was done for 599.170244 seconds
Epoch 92/299
----------
Mon Jan 31 12:51:52 2022
batch 0, train loss = 0.01, mean loss = 0.01
Mon Jan 31 12:51:58 2022
batch 10, train loss = 0.37, mean loss = 0.29
Mon Jan 31 12:53:09 2022
batch 20, train loss = 0.01, mean loss = 0.29
Mon Jan 31 12:54:19 2022
batch 30, train loss = 0.39, mean loss = 0.30
Mon Jan 31 12:55:27 2022
batch 40, train loss = 0.01, mean loss = 0.30
Mon Jan 31 12:56:36 2022
batch 50, train loss = 0.39, mean loss = 0.31
Mon Jan 31 12:57:45 2022
batch 60, train loss = 0.01, mean loss = 0.30
Mon Jan 31 12:58:53 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 12:59:57 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 13:01:07 2022
val Loss: 0.03

epoch 92 was done for 602.729967 seconds
Epoch 93/299
----------
Mon Jan 31 13:01:54 2022
batch 0, train loss = 0.01, mean loss = 0.01
Mon Jan 31 13:02:01 2022
batch 10, train loss = 0.37, mean loss = 0.30
Mon Jan 31 13:03:11 2022
batch 20, train loss = 0.01, mean loss = 0.29
Mon Jan 31 13:04:20 2022
batch 30, train loss = 0.40, mean loss = 0.30
Mon Jan 31 13:05:30 2022
batch 40, train loss = 0.01, mean loss = 0.30
Mon Jan 31 13:06:39 2022
batch 50, train loss = 0.39, mean loss = 0.31
Mon Jan 31 13:07:48 2022
batch 60, train loss = 0.01, mean loss = 0.30
Mon Jan 31 13:08:57 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 13:10:02 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 13:11:10 2022
val Loss: 0.03

epoch 93 was done for 602.746421 seconds
Epoch 94/299
----------
Mon Jan 31 13:11:57 2022
batch 0, train loss = 0.01, mean loss = 0.01
Mon Jan 31 13:12:03 2022
batch 10, train loss = 0.38, mean loss = 0.30
Mon Jan 31 13:13:12 2022
batch 20, train loss = 0.01, mean loss = 0.29
Mon Jan 31 13:14:20 2022
batch 30, train loss = 0.40, mean loss = 0.30
Mon Jan 31 13:15:31 2022
batch 40, train loss = 0.01, mean loss = 0.30
Mon Jan 31 13:16:47 2022
batch 50, train loss = 0.39, mean loss = 0.31
Mon Jan 31 13:17:56 2022
batch 60, train loss = 0.01, mean loss = 0.30
Mon Jan 31 13:19:06 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 13:20:10 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 13:21:17 2022
val Loss: 0.03

epoch 94 was done for 606.309604 seconds
Epoch 95/299
----------
Mon Jan 31 13:22:03 2022
batch 0, train loss = 0.01, mean loss = 0.01
Mon Jan 31 13:22:10 2022
batch 10, train loss = 0.38, mean loss = 0.30
Mon Jan 31 13:23:20 2022
batch 20, train loss = 0.01, mean loss = 0.30
Mon Jan 31 13:24:32 2022
batch 30, train loss = 0.40, mean loss = 0.30
Mon Jan 31 13:25:41 2022
batch 40, train loss = 0.01, mean loss = 0.30
Mon Jan 31 13:26:51 2022
batch 50, train loss = 0.39, mean loss = 0.31
Mon Jan 31 13:28:00 2022
batch 60, train loss = 0.01, mean loss = 0.30
Mon Jan 31 13:29:15 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 13:30:20 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 13:31:30 2022
val Loss: 0.03

epoch 95 was done for 615.474589 seconds
Epoch 96/299
----------
Mon Jan 31 13:32:19 2022
batch 0, train loss = 0.01, mean loss = 0.01
Mon Jan 31 13:32:25 2022
batch 10, train loss = 0.38, mean loss = 0.30
Mon Jan 31 13:33:36 2022
batch 20, train loss = 0.01, mean loss = 0.30
Mon Jan 31 13:34:45 2022
batch 30, train loss = 0.40, mean loss = 0.31
Mon Jan 31 13:35:53 2022
batch 40, train loss = 0.01, mean loss = 0.30
Mon Jan 31 13:37:02 2022
batch 50, train loss = 0.39, mean loss = 0.31
Mon Jan 31 13:38:12 2022
batch 60, train loss = 0.01, mean loss = 0.30
Mon Jan 31 13:39:21 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 13:40:26 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 13:41:35 2022
val Loss: 0.03

epoch 96 was done for 604.895751 seconds
Epoch 97/299
----------
Mon Jan 31 13:42:24 2022
batch 0, train loss = 0.01, mean loss = 0.01
Mon Jan 31 13:42:30 2022
batch 10, train loss = 0.38, mean loss = 0.30
Mon Jan 31 13:43:38 2022
batch 20, train loss = 0.01, mean loss = 0.30
Mon Jan 31 13:44:46 2022
batch 30, train loss = 0.40, mean loss = 0.31
Mon Jan 31 13:45:54 2022
batch 40, train loss = 0.01, mean loss = 0.30
Mon Jan 31 13:47:03 2022
batch 50, train loss = 0.39, mean loss = 0.31
Mon Jan 31 13:48:11 2022
batch 60, train loss = 0.01, mean loss = 0.30
Mon Jan 31 13:49:20 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 13:50:24 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 13:51:32 2022
val Loss: 0.03

epoch 97 was done for 595.188320 seconds
Epoch 98/299
----------
Mon Jan 31 13:52:19 2022
batch 0, train loss = 0.01, mean loss = 0.01
Mon Jan 31 13:52:26 2022
batch 10, train loss = 0.38, mean loss = 0.30
Mon Jan 31 13:53:34 2022
batch 20, train loss = 0.01, mean loss = 0.30
Mon Jan 31 13:54:45 2022
batch 30, train loss = 0.40, mean loss = 0.31
Mon Jan 31 13:55:53 2022
batch 40, train loss = 0.01, mean loss = 0.30
Mon Jan 31 13:57:02 2022
batch 50, train loss = 0.40, mean loss = 0.31
Mon Jan 31 13:58:10 2022
batch 60, train loss = 0.01, mean loss = 0.30
Mon Jan 31 13:59:19 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 14:00:23 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 14:01:33 2022
val Loss: 0.03

epoch 98 was done for 600.043890 seconds
Epoch 99/299
----------
Mon Jan 31 14:02:19 2022
batch 0, train loss = 0.01, mean loss = 0.01
Mon Jan 31 14:02:26 2022
batch 10, train loss = 0.38, mean loss = 0.30
Mon Jan 31 14:03:34 2022
batch 20, train loss = 0.01, mean loss = 0.30
Mon Jan 31 14:04:43 2022
batch 30, train loss = 0.40, mean loss = 0.31
Mon Jan 31 14:05:54 2022
batch 40, train loss = 0.01, mean loss = 0.30
Mon Jan 31 14:07:07 2022
batch 50, train loss = 0.40, mean loss = 0.31
Mon Jan 31 14:08:18 2022
batch 60, train loss = 0.01, mean loss = 0.30
Mon Jan 31 14:09:29 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 14:10:33 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 14:11:42 2022
val Loss: 0.03

epoch 99 was done for 613.212487 seconds
Epoch 100/299
----------
Mon Jan 31 14:12:32 2022
batch 0, train loss = 0.01, mean loss = 0.01
Mon Jan 31 14:12:38 2022
batch 10, train loss = 0.38, mean loss = 0.30
Mon Jan 31 14:13:46 2022
batch 20, train loss = 0.01, mean loss = 0.30
Mon Jan 31 14:14:55 2022
batch 30, train loss = 0.40, mean loss = 0.31
Mon Jan 31 14:16:06 2022
batch 40, train loss = 0.01, mean loss = 0.30
Mon Jan 31 14:17:15 2022
batch 50, train loss = 0.40, mean loss = 0.31
Mon Jan 31 14:18:25 2022
batch 60, train loss = 0.01, mean loss = 0.30
Mon Jan 31 14:19:34 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 14:20:39 2022
batch 10, val loss = 0.02, mean loss = 0.03
Mon Jan 31 14:21:49 2022
val Loss: 0.03

epoch 100 was done for 603.180982 seconds
Epoch 101/299
----------
Mon Jan 31 14:22:35 2022
batch 0, train loss = 0.01, mean loss = 0.01
Mon Jan 31 14:22:42 2022
batch 10, train loss = 0.38, mean loss = 0.30
Mon Jan 31 14:23:51 2022
batch 20, train loss = 0.01, mean loss = 0.30
Mon Jan 31 14:25:00 2022
batch 30, train loss = 0.40, mean loss = 0.31
Mon Jan 31 14:26:10 2022
batch 40, train loss = 0.01, mean loss = 0.30
Mon Jan 31 14:27:24 2022
batch 50, train loss = 0.40, mean loss = 0.31
Mon Jan 31 14:28:32 2022
batch 60, train loss = 0.01, mean loss = 0.30
Mon Jan 31 14:29:44 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 14:30:49 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 14:31:59 2022
val Loss: 0.03

epoch 101 was done for 610.780984 seconds
Epoch 102/299
----------
Mon Jan 31 14:32:46 2022
batch 0, train loss = 0.01, mean loss = 0.01
Mon Jan 31 14:32:53 2022
batch 10, train loss = 0.38, mean loss = 0.30
Mon Jan 31 14:34:04 2022
batch 20, train loss = 0.01, mean loss = 0.30
Mon Jan 31 14:35:13 2022
batch 30, train loss = 0.40, mean loss = 0.31
Mon Jan 31 14:36:22 2022
batch 40, train loss = 0.01, mean loss = 0.30
Mon Jan 31 14:37:30 2022
batch 50, train loss = 0.40, mean loss = 0.31
Mon Jan 31 14:38:42 2022
batch 60, train loss = 0.01, mean loss = 0.30
Mon Jan 31 14:39:51 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 14:40:55 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 14:42:04 2022
val Loss: 0.03

epoch 102 was done for 604.869630 seconds
Epoch 103/299
----------
Mon Jan 31 14:42:51 2022
batch 0, train loss = 0.01, mean loss = 0.01
Mon Jan 31 14:42:58 2022
batch 10, train loss = 0.38, mean loss = 0.30
Mon Jan 31 14:44:07 2022
batch 20, train loss = 0.01, mean loss = 0.30
Mon Jan 31 14:45:15 2022
batch 30, train loss = 0.40, mean loss = 0.31
Mon Jan 31 14:46:25 2022
batch 40, train loss = 0.01, mean loss = 0.30
Mon Jan 31 14:47:33 2022
batch 50, train loss = 0.40, mean loss = 0.31
Mon Jan 31 14:48:44 2022
batch 60, train loss = 0.01, mean loss = 0.31
Mon Jan 31 14:49:53 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 14:50:59 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 14:52:10 2022
val Loss: 0.03

epoch 103 was done for 606.024959 seconds
Epoch 104/299
----------
Mon Jan 31 14:52:57 2022
batch 0, train loss = 0.01, mean loss = 0.01
Mon Jan 31 14:53:04 2022
batch 10, train loss = 0.38, mean loss = 0.30
Mon Jan 31 14:54:13 2022
batch 20, train loss = 0.01, mean loss = 0.30
Mon Jan 31 14:55:22 2022
batch 30, train loss = 0.40, mean loss = 0.31
Mon Jan 31 14:56:31 2022
batch 40, train loss = 0.01, mean loss = 0.31
Mon Jan 31 14:57:40 2022
batch 50, train loss = 0.40, mean loss = 0.31
Mon Jan 31 14:58:49 2022
batch 60, train loss = 0.01, mean loss = 0.31
Mon Jan 31 15:00:01 2022
train Loss: 0.31

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 15:01:06 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 15:02:14 2022
val Loss: 0.03

epoch 104 was done for 603.800994 seconds
Epoch 105/299
----------
Mon Jan 31 15:03:01 2022
batch 0, train loss = 0.01, mean loss = 0.01
Mon Jan 31 15:03:07 2022
batch 10, train loss = 0.38, mean loss = 0.30
Mon Jan 31 15:04:15 2022
batch 20, train loss = 0.01, mean loss = 0.30
Mon Jan 31 15:05:24 2022
batch 30, train loss = 0.40, mean loss = 0.31
Mon Jan 31 15:06:32 2022
batch 40, train loss = 0.01, mean loss = 0.31
Mon Jan 31 15:07:41 2022
batch 50, train loss = 0.40, mean loss = 0.31
Mon Jan 31 15:08:49 2022
batch 60, train loss = 0.01, mean loss = 0.31
Mon Jan 31 15:09:58 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 15:11:01 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 15:12:09 2022
val Loss: 0.03

epoch 105 was done for 594.996233 seconds
Epoch 106/299
----------
Mon Jan 31 15:12:56 2022
batch 0, train loss = 0.01, mean loss = 0.01
Mon Jan 31 15:13:02 2022
batch 10, train loss = 0.38, mean loss = 0.30
Mon Jan 31 15:14:12 2022
batch 20, train loss = 0.01, mean loss = 0.30
Mon Jan 31 15:15:20 2022
batch 30, train loss = 0.40, mean loss = 0.31
Mon Jan 31 15:16:30 2022
batch 40, train loss = 0.01, mean loss = 0.31
Mon Jan 31 15:17:38 2022
batch 50, train loss = 0.40, mean loss = 0.31
Mon Jan 31 15:18:47 2022
batch 60, train loss = 0.01, mean loss = 0.31
Mon Jan 31 15:19:56 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 15:21:00 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 15:22:09 2022
val Loss: 0.03

epoch 106 was done for 599.624466 seconds
Epoch 107/299
----------
Mon Jan 31 15:22:55 2022
batch 0, train loss = 0.01, mean loss = 0.01
Mon Jan 31 15:23:02 2022
batch 10, train loss = 0.38, mean loss = 0.30
Mon Jan 31 15:24:11 2022
batch 20, train loss = 0.01, mean loss = 0.30
Mon Jan 31 15:25:20 2022
batch 30, train loss = 0.40, mean loss = 0.31
Mon Jan 31 15:26:29 2022
batch 40, train loss = 0.01, mean loss = 0.31
Mon Jan 31 15:27:38 2022
batch 50, train loss = 0.40, mean loss = 0.31
Mon Jan 31 15:28:48 2022
batch 60, train loss = 0.01, mean loss = 0.31
Mon Jan 31 15:29:57 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 15:31:01 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 15:32:09 2022
val Loss: 0.03

epoch 107 was done for 600.939268 seconds
Epoch 108/299
----------
Mon Jan 31 15:32:56 2022
batch 0, train loss = 0.01, mean loss = 0.01
Mon Jan 31 15:33:03 2022
batch 10, train loss = 0.39, mean loss = 0.30
Mon Jan 31 15:34:11 2022
batch 20, train loss = 0.01, mean loss = 0.30
Mon Jan 31 15:35:19 2022
batch 30, train loss = 0.40, mean loss = 0.31
Mon Jan 31 15:36:28 2022
batch 40, train loss = 0.01, mean loss = 0.31
Mon Jan 31 15:37:37 2022
batch 50, train loss = 0.40, mean loss = 0.31
Mon Jan 31 15:38:45 2022
batch 60, train loss = 0.01, mean loss = 0.31
Mon Jan 31 15:39:54 2022
train Loss: 0.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 15:40:59 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 15:42:07 2022
val Loss: 0.03

epoch 108 was done for 596.932959 seconds
Epoch 109/299
----------
Mon Jan 31 15:42:53 2022
batch 0, train loss = 0.01, mean loss = 0.01
Mon Jan 31 15:43:00 2022
batch 10, train loss = 0.39, mean loss = 0.30
Mon Jan 31 15:44:07 2022
batch 20, train loss = 0.01, mean loss = 0.30
Mon Jan 31 15:45:17 2022
batch 30, train loss = 0.40, mean loss = 0.31
Mon Jan 31 15:46:25 2022
batch 40, train loss = 0.01, mean loss = 0.31
Mon Jan 31 15:47:34 2022
batch 50, train loss = 0.41, mean loss = 0.31
Mon Jan 31 15:48:42 2022
