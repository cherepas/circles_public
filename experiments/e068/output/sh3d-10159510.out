The activation script must be sourced, otherwise the virtual environment will not work.
Setting vars
iscuda= True
opt.wandb =  
93
iscuda= True
iscuda= True
opt.wandb =  
file to frame csv ../../csv/598frame.csv
opt.wandb =  
93
93
file to frame csv ../../csv/598frame.csv
file to frame csv ../../csv/598frame.csv
iscuda= True
PyTorch Version:  1.8.1
Torchvision Version:  0.9.0a0
opt:
 Namespace(ampl=441, aug_gt='orient', batch_output=2, bs=15, cencrop=700, center=False, chidden_dim=[96, 128, 256, 256, 256], classicnorm=False, cmscrop=0, conTrain='', criterion='L2', csvname='598csv9', datapath='/p/project/delia-mp/cherepashkin1/phenoseed/', dfname='598frame', downsample=1, epoch=300, expdescr='', expnum='e068', feature_extract=False, framelim=6000, gradient_predivide_factor=1.0, gttype='single_file', haf=True, hidden_dim=[9], inputt='img', kernel_sizes=[7, 3, 3, 3, 3, 3], lb='orient', loadh5=False, localexp='', lr=0.001, machine='jureca', maintain=False, maintain_line=False, man_dist=False, measure_time=False, merging='batch', minmax=False, minmax3dimage=False, minmax_f=True, minmax_fn='', model_name='', netname=['cnet'], ngpu=4, noise_input=False, noise_output=False, normalize=False, num_input_images=3, num_sam_points=500, num_workers=0, outputt='orient', parallel='horovod', pin_memory=False, print_minibatch=10, pscale=100, rand_angle=False, rescale=500, rot_dirs=False, rotate_output=False, save_output=True, single_folder=False, specie='598', standardize=255, steplr=[1000.0, 1.0], transappendix='_image', ufmodel=100000, updateFraction=0.25, use_adasum=False, use_cuda=True, use_existing_csv=True, use_pretrained=False, use_sep_csv=True, view_sep=False, wandb='', weight_decay=0, zero_angle=True)
sys.argv:
 ['../../main.py', '-datapath', '/p/project/delia-mp/cherepashkin1/phenoseed/', '-epoch', '300', '-bs', '15', '-num_input_images', '3', '-framelim', '6000', '-criterion', 'L2', '-localexp', '', '-lr', '1e-3', '-expnum', 'e068', '-hidden_dim', '9', '-inputt', 'img', '-outputt', 'orient', '-lb', 'orient', '-no_loadh5', '-minmax_fn', '', '-parallel', 'horovod', '-machine', 'jureca', '-merging', 'batch', '-aug_gt', 'orient', '-updateFraction', '0.25', '-steplr', '1000', '1', '-print_minibatch', '10', '-dfname', '598frame']
seed =  0
path were main.py is located= ../../
opt.wandb =  
93
file to frame csv ../../csv/598frame.csv
lframe's length after laoding =  5283
lframe's length after laoding =  5283
lframe's length after laoding =  5283
lframe's length after laoding =  5283
lframe len after excluding all exceptions= 5200
lframe len after excluding all exceptions= 5200
lframe len after excluding all exceptions= 5200
lframe len after excluding all exceptions= 5200
len train =  4160
len train =  4160
len train =  4160
len train =  4160
train consists of 277 full batches with 15 tensors with 3 views
the last batch has size of 5 tensors with 3 views
val consists of 69 full batches with 15 tensors with 3 views
the last batch has size of 5 tensors with 3 views
Epoch 0/299
----------
Sun Jan 30 23:50:01 2022
batch 0, train loss = 0.52, mean loss = 0.52
Sun Jan 30 23:50:10 2022
batch 10, train loss = 6324.28, mean loss = 13826.60
Sun Jan 30 23:51:21 2022
batch 20, train loss = 1849.77, mean loss = 9720.80
Sun Jan 30 23:52:31 2022
batch 30, train loss = 1046.44, mean loss = 6915.47
Sun Jan 30 23:53:41 2022
batch 40, train loss = 1287.86, mean loss = 5485.36
Sun Jan 30 23:54:50 2022
batch 50, train loss = 281.12, mean loss = 4546.92
Sun Jan 30 23:55:59 2022
batch 60, train loss = 495.62, mean loss = 3882.14
Sun Jan 30 23:57:09 2022
train Loss: 3416.51

batch 0, val loss = 0.01, mean loss = 0.01
Sun Jan 30 23:58:13 2022
batch 10, val loss = 0.02, mean loss = 0.02
Sun Jan 30 23:59:23 2022
val Loss: 0.02

epoch 0 was done for 609.031868 seconds
Epoch 1/299
----------
Mon Jan 31 00:00:10 2022
batch 0, train loss = 304.38, mean loss = 304.38
Mon Jan 31 00:00:15 2022
batch 10, train loss = 204.51, mean loss = 218.00
Mon Jan 31 00:01:09 2022
batch 20, train loss = 222.49, mean loss = 214.84
Mon Jan 31 00:02:05 2022
batch 30, train loss = 174.71, mean loss = 195.73
Mon Jan 31 00:02:59 2022
batch 40, train loss = 114.43, mean loss = 185.34
Mon Jan 31 00:03:54 2022
batch 50, train loss = 156.15, mean loss = 173.42
Mon Jan 31 00:04:48 2022
batch 60, train loss = 66.58, mean loss = 158.98
Mon Jan 31 00:05:43 2022
train Loss: 150.55

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:06:34 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:07:27 2022
val Loss: 0.02

epoch 1 was done for 474.337270 seconds
Epoch 2/299
----------
Mon Jan 31 00:08:04 2022
batch 0, train loss = 48.97, mean loss = 48.97
Mon Jan 31 00:08:10 2022
batch 10, train loss = 55.62, mean loss = 60.61
Mon Jan 31 00:09:04 2022
batch 20, train loss = 64.53, mean loss = 58.66
Mon Jan 31 00:09:59 2022
batch 30, train loss = 40.90, mean loss = 53.45
Mon Jan 31 00:10:53 2022
batch 40, train loss = 28.04, mean loss = 47.76
Mon Jan 31 00:11:48 2022
batch 50, train loss = 18.23, mean loss = 43.69
Mon Jan 31 00:12:43 2022
batch 60, train loss = 23.97, mean loss = 40.69
Mon Jan 31 00:13:37 2022
train Loss: 38.27

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 00:14:29 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 00:15:23 2022
val Loss: 0.03

epoch 2 was done for 476.653735 seconds
Epoch 3/299
----------
Mon Jan 31 00:16:01 2022
batch 0, train loss = 17.33, mean loss = 17.33
Mon Jan 31 00:16:06 2022
batch 10, train loss = 13.74, mean loss = 14.68
Mon Jan 31 00:17:01 2022
batch 20, train loss = 15.42, mean loss = 14.29
Mon Jan 31 00:17:56 2022
batch 30, train loss = 15.12, mean loss = 14.69
Mon Jan 31 00:18:50 2022
batch 40, train loss = 10.87, mean loss = 14.18
Mon Jan 31 00:19:45 2022
batch 50, train loss = 8.24, mean loss = 13.71
Mon Jan 31 00:20:39 2022
batch 60, train loss = 7.95, mean loss = 13.10
Mon Jan 31 00:21:34 2022
train Loss: 12.69

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 00:22:25 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 00:23:20 2022
val Loss: 0.03

epoch 3 was done for 476.005816 seconds
Epoch 4/299
----------
Mon Jan 31 00:23:57 2022
batch 0, train loss = 6.47, mean loss = 6.47
Mon Jan 31 00:24:02 2022
batch 10, train loss = 7.43, mean loss = 7.32
Mon Jan 31 00:24:57 2022
batch 20, train loss = 7.78, mean loss = 7.21
Mon Jan 31 00:25:52 2022
batch 30, train loss = 10.71, mean loss = 7.76
Mon Jan 31 00:26:46 2022
batch 40, train loss = 6.37, mean loss = 7.75
Mon Jan 31 00:27:41 2022
batch 50, train loss = 5.40, mean loss = 7.93
Mon Jan 31 00:28:35 2022
batch 60, train loss = 5.35, mean loss = 7.77
Mon Jan 31 00:29:30 2022
train Loss: 7.60

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 00:30:21 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 00:31:16 2022
val Loss: 0.03

epoch 4 was done for 476.295380 seconds
Epoch 5/299
----------
Mon Jan 31 00:31:53 2022
batch 0, train loss = 4.49, mean loss = 4.49
Mon Jan 31 00:31:59 2022
batch 10, train loss = 4.11, mean loss = 5.27
Mon Jan 31 00:32:53 2022
batch 20, train loss = 4.87, mean loss = 5.27
Mon Jan 31 00:33:48 2022
batch 30, train loss = 8.06, mean loss = 5.76
Mon Jan 31 00:34:43 2022
batch 40, train loss = 4.67, mean loss = 5.83
Mon Jan 31 00:35:38 2022
batch 50, train loss = 4.27, mean loss = 5.94
Mon Jan 31 00:36:32 2022
batch 60, train loss = 4.42, mean loss = 5.85
Mon Jan 31 00:37:27 2022
train Loss: 5.75

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 00:38:18 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 00:39:13 2022
val Loss: 0.03

epoch 5 was done for 477.749795 seconds
Epoch 6/299
----------
Mon Jan 31 00:39:51 2022
batch 0, train loss = 4.13, mean loss = 4.13
Mon Jan 31 00:39:56 2022
batch 10, train loss = 3.03, mean loss = 4.20
Mon Jan 31 00:40:51 2022
batch 20, train loss = 3.87, mean loss = 4.26
Mon Jan 31 00:41:46 2022
batch 30, train loss = 6.42, mean loss = 4.65
Mon Jan 31 00:42:41 2022
batch 40, train loss = 3.34, mean loss = 4.61
Mon Jan 31 00:43:36 2022
batch 50, train loss = 3.05, mean loss = 4.69
Mon Jan 31 00:44:30 2022
batch 60, train loss = 3.47, mean loss = 4.65
Mon Jan 31 00:45:26 2022
train Loss: 4.58

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 00:46:17 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 00:47:11 2022
val Loss: 0.03

epoch 6 was done for 477.664282 seconds
Epoch 7/299
----------
Mon Jan 31 00:47:49 2022
batch 0, train loss = 3.10, mean loss = 3.10
Mon Jan 31 00:47:54 2022
batch 10, train loss = 2.33, mean loss = 3.32
Mon Jan 31 00:48:49 2022
batch 20, train loss = 3.12, mean loss = 3.53
Mon Jan 31 00:49:44 2022
batch 30, train loss = 5.55, mean loss = 3.82
Mon Jan 31 00:50:39 2022
batch 40, train loss = 3.03, mean loss = 3.78
Mon Jan 31 00:51:34 2022
batch 50, train loss = 2.59, mean loss = 3.95
Mon Jan 31 00:52:29 2022
batch 60, train loss = 2.93, mean loss = 3.93
Mon Jan 31 00:53:24 2022
train Loss: 3.91

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 00:54:15 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 00:55:10 2022
val Loss: 0.03

epoch 7 was done for 479.016694 seconds
Epoch 8/299
----------
Mon Jan 31 00:55:48 2022
batch 0, train loss = 2.78, mean loss = 2.78
Mon Jan 31 00:55:53 2022
batch 10, train loss = 2.08, mean loss = 3.01
Mon Jan 31 00:56:48 2022
batch 20, train loss = 2.80, mean loss = 3.22
Mon Jan 31 00:57:43 2022
batch 30, train loss = 4.78, mean loss = 3.45
Mon Jan 31 00:58:38 2022
batch 40, train loss = 2.81, mean loss = 3.39
Mon Jan 31 00:59:33 2022
batch 50, train loss = 2.34, mean loss = 3.55
Mon Jan 31 01:00:28 2022
batch 60, train loss = 2.58, mean loss = 3.52
Mon Jan 31 01:01:23 2022
train Loss: 3.50

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 01:02:15 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 01:03:10 2022
val Loss: 0.03

epoch 8 was done for 479.761246 seconds
Epoch 9/299
----------
Mon Jan 31 01:03:47 2022
batch 0, train loss = 2.57, mean loss = 2.57
Mon Jan 31 01:03:53 2022
batch 10, train loss = 2.05, mean loss = 2.70
Mon Jan 31 01:04:47 2022
batch 20, train loss = 2.37, mean loss = 2.91
Mon Jan 31 01:05:42 2022
batch 30, train loss = 4.04, mean loss = 3.10
Mon Jan 31 01:06:37 2022
batch 40, train loss = 2.51, mean loss = 3.03
Mon Jan 31 01:07:32 2022
batch 50, train loss = 2.18, mean loss = 3.20
Mon Jan 31 01:08:27 2022
batch 60, train loss = 2.46, mean loss = 3.17
Mon Jan 31 01:09:22 2022
train Loss: 3.16

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 01:10:13 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 01:11:08 2022
val Loss: 0.03

epoch 9 was done for 478.003008 seconds
Epoch 10/299
----------
Mon Jan 31 01:11:45 2022
batch 0, train loss = 2.40, mean loss = 2.40
Mon Jan 31 01:11:51 2022
batch 10, train loss = 1.93, mean loss = 2.42
Mon Jan 31 01:12:45 2022
batch 20, train loss = 2.05, mean loss = 2.61
Mon Jan 31 01:13:40 2022
batch 30, train loss = 3.56, mean loss = 2.74
Mon Jan 31 01:14:35 2022
batch 40, train loss = 2.23, mean loss = 2.71
Mon Jan 31 01:15:30 2022
batch 50, train loss = 2.18, mean loss = 2.90
Mon Jan 31 01:16:25 2022
batch 60, train loss = 2.44, mean loss = 2.87
Mon Jan 31 01:17:20 2022
train Loss: 2.89

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 01:18:11 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 01:19:06 2022
val Loss: 0.03

epoch 10 was done for 477.617238 seconds
Epoch 11/299
----------
Mon Jan 31 01:19:43 2022
batch 0, train loss = 2.10, mean loss = 2.10
Mon Jan 31 01:19:48 2022
batch 10, train loss = 1.76, mean loss = 2.20
Mon Jan 31 01:20:43 2022
batch 20, train loss = 1.76, mean loss = 2.39
Mon Jan 31 01:21:38 2022
batch 30, train loss = 3.31, mean loss = 2.48
Mon Jan 31 01:22:33 2022
batch 40, train loss = 2.14, mean loss = 2.49
Mon Jan 31 01:23:29 2022
batch 50, train loss = 2.21, mean loss = 2.68
Mon Jan 31 01:24:23 2022
batch 60, train loss = 2.53, mean loss = 2.66
Mon Jan 31 01:25:19 2022
train Loss: 2.69

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 01:26:10 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 01:27:05 2022
val Loss: 0.03

epoch 11 was done for 478.864762 seconds
Epoch 12/299
----------
Mon Jan 31 01:27:42 2022
batch 0, train loss = 1.75, mean loss = 1.75
Mon Jan 31 01:27:47 2022
batch 10, train loss = 1.93, mean loss = 2.02
Mon Jan 31 01:28:42 2022
batch 20, train loss = 1.44, mean loss = 2.20
Mon Jan 31 01:29:37 2022
batch 30, train loss = 3.20, mean loss = 2.28
Mon Jan 31 01:30:31 2022
batch 40, train loss = 2.14, mean loss = 2.35
Mon Jan 31 01:31:26 2022
batch 50, train loss = 2.06, mean loss = 2.48
Mon Jan 31 01:32:20 2022
batch 60, train loss = 2.49, mean loss = 2.48
Mon Jan 31 01:33:15 2022
train Loss: 2.51

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 01:34:06 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 01:35:00 2022
val Loss: 0.03

epoch 12 was done for 475.574376 seconds
Epoch 13/299
----------
Mon Jan 31 01:35:37 2022
batch 0, train loss = 1.47, mean loss = 1.47
Mon Jan 31 01:35:43 2022
batch 10, train loss = 2.19, mean loss = 1.85
Mon Jan 31 01:36:37 2022
batch 20, train loss = 1.26, mean loss = 2.01
Mon Jan 31 01:37:32 2022
batch 30, train loss = 2.99, mean loss = 2.08
Mon Jan 31 01:38:27 2022
batch 40, train loss = 2.10, mean loss = 2.19
Mon Jan 31 01:39:22 2022
batch 50, train loss = 1.70, mean loss = 2.26
Mon Jan 31 01:40:16 2022
batch 60, train loss = 2.07, mean loss = 2.26
Mon Jan 31 01:41:11 2022
train Loss: 2.29

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 01:42:03 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 01:42:57 2022
val Loss: 0.03

epoch 13 was done for 476.455387 seconds
Epoch 14/299
----------
Mon Jan 31 01:43:34 2022
batch 0, train loss = 1.29, mean loss = 1.29
Mon Jan 31 01:43:39 2022
batch 10, train loss = 2.19, mean loss = 1.63
Mon Jan 31 01:44:34 2022
batch 20, train loss = 1.18, mean loss = 1.76
Mon Jan 31 01:45:29 2022
batch 30, train loss = 2.54, mean loss = 1.86
Mon Jan 31 01:46:24 2022
batch 40, train loss = 1.81, mean loss = 1.96
Mon Jan 31 01:47:19 2022
batch 50, train loss = 1.43, mean loss = 2.01
Mon Jan 31 01:48:13 2022
batch 60, train loss = 1.62, mean loss = 2.02
Mon Jan 31 01:49:08 2022
train Loss: 2.03

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 01:49:59 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 01:50:54 2022
val Loss: 0.03

epoch 14 was done for 477.319497 seconds
Epoch 15/299
----------
Mon Jan 31 01:51:31 2022
batch 0, train loss = 1.22, mean loss = 1.22
Mon Jan 31 01:51:37 2022
batch 10, train loss = 1.82, mean loss = 1.46
Mon Jan 31 01:52:31 2022
batch 20, train loss = 1.09, mean loss = 1.59
Mon Jan 31 01:53:26 2022
batch 30, train loss = 2.19, mean loss = 1.71
Mon Jan 31 01:54:21 2022
batch 40, train loss = 1.60, mean loss = 1.77
Mon Jan 31 01:55:16 2022
batch 50, train loss = 1.34, mean loss = 1.83
Mon Jan 31 01:56:10 2022
batch 60, train loss = 1.42, mean loss = 1.84
Mon Jan 31 01:57:05 2022
train Loss: 1.86

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 01:57:56 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 01:58:50 2022
val Loss: 0.03

epoch 15 was done for 475.905889 seconds
Epoch 16/299
----------
Mon Jan 31 01:59:27 2022
batch 0, train loss = 1.16, mean loss = 1.16
Mon Jan 31 01:59:33 2022
batch 10, train loss = 1.65, mean loss = 1.36
Mon Jan 31 02:00:27 2022
batch 20, train loss = 1.02, mean loss = 1.47
Mon Jan 31 02:01:22 2022
batch 30, train loss = 1.95, mean loss = 1.59
Mon Jan 31 02:02:17 2022
batch 40, train loss = 1.47, mean loss = 1.65
Mon Jan 31 02:03:12 2022
batch 50, train loss = 1.27, mean loss = 1.71
Mon Jan 31 02:04:06 2022
batch 60, train loss = 1.27, mean loss = 1.72
Mon Jan 31 02:05:01 2022
train Loss: 1.73

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 02:05:52 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 02:06:47 2022
val Loss: 0.03

epoch 16 was done for 477.101019 seconds
Epoch 17/299
----------
Mon Jan 31 02:07:24 2022
batch 0, train loss = 1.10, mean loss = 1.10
Mon Jan 31 02:07:30 2022
batch 10, train loss = 1.52, mean loss = 1.28
Mon Jan 31 02:08:24 2022
batch 20, train loss = 0.98, mean loss = 1.39
Mon Jan 31 02:09:19 2022
batch 30, train loss = 1.77, mean loss = 1.51
Mon Jan 31 02:10:13 2022
batch 40, train loss = 1.38, mean loss = 1.54
Mon Jan 31 02:11:08 2022
batch 50, train loss = 1.25, mean loss = 1.61
Mon Jan 31 02:12:02 2022
batch 60, train loss = 1.20, mean loss = 1.62
Mon Jan 31 02:12:57 2022
train Loss: 1.63

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 02:13:48 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 02:14:43 2022
val Loss: 0.03

epoch 17 was done for 476.747014 seconds
Epoch 18/299
----------
Mon Jan 31 02:15:21 2022
batch 0, train loss = 1.05, mean loss = 1.05
Mon Jan 31 02:15:26 2022
batch 10, train loss = 1.46, mean loss = 1.21
Mon Jan 31 02:16:21 2022
batch 20, train loss = 0.93, mean loss = 1.32
Mon Jan 31 02:17:16 2022
batch 30, train loss = 1.63, mean loss = 1.44
Mon Jan 31 02:18:11 2022
batch 40, train loss = 1.31, mean loss = 1.46
Mon Jan 31 02:19:08 2022
batch 50, train loss = 1.21, mean loss = 1.53
Mon Jan 31 02:20:02 2022
batch 60, train loss = 1.13, mean loss = 1.55
Mon Jan 31 02:20:57 2022
train Loss: 1.55

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 02:21:48 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 02:22:43 2022
val Loss: 0.03

epoch 18 was done for 479.310016 seconds
Epoch 19/299
----------
Mon Jan 31 02:23:20 2022
batch 0, train loss = 1.00, mean loss = 1.00
Mon Jan 31 02:23:26 2022
batch 10, train loss = 1.39, mean loss = 1.16
Mon Jan 31 02:24:20 2022
batch 20, train loss = 0.88, mean loss = 1.26
Mon Jan 31 02:25:15 2022
batch 30, train loss = 1.53, mean loss = 1.38
Mon Jan 31 02:26:09 2022
batch 40, train loss = 1.25, mean loss = 1.39
Mon Jan 31 02:27:04 2022
batch 50, train loss = 1.18, mean loss = 1.46
Mon Jan 31 02:27:59 2022
batch 60, train loss = 1.07, mean loss = 1.48
Mon Jan 31 02:28:54 2022
train Loss: 1.48

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 02:29:45 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 02:30:40 2022
val Loss: 0.03

epoch 19 was done for 476.863620 seconds
Epoch 20/299
----------
Mon Jan 31 02:31:17 2022
batch 0, train loss = 0.96, mean loss = 0.96
Mon Jan 31 02:31:22 2022
batch 10, train loss = 1.36, mean loss = 1.12
Mon Jan 31 02:32:17 2022
batch 20, train loss = 0.84, mean loss = 1.21
Mon Jan 31 02:33:12 2022
batch 30, train loss = 1.44, mean loss = 1.32
Mon Jan 31 02:34:06 2022
batch 40, train loss = 1.21, mean loss = 1.33
Mon Jan 31 02:35:01 2022
batch 50, train loss = 1.13, mean loss = 1.40
Mon Jan 31 02:35:56 2022
batch 60, train loss = 1.01, mean loss = 1.41
Mon Jan 31 02:36:51 2022
train Loss: 1.42

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 02:37:42 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 02:38:37 2022
val Loss: 0.03

epoch 20 was done for 477.258126 seconds
Epoch 21/299
----------
Mon Jan 31 02:39:14 2022
batch 0, train loss = 0.93, mean loss = 0.93
Mon Jan 31 02:39:20 2022
batch 10, train loss = 1.32, mean loss = 1.09
Mon Jan 31 02:40:14 2022
batch 20, train loss = 0.81, mean loss = 1.16
Mon Jan 31 02:41:09 2022
batch 30, train loss = 1.35, mean loss = 1.26
Mon Jan 31 02:42:04 2022
batch 40, train loss = 1.15, mean loss = 1.27
Mon Jan 31 02:42:59 2022
batch 50, train loss = 1.08, mean loss = 1.34
Mon Jan 31 02:43:53 2022
batch 60, train loss = 0.94, mean loss = 1.35
Mon Jan 31 02:44:48 2022
train Loss: 1.35

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 02:45:40 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 02:46:34 2022
val Loss: 0.03

epoch 21 was done for 477.221546 seconds
Epoch 22/299
----------
Mon Jan 31 02:47:12 2022
batch 0, train loss = 0.91, mean loss = 0.91
Mon Jan 31 02:47:17 2022
batch 10, train loss = 1.27, mean loss = 1.05
Mon Jan 31 02:48:11 2022
batch 20, train loss = 0.78, mean loss = 1.12
Mon Jan 31 02:49:06 2022
batch 30, train loss = 1.28, mean loss = 1.21
Mon Jan 31 02:50:01 2022
batch 40, train loss = 1.09, mean loss = 1.22
Mon Jan 31 02:50:56 2022
batch 50, train loss = 1.03, mean loss = 1.28
Mon Jan 31 02:51:50 2022
batch 60, train loss = 0.88, mean loss = 1.30
Mon Jan 31 02:52:45 2022
train Loss: 1.30

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 02:53:36 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 02:54:30 2022
val Loss: 0.03

epoch 22 was done for 478.383508 seconds
Epoch 23/299
----------
Mon Jan 31 02:55:10 2022
batch 0, train loss = 0.88, mean loss = 0.88
Mon Jan 31 02:55:15 2022
batch 10, train loss = 1.20, mean loss = 1.02
Mon Jan 31 02:56:10 2022
batch 20, train loss = 0.75, mean loss = 1.08
Mon Jan 31 02:57:05 2022
batch 30, train loss = 1.21, mean loss = 1.16
Mon Jan 31 02:57:59 2022
batch 40, train loss = 1.02, mean loss = 1.17
Mon Jan 31 02:58:54 2022
batch 50, train loss = 0.98, mean loss = 1.23
Mon Jan 31 02:59:49 2022
batch 60, train loss = 0.83, mean loss = 1.24
Mon Jan 31 03:00:44 2022
train Loss: 1.24

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 03:01:35 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 03:02:30 2022
val Loss: 0.03

epoch 23 was done for 477.158418 seconds
Epoch 24/299
----------
Mon Jan 31 03:03:07 2022
batch 0, train loss = 0.85, mean loss = 0.85
Mon Jan 31 03:03:13 2022
batch 10, train loss = 1.15, mean loss = 0.98
Mon Jan 31 03:04:07 2022
batch 20, train loss = 0.73, mean loss = 1.04
Mon Jan 31 03:05:02 2022
batch 30, train loss = 1.15, mean loss = 1.11
Mon Jan 31 03:05:56 2022
batch 40, train loss = 0.97, mean loss = 1.12
Mon Jan 31 03:06:51 2022
batch 50, train loss = 0.95, mean loss = 1.18
Mon Jan 31 03:07:45 2022
batch 60, train loss = 0.80, mean loss = 1.19
Mon Jan 31 03:08:40 2022
train Loss: 1.20

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 03:09:31 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 03:10:26 2022
val Loss: 0.03

epoch 24 was done for 476.541602 seconds
Epoch 25/299
----------
Mon Jan 31 03:11:04 2022
batch 0, train loss = 0.82, mean loss = 0.82
Mon Jan 31 03:11:09 2022
batch 10, train loss = 1.13, mean loss = 0.95
Mon Jan 31 03:12:03 2022
batch 20, train loss = 0.70, mean loss = 1.01
Mon Jan 31 03:12:58 2022
batch 30, train loss = 1.09, mean loss = 1.07
Mon Jan 31 03:13:53 2022
batch 40, train loss = 0.91, mean loss = 1.08
Mon Jan 31 03:14:47 2022
batch 50, train loss = 0.92, mean loss = 1.13
Mon Jan 31 03:15:42 2022
batch 60, train loss = 0.77, mean loss = 1.15
Mon Jan 31 03:16:36 2022
train Loss: 1.15

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 03:17:28 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 03:18:22 2022
val Loss: 0.03

epoch 25 was done for 475.524812 seconds
Epoch 26/299
----------
Mon Jan 31 03:18:59 2022
batch 0, train loss = 0.79, mean loss = 0.79
Mon Jan 31 03:19:05 2022
batch 10, train loss = 1.12, mean loss = 0.92
Mon Jan 31 03:19:59 2022
batch 20, train loss = 0.69, mean loss = 0.98
Mon Jan 31 03:20:54 2022
batch 30, train loss = 1.04, mean loss = 1.04
Mon Jan 31 03:21:48 2022
batch 40, train loss = 0.87, mean loss = 1.05
Mon Jan 31 03:22:43 2022
batch 50, train loss = 0.90, mean loss = 1.10
Mon Jan 31 03:23:37 2022
batch 60, train loss = 0.76, mean loss = 1.11
Mon Jan 31 03:24:32 2022
train Loss: 1.12

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 03:25:23 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 03:26:18 2022
val Loss: 0.03

epoch 26 was done for 476.065541 seconds
Epoch 27/299
----------
Mon Jan 31 03:26:55 2022
batch 0, train loss = 0.77, mean loss = 0.77
Mon Jan 31 03:27:01 2022
batch 10, train loss = 1.11, mean loss = 0.89
Mon Jan 31 03:27:55 2022
batch 20, train loss = 0.67, mean loss = 0.95
Mon Jan 31 03:28:50 2022
batch 30, train loss = 0.99, mean loss = 1.01
Mon Jan 31 03:29:44 2022
batch 40, train loss = 0.83, mean loss = 1.01
Mon Jan 31 03:30:39 2022
batch 50, train loss = 0.88, mean loss = 1.06
Mon Jan 31 03:31:33 2022
batch 60, train loss = 0.74, mean loss = 1.08
Mon Jan 31 03:32:28 2022
train Loss: 1.09

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 03:33:19 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 03:34:13 2022
val Loss: 0.03

epoch 27 was done for 475.496706 seconds
Epoch 28/299
----------
Mon Jan 31 03:34:51 2022
batch 0, train loss = 0.74, mean loss = 0.74
Mon Jan 31 03:34:56 2022
batch 10, train loss = 1.10, mean loss = 0.86
Mon Jan 31 03:35:51 2022
batch 20, train loss = 0.65, mean loss = 0.92
Mon Jan 31 03:36:46 2022
batch 30, train loss = 0.94, mean loss = 0.98
Mon Jan 31 03:37:40 2022
batch 40, train loss = 0.79, mean loss = 0.99
Mon Jan 31 03:38:35 2022
batch 50, train loss = 0.86, mean loss = 1.03
Mon Jan 31 03:39:29 2022
batch 60, train loss = 0.73, mean loss = 1.05
Mon Jan 31 03:40:24 2022
train Loss: 1.06

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 03:41:15 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 03:42:09 2022
val Loss: 0.03

epoch 28 was done for 475.753555 seconds
Epoch 29/299
----------
Mon Jan 31 03:42:46 2022
batch 0, train loss = 0.72, mean loss = 0.72
Mon Jan 31 03:42:52 2022
batch 10, train loss = 1.09, mean loss = 0.84
Mon Jan 31 03:43:46 2022
batch 20, train loss = 0.63, mean loss = 0.89
Mon Jan 31 03:44:41 2022
batch 30, train loss = 0.90, mean loss = 0.95
Mon Jan 31 03:45:36 2022
batch 40, train loss = 0.77, mean loss = 0.96
Mon Jan 31 03:46:31 2022
batch 50, train loss = 0.84, mean loss = 1.00
Mon Jan 31 03:47:25 2022
batch 60, train loss = 0.71, mean loss = 1.02
Mon Jan 31 03:48:20 2022
train Loss: 1.03

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 03:49:12 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 03:50:07 2022
val Loss: 0.03

epoch 29 was done for 477.589040 seconds
Epoch 30/299
----------
Mon Jan 31 03:50:44 2022
batch 0, train loss = 0.69, mean loss = 0.69
Mon Jan 31 03:50:50 2022
batch 10, train loss = 1.10, mean loss = 0.82
Mon Jan 31 03:51:44 2022
batch 20, train loss = 0.62, mean loss = 0.87
Mon Jan 31 03:52:39 2022
batch 30, train loss = 0.87, mean loss = 0.93
Mon Jan 31 03:53:34 2022
batch 40, train loss = 0.75, mean loss = 0.94
Mon Jan 31 03:54:29 2022
batch 50, train loss = 0.81, mean loss = 0.98
Mon Jan 31 03:55:23 2022
batch 60, train loss = 0.69, mean loss = 1.00
Mon Jan 31 03:56:18 2022
train Loss: 1.01

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 03:57:09 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 03:58:03 2022
val Loss: 0.03

epoch 30 was done for 476.806138 seconds
Epoch 31/299
----------
Mon Jan 31 03:58:41 2022
batch 0, train loss = 0.67, mean loss = 0.67
Mon Jan 31 03:58:46 2022
batch 10, train loss = 1.11, mean loss = 0.80
Mon Jan 31 03:59:41 2022
batch 20, train loss = 0.62, mean loss = 0.84
Mon Jan 31 04:00:36 2022
batch 30, train loss = 0.84, mean loss = 0.91
Mon Jan 31 04:01:30 2022
batch 40, train loss = 0.73, mean loss = 0.92
Mon Jan 31 04:02:25 2022
batch 50, train loss = 0.79, mean loss = 0.96
Mon Jan 31 04:03:20 2022
batch 60, train loss = 0.68, mean loss = 0.98
Mon Jan 31 04:04:15 2022
train Loss: 0.99

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 04:05:06 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 04:06:00 2022
val Loss: 0.03

epoch 31 was done for 476.914967 seconds
Epoch 32/299
----------
Mon Jan 31 04:06:38 2022
batch 0, train loss = 0.65, mean loss = 0.65
Mon Jan 31 04:06:43 2022
batch 10, train loss = 1.12, mean loss = 0.78
Mon Jan 31 04:07:38 2022
batch 20, train loss = 0.61, mean loss = 0.82
Mon Jan 31 04:08:33 2022
batch 30, train loss = 0.81, mean loss = 0.90
Mon Jan 31 04:09:27 2022
batch 40, train loss = 0.73, mean loss = 0.90
Mon Jan 31 04:10:22 2022
batch 50, train loss = 0.78, mean loss = 0.94
Mon Jan 31 04:11:16 2022
batch 60, train loss = 0.67, mean loss = 0.96
Mon Jan 31 04:12:11 2022
train Loss: 0.98

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 04:13:02 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 04:13:57 2022
val Loss: 0.03

epoch 32 was done for 477.224065 seconds
Epoch 33/299
----------
Mon Jan 31 04:14:35 2022
batch 0, train loss = 0.63, mean loss = 0.63
Mon Jan 31 04:14:40 2022
batch 10, train loss = 1.13, mean loss = 0.77
Mon Jan 31 04:15:35 2022
batch 20, train loss = 0.60, mean loss = 0.80
Mon Jan 31 04:16:30 2022
batch 30, train loss = 0.78, mean loss = 0.89
Mon Jan 31 04:17:24 2022
batch 40, train loss = 0.72, mean loss = 0.89
Mon Jan 31 04:18:19 2022
batch 50, train loss = 0.77, mean loss = 0.93
Mon Jan 31 04:19:13 2022
batch 60, train loss = 0.67, mean loss = 0.94
Mon Jan 31 04:20:08 2022
train Loss: 0.97

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 04:20:59 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 04:21:54 2022
val Loss: 0.03

epoch 33 was done for 476.364716 seconds
Epoch 34/299
----------
Mon Jan 31 04:22:31 2022
batch 0, train loss = 0.61, mean loss = 0.61
Mon Jan 31 04:22:37 2022
batch 10, train loss = 1.13, mean loss = 0.75
Mon Jan 31 04:23:31 2022
batch 20, train loss = 0.59, mean loss = 0.78
Mon Jan 31 04:24:26 2022
batch 30, train loss = 0.76, mean loss = 0.88
Mon Jan 31 04:25:21 2022
batch 40, train loss = 0.72, mean loss = 0.89
Mon Jan 31 04:26:16 2022
batch 50, train loss = 0.76, mean loss = 0.91
Mon Jan 31 04:27:10 2022
batch 60, train loss = 0.66, mean loss = 0.93
Mon Jan 31 04:28:05 2022
train Loss: 0.96

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 04:28:56 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 04:29:51 2022
val Loss: 0.03

epoch 34 was done for 476.807532 seconds
Epoch 35/299
----------
Mon Jan 31 04:30:28 2022
batch 0, train loss = 0.59, mean loss = 0.59
Mon Jan 31 04:30:34 2022
batch 10, train loss = 1.09, mean loss = 0.73
Mon Jan 31 04:31:28 2022
batch 20, train loss = 0.57, mean loss = 0.76
Mon Jan 31 04:32:23 2022
batch 30, train loss = 0.74, mean loss = 0.86
Mon Jan 31 04:33:18 2022
batch 40, train loss = 0.72, mean loss = 0.87
Mon Jan 31 04:34:13 2022
batch 50, train loss = 0.76, mean loss = 0.90
Mon Jan 31 04:35:07 2022
batch 60, train loss = 0.65, mean loss = 0.92
Mon Jan 31 04:36:02 2022
train Loss: 0.95

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 04:36:53 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 04:37:48 2022
val Loss: 0.03

epoch 35 was done for 477.408027 seconds
Epoch 36/299
----------
Mon Jan 31 04:38:26 2022
batch 0, train loss = 0.58, mean loss = 0.58
Mon Jan 31 04:38:31 2022
batch 10, train loss = 1.03, mean loss = 0.71
Mon Jan 31 04:39:25 2022
batch 20, train loss = 0.55, mean loss = 0.74
Mon Jan 31 04:40:20 2022
batch 30, train loss = 0.73, mean loss = 0.84
Mon Jan 31 04:41:15 2022
batch 40, train loss = 0.70, mean loss = 0.86
Mon Jan 31 04:42:10 2022
batch 50, train loss = 0.75, mean loss = 0.88
Mon Jan 31 04:43:04 2022
batch 60, train loss = 0.63, mean loss = 0.90
Mon Jan 31 04:43:59 2022
train Loss: 0.93

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 04:44:50 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 04:45:45 2022
val Loss: 0.03

epoch 36 was done for 476.525062 seconds
Epoch 37/299
----------
Mon Jan 31 04:46:22 2022
batch 0, train loss = 0.58, mean loss = 0.58
Mon Jan 31 04:46:28 2022
batch 10, train loss = 0.97, mean loss = 0.69
Mon Jan 31 04:47:22 2022
batch 20, train loss = 0.53, mean loss = 0.72
Mon Jan 31 04:48:17 2022
batch 30, train loss = 0.72, mean loss = 0.82
Mon Jan 31 04:49:12 2022
batch 40, train loss = 0.67, mean loss = 0.84
Mon Jan 31 04:50:07 2022
batch 50, train loss = 0.73, mean loss = 0.86
Mon Jan 31 04:51:02 2022
batch 60, train loss = 0.61, mean loss = 0.88
Mon Jan 31 04:51:57 2022
train Loss: 0.92

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 04:52:48 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 04:53:43 2022
val Loss: 0.03

epoch 37 was done for 477.795406 seconds
Epoch 38/299
----------
Mon Jan 31 04:54:20 2022
batch 0, train loss = 0.58, mean loss = 0.58
Mon Jan 31 04:54:25 2022
batch 10, train loss = 0.92, mean loss = 0.68
Mon Jan 31 04:55:20 2022
batch 20, train loss = 0.52, mean loss = 0.71
Mon Jan 31 04:56:15 2022
batch 30, train loss = 0.70, mean loss = 0.80
Mon Jan 31 04:57:09 2022
batch 40, train loss = 0.64, mean loss = 0.82
Mon Jan 31 04:58:04 2022
batch 50, train loss = 0.71, mean loss = 0.85
Mon Jan 31 04:58:59 2022
batch 60, train loss = 0.59, mean loss = 0.86
Mon Jan 31 04:59:54 2022
train Loss: 0.90

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 05:00:45 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 05:01:40 2022
val Loss: 0.03

epoch 38 was done for 477.880959 seconds
Epoch 39/299
----------
Mon Jan 31 05:02:18 2022
batch 0, train loss = 0.59, mean loss = 0.59
Mon Jan 31 05:02:23 2022
batch 10, train loss = 0.87, mean loss = 0.66
Mon Jan 31 05:03:18 2022
batch 20, train loss = 0.51, mean loss = 0.70
Mon Jan 31 05:04:13 2022
batch 30, train loss = 0.68, mean loss = 0.78
Mon Jan 31 05:05:07 2022
batch 40, train loss = 0.61, mean loss = 0.80
Mon Jan 31 05:06:02 2022
batch 50, train loss = 0.70, mean loss = 0.83
Mon Jan 31 05:06:56 2022
batch 60, train loss = 0.57, mean loss = 0.84
Mon Jan 31 05:07:51 2022
train Loss: 0.88

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 05:08:42 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 05:09:37 2022
val Loss: 0.03

epoch 39 was done for 476.321534 seconds
Epoch 40/299
----------
Mon Jan 31 05:10:14 2022
batch 0, train loss = 0.59, mean loss = 0.59
Mon Jan 31 05:10:20 2022
batch 10, train loss = 0.84, mean loss = 0.65
Mon Jan 31 05:11:14 2022
batch 20, train loss = 0.51, mean loss = 0.69
Mon Jan 31 05:12:09 2022
batch 30, train loss = 0.66, mean loss = 0.77
Mon Jan 31 05:13:03 2022
batch 40, train loss = 0.59, mean loss = 0.78
Mon Jan 31 05:13:58 2022
batch 50, train loss = 0.69, mean loss = 0.81
Mon Jan 31 05:14:52 2022
batch 60, train loss = 0.56, mean loss = 0.82
Mon Jan 31 05:15:47 2022
train Loss: 0.87

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 05:16:39 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 05:17:33 2022
val Loss: 0.03

epoch 40 was done for 476.017701 seconds
Epoch 41/299
----------
Mon Jan 31 05:18:10 2022
batch 0, train loss = 0.60, mean loss = 0.60
Mon Jan 31 05:18:15 2022
batch 10, train loss = 0.80, mean loss = 0.65
Mon Jan 31 05:19:10 2022
batch 20, train loss = 0.50, mean loss = 0.68
Mon Jan 31 05:20:05 2022
batch 30, train loss = 0.65, mean loss = 0.75
Mon Jan 31 05:20:59 2022
batch 40, train loss = 0.58, mean loss = 0.77
Mon Jan 31 05:21:54 2022
batch 50, train loss = 0.68, mean loss = 0.80
Mon Jan 31 05:22:48 2022
batch 60, train loss = 0.55, mean loss = 0.81
Mon Jan 31 05:23:43 2022
train Loss: 0.86

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 05:24:34 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 05:25:29 2022
val Loss: 0.03

epoch 41 was done for 476.369246 seconds
Epoch 42/299
----------
Mon Jan 31 05:26:07 2022
batch 0, train loss = 0.61, mean loss = 0.61
Mon Jan 31 05:26:12 2022
batch 10, train loss = 0.76, mean loss = 0.64
Mon Jan 31 05:27:06 2022
batch 20, train loss = 0.50, mean loss = 0.68
Mon Jan 31 05:28:01 2022
batch 30, train loss = 0.63, mean loss = 0.74
Mon Jan 31 05:28:56 2022
batch 40, train loss = 0.56, mean loss = 0.75
Mon Jan 31 05:29:50 2022
batch 50, train loss = 0.67, mean loss = 0.79
Mon Jan 31 05:30:45 2022
batch 60, train loss = 0.55, mean loss = 0.80
Mon Jan 31 05:31:40 2022
train Loss: 0.85

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 05:32:31 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 05:33:25 2022
val Loss: 0.03

epoch 42 was done for 475.762422 seconds
Epoch 43/299
----------
Mon Jan 31 05:34:02 2022
batch 0, train loss = 0.63, mean loss = 0.63
Mon Jan 31 05:34:08 2022
batch 10, train loss = 0.72, mean loss = 0.63
Mon Jan 31 05:35:02 2022
batch 20, train loss = 0.50, mean loss = 0.68
Mon Jan 31 05:35:57 2022
batch 30, train loss = 0.62, mean loss = 0.73
Mon Jan 31 05:36:52 2022
batch 40, train loss = 0.54, mean loss = 0.74
Mon Jan 31 05:37:46 2022
batch 50, train loss = 0.67, mean loss = 0.77
Mon Jan 31 05:38:41 2022
batch 60, train loss = 0.56, mean loss = 0.79
Mon Jan 31 05:39:36 2022
train Loss: 0.85

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 05:40:27 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 05:41:22 2022
val Loss: 0.03

epoch 43 was done for 476.497515 seconds
Epoch 44/299
----------
Mon Jan 31 05:41:59 2022
batch 0, train loss = 0.64, mean loss = 0.64
Mon Jan 31 05:42:04 2022
batch 10, train loss = 0.69, mean loss = 0.63
Mon Jan 31 05:42:58 2022
batch 20, train loss = 0.49, mean loss = 0.67
Mon Jan 31 05:43:54 2022
batch 30, train loss = 0.61, mean loss = 0.72
Mon Jan 31 05:44:48 2022
batch 40, train loss = 0.52, mean loss = 0.73
Mon Jan 31 05:45:43 2022
batch 50, train loss = 0.67, mean loss = 0.76
Mon Jan 31 05:46:37 2022
batch 60, train loss = 0.58, mean loss = 0.78
Mon Jan 31 05:47:32 2022
train Loss: 0.86

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 05:48:23 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 05:49:17 2022
val Loss: 0.03

epoch 44 was done for 475.658989 seconds
Epoch 45/299
----------
Mon Jan 31 05:49:54 2022
batch 0, train loss = 0.69, mean loss = 0.69
Mon Jan 31 05:50:00 2022
batch 10, train loss = 0.65, mean loss = 0.63
Mon Jan 31 05:50:54 2022
batch 20, train loss = 0.49, mean loss = 0.68
Mon Jan 31 05:51:49 2022
batch 30, train loss = 0.60, mean loss = 0.72
Mon Jan 31 05:52:44 2022
batch 40, train loss = 0.51, mean loss = 0.73
Mon Jan 31 05:53:39 2022
batch 50, train loss = 0.66, mean loss = 0.76
Mon Jan 31 05:54:33 2022
batch 60, train loss = 0.59, mean loss = 0.78
Mon Jan 31 05:55:28 2022
train Loss: 0.86

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 05:56:19 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 05:57:13 2022
val Loss: 0.03

epoch 45 was done for 476.305486 seconds
Epoch 46/299
----------
Mon Jan 31 05:57:51 2022
batch 0, train loss = 0.71, mean loss = 0.71
Mon Jan 31 05:57:56 2022
batch 10, train loss = 0.63, mean loss = 0.62
Mon Jan 31 05:58:51 2022
batch 20, train loss = 0.49, mean loss = 0.68
Mon Jan 31 05:59:46 2022
batch 30, train loss = 0.59, mean loss = 0.72
Mon Jan 31 06:00:40 2022
batch 40, train loss = 0.49, mean loss = 0.72
Mon Jan 31 06:01:35 2022
batch 50, train loss = 0.65, mean loss = 0.75
Mon Jan 31 06:02:29 2022
batch 60, train loss = 0.60, mean loss = 0.77
Mon Jan 31 06:03:24 2022
train Loss: 0.86

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 06:04:15 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 06:05:10 2022
val Loss: 0.03

epoch 46 was done for 476.834861 seconds
Epoch 47/299
----------
Mon Jan 31 06:05:48 2022
batch 0, train loss = 0.77, mean loss = 0.77
Mon Jan 31 06:05:53 2022
batch 10, train loss = 0.62, mean loss = 0.62
Mon Jan 31 06:06:47 2022
batch 20, train loss = 0.49, mean loss = 0.68
Mon Jan 31 06:07:42 2022
batch 30, train loss = 0.59, mean loss = 0.72
Mon Jan 31 06:08:37 2022
batch 40, train loss = 0.48, mean loss = 0.72
Mon Jan 31 06:09:32 2022
batch 50, train loss = 0.65, mean loss = 0.75
Mon Jan 31 06:10:26 2022
batch 60, train loss = 0.62, mean loss = 0.77
Mon Jan 31 06:11:21 2022
train Loss: 0.87

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 06:12:12 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 06:13:06 2022
val Loss: 0.03

epoch 47 was done for 475.590778 seconds
Epoch 48/299
----------
Mon Jan 31 06:13:43 2022
batch 0, train loss = 0.82, mean loss = 0.82
Mon Jan 31 06:13:49 2022
batch 10, train loss = 0.61, mean loss = 0.62
Mon Jan 31 06:14:43 2022
batch 20, train loss = 0.50, mean loss = 0.69
Mon Jan 31 06:15:38 2022
batch 30, train loss = 0.59, mean loss = 0.72
Mon Jan 31 06:16:33 2022
batch 40, train loss = 0.46, mean loss = 0.72
Mon Jan 31 06:17:28 2022
batch 50, train loss = 0.64, mean loss = 0.75
Mon Jan 31 06:18:22 2022
batch 60, train loss = 0.62, mean loss = 0.77
Mon Jan 31 06:19:17 2022
train Loss: 0.87

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 06:20:08 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 06:21:02 2022
val Loss: 0.03

epoch 48 was done for 476.439869 seconds
Epoch 49/299
----------
Mon Jan 31 06:21:40 2022
batch 0, train loss = 0.85, mean loss = 0.85
Mon Jan 31 06:21:45 2022
batch 10, train loss = 0.61, mean loss = 0.62
Mon Jan 31 06:22:40 2022
batch 20, train loss = 0.51, mean loss = 0.69
Mon Jan 31 06:23:34 2022
batch 30, train loss = 0.59, mean loss = 0.72
Mon Jan 31 06:24:29 2022
batch 40, train loss = 0.45, mean loss = 0.72
Mon Jan 31 06:25:23 2022
batch 50, train loss = 0.63, mean loss = 0.75
Mon Jan 31 06:26:18 2022
batch 60, train loss = 0.63, mean loss = 0.77
Mon Jan 31 06:27:13 2022
train Loss: 0.88

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 06:28:04 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 06:28:59 2022
val Loss: 0.03

epoch 49 was done for 476.487693 seconds
Epoch 50/299
----------
Mon Jan 31 06:29:36 2022
batch 0, train loss = 0.95, mean loss = 0.95
Mon Jan 31 06:29:41 2022
batch 10, train loss = 0.61, mean loss = 0.64
Mon Jan 31 06:30:36 2022
batch 20, train loss = 0.53, mean loss = 0.70
Mon Jan 31 06:31:31 2022
batch 30, train loss = 0.58, mean loss = 0.74
Mon Jan 31 06:32:25 2022
batch 40, train loss = 0.44, mean loss = 0.73
Mon Jan 31 06:33:20 2022
batch 50, train loss = 0.65, mean loss = 0.76
Mon Jan 31 06:34:15 2022
batch 60, train loss = 0.69, mean loss = 0.78
Mon Jan 31 06:35:10 2022
train Loss: 0.91

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 06:36:01 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 06:36:55 2022
val Loss: 0.03

epoch 50 was done for 476.094663 seconds
Epoch 51/299
----------
Mon Jan 31 06:37:32 2022
batch 0, train loss = 1.07, mean loss = 1.07
Mon Jan 31 06:37:38 2022
batch 10, train loss = 0.61, mean loss = 0.66
Mon Jan 31 06:38:32 2022
batch 20, train loss = 0.55, mean loss = 0.72
Mon Jan 31 06:39:27 2022
batch 30, train loss = 0.58, mean loss = 0.77
Mon Jan 31 06:40:21 2022
batch 40, train loss = 0.44, mean loss = 0.75
Mon Jan 31 06:41:16 2022
batch 50, train loss = 0.69, mean loss = 0.77
Mon Jan 31 06:42:11 2022
batch 60, train loss = 0.76, mean loss = 0.81
Mon Jan 31 06:43:05 2022
train Loss: 0.94

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 06:43:57 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 06:44:51 2022
val Loss: 0.03

epoch 51 was done for 475.826277 seconds
Epoch 52/299
----------
Mon Jan 31 06:45:28 2022
batch 0, train loss = 1.14, mean loss = 1.14
Mon Jan 31 06:45:34 2022
batch 10, train loss = 0.60, mean loss = 0.67
Mon Jan 31 06:46:28 2022
batch 20, train loss = 0.57, mean loss = 0.74
Mon Jan 31 06:47:23 2022
batch 30, train loss = 0.59, mean loss = 0.78
Mon Jan 31 06:48:17 2022
batch 40, train loss = 0.44, mean loss = 0.77
Mon Jan 31 06:49:12 2022
batch 50, train loss = 0.72, mean loss = 0.78
Mon Jan 31 06:50:06 2022
batch 60, train loss = 0.77, mean loss = 0.81
Mon Jan 31 06:51:01 2022
train Loss: 0.94

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 06:51:52 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 06:52:47 2022
val Loss: 0.03

epoch 52 was done for 476.492653 seconds
Epoch 53/299
----------
Mon Jan 31 06:53:25 2022
batch 0, train loss = 1.09, mean loss = 1.09
Mon Jan 31 06:53:30 2022
batch 10, train loss = 0.60, mean loss = 0.65
Mon Jan 31 06:54:24 2022
batch 20, train loss = 0.54, mean loss = 0.72
Mon Jan 31 06:55:19 2022
batch 30, train loss = 0.62, mean loss = 0.76
Mon Jan 31 06:56:14 2022
batch 40, train loss = 0.44, mean loss = 0.74
Mon Jan 31 06:57:09 2022
batch 50, train loss = 0.68, mean loss = 0.75
Mon Jan 31 06:58:03 2022
batch 60, train loss = 0.64, mean loss = 0.77
Mon Jan 31 06:58:58 2022
train Loss: 0.87

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 06:59:49 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 07:00:43 2022
val Loss: 0.03

epoch 53 was done for 475.912258 seconds
Epoch 54/299
----------
Mon Jan 31 07:01:20 2022
batch 0, train loss = 0.86, mean loss = 0.86
Mon Jan 31 07:01:26 2022
batch 10, train loss = 0.58, mean loss = 0.60
Mon Jan 31 07:02:20 2022
batch 20, train loss = 0.49, mean loss = 0.66
Mon Jan 31 07:03:15 2022
batch 30, train loss = 0.69, mean loss = 0.71
Mon Jan 31 07:04:10 2022
batch 40, train loss = 0.42, mean loss = 0.70
Mon Jan 31 07:05:05 2022
batch 50, train loss = 0.65, mean loss = 0.72
Mon Jan 31 07:05:59 2022
batch 60, train loss = 0.48, mean loss = 0.72
Mon Jan 31 07:06:54 2022
train Loss: 0.75

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 07:07:45 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 07:08:39 2022
val Loss: 0.03

epoch 54 was done for 476.144725 seconds
Epoch 55/299
----------
Mon Jan 31 07:09:17 2022
batch 0, train loss = 0.55, mean loss = 0.55
Mon Jan 31 07:09:22 2022
batch 10, train loss = 0.54, mean loss = 0.58
Mon Jan 31 07:10:16 2022
batch 20, train loss = 0.46, mean loss = 0.63
Mon Jan 31 07:11:11 2022
batch 30, train loss = 0.97, mean loss = 0.69
Mon Jan 31 07:12:06 2022
batch 40, train loss = 0.45, mean loss = 0.71
Mon Jan 31 07:13:01 2022
batch 50, train loss = 0.65, mean loss = 0.72
Mon Jan 31 07:13:55 2022
batch 60, train loss = 0.47, mean loss = 0.71
Mon Jan 31 07:14:50 2022
train Loss: 0.75

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 07:15:42 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 07:16:36 2022
val Loss: 0.03

epoch 55 was done for 477.157048 seconds
Epoch 56/299
----------
Mon Jan 31 07:17:14 2022
batch 0, train loss = 0.64, mean loss = 0.64
Mon Jan 31 07:17:19 2022
batch 10, train loss = 0.89, mean loss = 0.60
Mon Jan 31 07:18:14 2022
batch 20, train loss = 0.74, mean loss = 0.65
Mon Jan 31 07:19:09 2022
batch 30, train loss = 1.43, mean loss = 0.81
Mon Jan 31 07:20:03 2022
batch 40, train loss = 0.51, mean loss = 0.83
Mon Jan 31 07:20:58 2022
batch 50, train loss = 0.80, mean loss = 0.83
Mon Jan 31 07:21:52 2022
batch 60, train loss = 0.57, mean loss = 0.82
Mon Jan 31 07:22:47 2022
train Loss: 0.80

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:23:38 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 07:24:33 2022
val Loss: 0.03

epoch 56 was done for 476.020909 seconds
Epoch 57/299
----------
Mon Jan 31 07:25:10 2022
batch 0, train loss = 0.63, mean loss = 0.63
Mon Jan 31 07:25:15 2022
batch 10, train loss = 0.98, mean loss = 1.25
Mon Jan 31 07:26:10 2022
batch 20, train loss = 0.92, mean loss = 1.34
Mon Jan 31 07:27:05 2022
batch 30, train loss = 0.60, mean loss = 1.25
Mon Jan 31 07:28:00 2022
batch 40, train loss = 2.13, mean loss = 1.27
Mon Jan 31 07:28:55 2022
batch 50, train loss = 18.45, mean loss = 2.95
Mon Jan 31 07:29:50 2022
batch 60, train loss = 56.69, mean loss = 9.23
Mon Jan 31 07:30:44 2022
train Loss: 21.15

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:31:36 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:32:30 2022
val Loss: 0.02

epoch 57 was done for 478.188588 seconds
Epoch 58/299
----------
Mon Jan 31 07:33:08 2022
batch 0, train loss = 116.38, mean loss = 116.38
Mon Jan 31 07:33:13 2022
batch 10, train loss = 26.66, mean loss = 38.76
Mon Jan 31 07:34:08 2022
batch 20, train loss = 21.62, mean loss = 35.83
Mon Jan 31 07:35:03 2022
batch 30, train loss = 38.83, mean loss = 32.00
Mon Jan 31 07:35:57 2022
batch 40, train loss = 6.33, mean loss = 28.07
Mon Jan 31 07:36:52 2022
batch 50, train loss = 5.97, mean loss = 26.34
Mon Jan 31 07:37:47 2022
batch 60, train loss = 9.36, mean loss = 23.43
Mon Jan 31 07:38:42 2022
train Loss: 21.92

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:39:33 2022
batch 10, val loss = 0.03, mean loss = 0.02
Mon Jan 31 07:40:32 2022
val Loss: 0.02

epoch 58 was done for 483.747432 seconds
Epoch 59/299
----------
Mon Jan 31 07:41:12 2022
batch 0, train loss = 21.80, mean loss = 21.80
Mon Jan 31 07:41:17 2022
batch 10, train loss = 7.32, mean loss = 7.79
Mon Jan 31 07:42:12 2022
batch 20, train loss = 2.67, mean loss = 6.77
Mon Jan 31 07:43:07 2022
batch 30, train loss = 2.74, mean loss = 5.74
Mon Jan 31 07:44:01 2022
batch 40, train loss = 1.69, mean loss = 4.83
Mon Jan 31 07:44:56 2022
batch 50, train loss = 1.64, mean loss = 4.37
Mon Jan 31 07:45:52 2022
batch 60, train loss = 2.69, mean loss = 4.04
Mon Jan 31 07:46:47 2022
train Loss: 3.86

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:47:39 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:48:34 2022
val Loss: 0.02

epoch 59 was done for 479.640454 seconds
Epoch 60/299
----------
Mon Jan 31 07:49:11 2022
batch 0, train loss = 1.31, mean loss = 1.31
Mon Jan 31 07:49:17 2022
batch 10, train loss = 2.13, mean loss = 1.80
Mon Jan 31 07:50:11 2022
batch 20, train loss = 2.23, mean loss = 1.90
Mon Jan 31 07:51:06 2022
batch 30, train loss = 2.81, mean loss = 2.08
Mon Jan 31 07:52:01 2022
batch 40, train loss = 2.00, mean loss = 2.21
Mon Jan 31 07:52:56 2022
batch 50, train loss = 2.32, mean loss = 2.30
Mon Jan 31 07:53:50 2022
batch 60, train loss = 2.25, mean loss = 2.29
Mon Jan 31 07:54:45 2022
train Loss: 2.33

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:55:37 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:56:32 2022
val Loss: 0.02

epoch 60 was done for 478.656707 seconds
Epoch 61/299
----------
Mon Jan 31 07:57:10 2022
batch 0, train loss = 1.82, mean loss = 1.82
Mon Jan 31 07:57:15 2022
batch 10, train loss = 2.31, mean loss = 2.05
Mon Jan 31 07:58:10 2022
batch 20, train loss = 2.00, mean loss = 2.09
Mon Jan 31 07:59:06 2022
batch 30, train loss = 2.32, mean loss = 2.46
Mon Jan 31 08:00:00 2022
batch 40, train loss = 2.74, mean loss = 2.54
Mon Jan 31 08:00:56 2022
batch 50, train loss = 2.76, mean loss = 2.60
Mon Jan 31 08:01:50 2022
batch 60, train loss = 2.24, mean loss = 2.52
Mon Jan 31 08:02:46 2022
train Loss: 2.47

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:03:37 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:04:32 2022
val Loss: 0.02

epoch 61 was done for 479.274989 seconds
Epoch 62/299
----------
Mon Jan 31 08:05:09 2022
batch 0, train loss = 1.87, mean loss = 1.87
Mon Jan 31 08:05:15 2022
batch 10, train loss = 2.89, mean loss = 1.99
Mon Jan 31 08:06:09 2022
batch 20, train loss = 2.04, mean loss = 2.10
Mon Jan 31 08:07:05 2022
batch 30, train loss = 3.26, mean loss = 2.53
Mon Jan 31 08:07:59 2022
batch 40, train loss = 2.25, mean loss = 2.68
Mon Jan 31 08:08:55 2022
batch 50, train loss = 2.07, mean loss = 2.84
Mon Jan 31 08:09:49 2022
batch 60, train loss = 3.26, mean loss = 2.81
Mon Jan 31 08:10:44 2022
train Loss: 2.70

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:11:35 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:12:30 2022
val Loss: 0.02

epoch 62 was done for 477.840027 seconds
Epoch 63/299
----------
Mon Jan 31 08:13:07 2022
batch 0, train loss = 3.27, mean loss = 3.27
Mon Jan 31 08:13:13 2022
batch 10, train loss = 3.79, mean loss = 2.44
Mon Jan 31 08:14:07 2022
batch 20, train loss = 3.79, mean loss = 2.57
Mon Jan 31 08:15:02 2022
batch 30, train loss = 5.08, mean loss = 2.82
Mon Jan 31 08:15:57 2022
batch 40, train loss = 1.12, mean loss = 2.83
Mon Jan 31 08:16:52 2022
batch 50, train loss = 1.14, mean loss = 3.01
Mon Jan 31 08:17:46 2022
batch 60, train loss = 2.47, mean loss = 2.99
Mon Jan 31 08:18:41 2022
train Loss: 2.92

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:19:33 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:20:28 2022
val Loss: 0.02

epoch 63 was done for 478.052951 seconds
Epoch 64/299
----------
Mon Jan 31 08:21:05 2022
batch 0, train loss = 3.05, mean loss = 3.05
Mon Jan 31 08:21:11 2022
batch 10, train loss = 3.22, mean loss = 3.24
Mon Jan 31 08:22:05 2022
batch 20, train loss = 5.42, mean loss = 3.34
Mon Jan 31 08:23:00 2022
batch 30, train loss = 6.88, mean loss = 3.38
Mon Jan 31 08:23:55 2022
batch 40, train loss = 2.79, mean loss = 3.28
Mon Jan 31 08:24:50 2022
batch 50, train loss = 2.74, mean loss = 3.43
Mon Jan 31 08:25:44 2022
batch 60, train loss = 1.25, mean loss = 3.44
Mon Jan 31 08:26:40 2022
train Loss: 3.52

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:27:31 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:28:28 2022
val Loss: 0.02

epoch 64 was done for 479.843526 seconds
Epoch 65/299
----------
Mon Jan 31 08:29:05 2022
batch 0, train loss = 1.83, mean loss = 1.83
Mon Jan 31 08:29:12 2022
batch 10, train loss = 2.23, mean loss = 3.98
Mon Jan 31 08:30:08 2022
batch 20, train loss = 4.63, mean loss = 3.96
Mon Jan 31 08:31:03 2022
batch 30, train loss = 5.90, mean loss = 4.08
Mon Jan 31 08:32:01 2022
batch 40, train loss = 6.01, mean loss = 4.05
Mon Jan 31 08:32:57 2022
batch 50, train loss = 6.01, mean loss = 4.12
Mon Jan 31 08:33:53 2022
batch 60, train loss = 1.53, mean loss = 4.11
Mon Jan 31 08:34:49 2022
train Loss: 4.27

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:35:47 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:36:52 2022
val Loss: 0.02

epoch 65 was done for 510.487442 seconds
Epoch 66/299
----------
Mon Jan 31 08:37:35 2022
batch 0, train loss = 1.67, mean loss = 1.67
Mon Jan 31 08:37:42 2022
batch 10, train loss = 2.57, mean loss = 4.64
Mon Jan 31 08:38:44 2022
batch 20, train loss = 3.86, mean loss = 4.57
Mon Jan 31 08:39:47 2022
batch 30, train loss = 6.23, mean loss = 5.11
Mon Jan 31 08:40:49 2022
batch 40, train loss = 8.14, mean loss = 5.29
Mon Jan 31 08:41:52 2022
batch 50, train loss = 8.47, mean loss = 5.37
Mon Jan 31 08:42:56 2022
batch 60, train loss = 2.74, mean loss = 5.27
Mon Jan 31 08:44:00 2022
train Loss: 5.37

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:45:00 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:46:03 2022
val Loss: 0.02

epoch 66 was done for 550.174081 seconds
Epoch 67/299
----------
Mon Jan 31 08:46:46 2022
batch 0, train loss = 2.36, mean loss = 2.36
Mon Jan 31 08:46:52 2022
batch 10, train loss = 2.97, mean loss = 4.94
Mon Jan 31 08:47:55 2022
batch 20, train loss = 3.10, mean loss = 4.86
Mon Jan 31 08:48:59 2022
batch 30, train loss = 4.79, mean loss = 5.77
Mon Jan 31 08:50:03 2022
batch 40, train loss = 9.25, mean loss = 6.18
Mon Jan 31 08:51:12 2022
batch 50, train loss = 10.56, mean loss = 6.30
Mon Jan 31 08:52:17 2022
batch 60, train loss = 6.31, mean loss = 6.16
Mon Jan 31 08:53:22 2022
train Loss: 6.13

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:54:24 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:55:29 2022
val Loss: 0.02

epoch 67 was done for 568.812144 seconds
Epoch 68/299
----------
Mon Jan 31 08:56:14 2022
batch 0, train loss = 6.21, mean loss = 6.21
Mon Jan 31 08:56:21 2022
batch 10, train loss = 7.67, mean loss = 5.27
Mon Jan 31 08:57:28 2022
batch 20, train loss = 3.13, mean loss = 5.34
Mon Jan 31 08:58:35 2022
batch 30, train loss = 4.35, mean loss = 6.22
Mon Jan 31 08:59:41 2022
batch 40, train loss = 3.30, mean loss = 6.44
Mon Jan 31 09:00:48 2022
batch 50, train loss = 3.71, mean loss = 6.81
Mon Jan 31 09:01:56 2022
batch 60, train loss = 10.56, mean loss = 6.81
Mon Jan 31 09:03:03 2022
train Loss: 6.57

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:04:07 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:05:14 2022
val Loss: 0.02

epoch 68 was done for 585.608331 seconds
Epoch 69/299
----------
Mon Jan 31 09:06:00 2022
batch 0, train loss = 10.41, mean loss = 10.41
Mon Jan 31 09:06:07 2022
batch 10, train loss = 8.31, mean loss = 6.13
Mon Jan 31 09:07:14 2022
batch 20, train loss = 11.43, mean loss = 6.49
Mon Jan 31 09:08:27 2022
batch 30, train loss = 12.42, mean loss = 6.38
Mon Jan 31 09:09:34 2022
batch 40, train loss = 5.91, mean loss = 6.37
Mon Jan 31 09:10:41 2022
batch 50, train loss = 7.14, mean loss = 6.62
Mon Jan 31 09:11:49 2022
batch 60, train loss = 1.60, mean loss = 6.64
Mon Jan 31 09:12:56 2022
train Loss: 7.06

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:13:59 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:15:06 2022
val Loss: 0.02

epoch 69 was done for 590.955406 seconds
Epoch 70/299
----------
Mon Jan 31 09:15:51 2022
batch 0, train loss = 1.94, mean loss = 1.94
Mon Jan 31 09:15:58 2022
batch 10, train loss = 1.72, mean loss = 6.98
Mon Jan 31 09:17:04 2022
batch 20, train loss = 3.14, mean loss = 6.08
Mon Jan 31 09:18:12 2022
batch 30, train loss = 2.00, mean loss = 6.58
Mon Jan 31 09:19:19 2022
batch 40, train loss = 10.15, mean loss = 6.84
Mon Jan 31 09:20:26 2022
batch 50, train loss = 7.66, mean loss = 6.48
Mon Jan 31 09:21:33 2022
batch 60, train loss = 7.19, mean loss = 6.22
Mon Jan 31 09:22:41 2022
train Loss: 6.03

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:23:44 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:24:51 2022
val Loss: 0.02

epoch 70 was done for 585.768696 seconds
Epoch 71/299
----------
Mon Jan 31 09:25:37 2022
batch 0, train loss = 6.17, mean loss = 6.17
Mon Jan 31 09:25:44 2022
batch 10, train loss = 6.22, mean loss = 3.75
Mon Jan 31 09:26:51 2022
batch 20, train loss = 5.47, mean loss = 3.92
Mon Jan 31 09:27:59 2022
batch 30, train loss = 8.94, mean loss = 4.53
Mon Jan 31 09:29:07 2022
batch 40, train loss = 3.23, mean loss = 4.66
Mon Jan 31 09:30:14 2022
batch 50, train loss = 5.10, mean loss = 4.82
Mon Jan 31 09:31:22 2022
batch 60, train loss = 1.90, mean loss = 4.58
Mon Jan 31 09:32:29 2022
train Loss: 4.41

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:33:32 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:34:39 2022
val Loss: 0.02

epoch 71 was done for 587.706310 seconds
Epoch 72/299
----------
Mon Jan 31 09:35:24 2022
batch 0, train loss = 2.53, mean loss = 2.53
Mon Jan 31 09:35:31 2022
batch 10, train loss = 2.27, mean loss = 3.16
Mon Jan 31 09:36:38 2022
batch 20, train loss = 1.06, mean loss = 2.86
Mon Jan 31 09:37:46 2022
batch 30, train loss = 1.68, mean loss = 3.18
Mon Jan 31 09:38:53 2022
batch 40, train loss = 3.11, mean loss = 3.48
Mon Jan 31 09:40:00 2022
batch 50, train loss = 2.65, mean loss = 3.60
Mon Jan 31 09:41:08 2022
batch 60, train loss = 4.71, mean loss = 3.57
Mon Jan 31 09:42:15 2022
train Loss: 3.39

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:43:18 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:44:25 2022
val Loss: 0.02

epoch 72 was done for 585.661949 seconds
Epoch 73/299
----------
Mon Jan 31 09:45:10 2022
batch 0, train loss = 4.03, mean loss = 4.03
Mon Jan 31 09:45:16 2022
batch 10, train loss = 2.07, mean loss = 2.31
Mon Jan 31 09:46:25 2022
batch 20, train loss = 2.86, mean loss = 2.35
Mon Jan 31 09:47:32 2022
batch 30, train loss = 2.78, mean loss = 2.44
Mon Jan 31 09:48:39 2022
batch 40, train loss = 2.04, mean loss = 2.52
Mon Jan 31 09:49:46 2022
batch 50, train loss = 2.47, mean loss = 2.57
Mon Jan 31 09:50:56 2022
batch 60, train loss = 2.24, mean loss = 2.56
Mon Jan 31 09:52:03 2022
train Loss: 2.56

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:53:06 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:54:13 2022
val Loss: 0.02

epoch 73 was done for 588.286992 seconds
Epoch 74/299
----------
Mon Jan 31 09:54:58 2022
batch 0, train loss = 2.83, mean loss = 2.83
Mon Jan 31 09:55:05 2022
batch 10, train loss = 2.67, mean loss = 2.57
Mon Jan 31 09:56:11 2022
batch 20, train loss = 2.58, mean loss = 2.47
Mon Jan 31 09:57:18 2022
batch 30, train loss = 3.22, mean loss = 2.62
Mon Jan 31 09:58:25 2022
batch 40, train loss = 1.42, mean loss = 2.63
Mon Jan 31 09:59:33 2022
batch 50, train loss = 1.19, mean loss = 2.67
Mon Jan 31 10:00:41 2022
batch 60, train loss = 2.06, mean loss = 2.61
Mon Jan 31 10:01:48 2022
train Loss: 2.55

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:02:51 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:03:58 2022
val Loss: 0.02

epoch 74 was done for 584.429966 seconds
Epoch 75/299
----------
Mon Jan 31 10:04:43 2022
batch 0, train loss = 2.23, mean loss = 2.23
Mon Jan 31 10:04:49 2022
batch 10, train loss = 1.28, mean loss = 2.30
Mon Jan 31 10:05:56 2022
batch 20, train loss = 3.37, mean loss = 2.31
Mon Jan 31 10:07:04 2022
batch 30, train loss = 4.08, mean loss = 2.35
Mon Jan 31 10:08:11 2022
batch 40, train loss = 2.64, mean loss = 2.41
Mon Jan 31 10:09:17 2022
batch 50, train loss = 2.35, mean loss = 2.55
Mon Jan 31 10:10:24 2022
batch 60, train loss = 1.91, mean loss = 2.59
Mon Jan 31 10:11:31 2022
train Loss: 2.68

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:12:34 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:13:42 2022
val Loss: 0.02

epoch 75 was done for 584.963852 seconds
Epoch 76/299
----------
Mon Jan 31 10:14:28 2022
batch 0, train loss = 2.40, mean loss = 2.40
Mon Jan 31 10:14:34 2022
batch 10, train loss = 1.30, mean loss = 2.85
Mon Jan 31 10:15:40 2022
batch 20, train loss = 3.38, mean loss = 2.76
Mon Jan 31 10:16:46 2022
batch 30, train loss = 3.81, mean loss = 2.71
Mon Jan 31 10:17:52 2022
batch 40, train loss = 3.72, mean loss = 2.77
Mon Jan 31 10:18:59 2022
batch 50, train loss = 3.36, mean loss = 2.87
Mon Jan 31 10:20:06 2022
batch 60, train loss = 1.47, mean loss = 2.95
Mon Jan 31 10:21:13 2022
train Loss: 3.18

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:22:18 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:23:25 2022
val Loss: 0.02

epoch 76 was done for 582.384837 seconds
Epoch 77/299
----------
Mon Jan 31 10:24:10 2022
batch 0, train loss = 1.59, mean loss = 1.59
Mon Jan 31 10:24:17 2022
batch 10, train loss = 0.97, mean loss = 3.49
Mon Jan 31 10:25:24 2022
batch 20, train loss = 3.10, mean loss = 3.21
Mon Jan 31 10:26:30 2022
batch 30, train loss = 3.50, mean loss = 3.22
Mon Jan 31 10:27:37 2022
batch 40, train loss = 6.06, mean loss = 3.38
Mon Jan 31 10:28:43 2022
batch 50, train loss = 5.93, mean loss = 3.34
Mon Jan 31 10:29:49 2022
batch 60, train loss = 1.14, mean loss = 3.40
Mon Jan 31 10:30:55 2022
train Loss: 3.73

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:31:58 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:33:04 2022
val Loss: 0.02

epoch 77 was done for 578.452233 seconds
Epoch 78/299
----------
Mon Jan 31 10:33:49 2022
batch 0, train loss = 0.90, mean loss = 0.90
Mon Jan 31 10:33:55 2022
batch 10, train loss = 2.23, mean loss = 3.98
Mon Jan 31 10:35:02 2022
batch 20, train loss = 1.91, mean loss = 3.61
Mon Jan 31 10:36:09 2022
batch 30, train loss = 2.39, mean loss = 3.98
Mon Jan 31 10:37:15 2022
batch 40, train loss = 7.38, mean loss = 4.24
Mon Jan 31 10:38:21 2022
batch 50, train loss = 7.22, mean loss = 4.13
Mon Jan 31 10:39:27 2022
batch 60, train loss = 2.77, mean loss = 4.08
Mon Jan 31 10:40:32 2022
train Loss: 4.28

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:41:35 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:42:41 2022
val Loss: 0.02

epoch 78 was done for 577.522496 seconds
Epoch 79/299
----------
Mon Jan 31 10:43:26 2022
batch 0, train loss = 1.94, mean loss = 1.94
Mon Jan 31 10:43:33 2022
batch 10, train loss = 5.40, mean loss = 4.15
Mon Jan 31 10:44:39 2022
batch 20, train loss = 1.67, mean loss = 4.08
Mon Jan 31 10:45:45 2022
batch 30, train loss = 2.63, mean loss = 4.89
Mon Jan 31 10:46:51 2022
batch 40, train loss = 6.70, mean loss = 5.21
Mon Jan 31 10:47:57 2022
batch 50, train loss = 6.21, mean loss = 5.17
Mon Jan 31 10:49:04 2022
batch 60, train loss = 6.45, mean loss = 5.07
Mon Jan 31 10:50:10 2022
train Loss: 4.99

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:51:12 2022
