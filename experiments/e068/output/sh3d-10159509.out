The activation script must be sourced, otherwise the virtual environment will not work.
Setting vars
iscuda= True
iscuda= True
iscuda= True
opt.wandb =  
opt.wandb =  
opt.wandb =  
93
93
93
file to frame csv ../../csv/598frame.csv
file to frame csv ../../csv/598frame.csv
file to frame csv ../../csv/598frame.csv
iscuda= True
PyTorch Version:  1.8.1
Torchvision Version:  0.9.0a0
opt:
 Namespace(ampl=441, aug_gt='orient', batch_output=2, bs=15, cencrop=700, center=False, chidden_dim=[96, 128, 256, 256, 256], classicnorm=False, cmscrop=0, conTrain='', criterion='L2', csvname='598csv9', datapath='/p/project/delia-mp/cherepashkin1/phenoseed/', dfname='598frame', downsample=1, epoch=300, expdescr='', expnum='e068', feature_extract=False, framelim=6000, gradient_predivide_factor=1.0, gttype='single_file', haf=True, hidden_dim=[9], inputt='img', kernel_sizes=[7, 3, 3, 3, 3, 3], lb='orient', loadh5=False, localexp='', lr=0.0005, machine='jureca', maintain=False, maintain_line=False, man_dist=False, measure_time=False, merging='batch', minmax=False, minmax3dimage=False, minmax_f=True, minmax_fn='', model_name='', netname=['cnet'], ngpu=4, noise_input=False, noise_output=False, normalize=False, num_input_images=3, num_sam_points=500, num_workers=0, outputt='orient', parallel='horovod', pin_memory=False, print_minibatch=10, pscale=100, rand_angle=False, rescale=500, rot_dirs=False, rotate_output=False, save_output=True, single_folder=False, specie='598', standardize=255, steplr=[1000.0, 1.0], transappendix='_image', ufmodel=100000, updateFraction=0.25, use_adasum=False, use_cuda=True, use_existing_csv=True, use_pretrained=False, use_sep_csv=True, view_sep=False, wandb='', weight_decay=0, zero_angle=True)
sys.argv:
 ['../../main.py', '-datapath', '/p/project/delia-mp/cherepashkin1/phenoseed/', '-epoch', '300', '-bs', '15', '-num_input_images', '3', '-framelim', '6000', '-criterion', 'L2', '-localexp', '', '-lr', '5e-4', '-expnum', 'e068', '-hidden_dim', '9', '-inputt', 'img', '-outputt', 'orient', '-lb', 'orient', '-no_loadh5', '-minmax_fn', '', '-parallel', 'horovod', '-machine', 'jureca', '-merging', 'batch', '-aug_gt', 'orient', '-updateFraction', '0.25', '-steplr', '1000', '1', '-print_minibatch', '10', '-dfname', '598frame']
seed =  0
path were main.py is located= ../../
opt.wandb =  
93
file to frame csv ../../csv/598frame.csv
lframe's length after laoding =  5283
lframe's length after laoding =  5283
lframe's length after laoding =  5283
lframe's length after laoding =  5283
lframe len after excluding all exceptions= 5200
lframe len after excluding all exceptions= 5200
lframe len after excluding all exceptions= 5200
lframe len after excluding all exceptions= 5200
len train =  4160
len train =  4160
len train =  4160
len train =  4160
train consists of 277 full batches with 15 tensors with 3 views
the last batch has size of 5 tensors with 3 views
val consists of 69 full batches with 15 tensors with 3 views
the last batch has size of 5 tensors with 3 views
Epoch 0/299
----------
Sun Jan 30 23:50:01 2022
batch 0, train loss = 0.52, mean loss = 0.52
Sun Jan 30 23:50:10 2022
batch 10, train loss = 1452.44, mean loss = 3341.99
Sun Jan 30 23:51:21 2022
batch 20, train loss = 740.19, mean loss = 2393.16
Sun Jan 30 23:52:31 2022
batch 30, train loss = 626.49, mean loss = 1746.76
Sun Jan 30 23:53:41 2022
batch 40, train loss = 372.92, mean loss = 1404.54
Sun Jan 30 23:54:50 2022
batch 50, train loss = 156.42, mean loss = 1158.30
Sun Jan 30 23:55:59 2022
batch 60, train loss = 64.28, mean loss = 983.45
Sun Jan 30 23:57:09 2022
train Loss: 870.68

batch 0, val loss = 0.02, mean loss = 0.02
Sun Jan 30 23:58:13 2022
batch 10, val loss = 0.02, mean loss = 0.02
Sun Jan 30 23:59:23 2022
val Loss: 0.02

epoch 0 was done for 608.540684 seconds
Epoch 1/299
----------
Mon Jan 31 00:00:10 2022
batch 0, train loss = 71.41, mean loss = 71.41
Mon Jan 31 00:00:15 2022
batch 10, train loss = 67.42, mean loss = 60.09
Mon Jan 31 00:01:09 2022
batch 20, train loss = 31.44, mean loss = 56.95
Mon Jan 31 00:02:04 2022
batch 30, train loss = 38.49, mean loss = 53.36
Mon Jan 31 00:02:57 2022
batch 40, train loss = 17.95, mean loss = 47.32
Mon Jan 31 00:03:51 2022
batch 50, train loss = 20.10, mean loss = 43.58
Mon Jan 31 00:04:45 2022
batch 60, train loss = 18.29, mean loss = 39.21
Mon Jan 31 00:05:38 2022
train Loss: 36.65

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:06:28 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:07:22 2022
val Loss: 0.02

epoch 1 was done for 469.068820 seconds
Epoch 2/299
----------
Mon Jan 31 00:07:59 2022
batch 0, train loss = 12.48, mean loss = 12.48
Mon Jan 31 00:08:04 2022
batch 10, train loss = 15.19, mean loss = 13.13
Mon Jan 31 00:08:59 2022
batch 20, train loss = 7.93, mean loss = 11.63
Mon Jan 31 00:09:53 2022
batch 30, train loss = 7.28, mean loss = 10.74
Mon Jan 31 00:10:47 2022
batch 40, train loss = 7.10, mean loss = 9.93
Mon Jan 31 00:11:42 2022
batch 50, train loss = 6.12, mean loss = 9.26
Mon Jan 31 00:12:36 2022
batch 60, train loss = 6.68, mean loss = 8.65
Mon Jan 31 00:13:30 2022
train Loss: 8.17

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:14:21 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:15:16 2022
val Loss: 0.02

epoch 2 was done for 474.446697 seconds
Epoch 3/299
----------
Mon Jan 31 00:15:53 2022
batch 0, train loss = 3.12, mean loss = 3.12
Mon Jan 31 00:15:59 2022
batch 10, train loss = 2.90, mean loss = 3.60
Mon Jan 31 00:16:52 2022
batch 20, train loss = 3.72, mean loss = 3.38
Mon Jan 31 00:17:46 2022
batch 30, train loss = 2.25, mean loss = 3.36
Mon Jan 31 00:18:40 2022
batch 40, train loss = 2.87, mean loss = 3.28
Mon Jan 31 00:19:33 2022
batch 50, train loss = 2.39, mean loss = 3.14
Mon Jan 31 00:20:27 2022
batch 60, train loss = 2.64, mean loss = 3.04
Mon Jan 31 00:21:21 2022
train Loss: 2.93

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:22:11 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:23:05 2022
val Loss: 0.02

epoch 3 was done for 468.736081 seconds
Epoch 4/299
----------
Mon Jan 31 00:23:42 2022
batch 0, train loss = 1.81, mean loss = 1.81
Mon Jan 31 00:23:47 2022
batch 10, train loss = 1.51, mean loss = 1.97
Mon Jan 31 00:24:41 2022
batch 20, train loss = 2.33, mean loss = 1.92
Mon Jan 31 00:25:35 2022
batch 30, train loss = 1.43, mean loss = 1.97
Mon Jan 31 00:26:28 2022
batch 40, train loss = 2.12, mean loss = 2.02
Mon Jan 31 00:27:22 2022
batch 50, train loss = 1.80, mean loss = 1.97
Mon Jan 31 00:28:16 2022
batch 60, train loss = 1.88, mean loss = 1.96
Mon Jan 31 00:29:10 2022
train Loss: 1.93

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:30:00 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:30:54 2022
val Loss: 0.02

epoch 4 was done for 468.936308 seconds
Epoch 5/299
----------
Mon Jan 31 00:31:31 2022
batch 0, train loss = 1.16, mean loss = 1.16
Mon Jan 31 00:31:37 2022
batch 10, train loss = 1.19, mean loss = 1.47
Mon Jan 31 00:32:31 2022
batch 20, train loss = 1.58, mean loss = 1.44
Mon Jan 31 00:33:25 2022
batch 30, train loss = 1.31, mean loss = 1.51
Mon Jan 31 00:34:19 2022
batch 40, train loss = 1.76, mean loss = 1.55
Mon Jan 31 00:35:13 2022
batch 50, train loss = 1.43, mean loss = 1.53
Mon Jan 31 00:36:07 2022
batch 60, train loss = 1.55, mean loss = 1.54
Mon Jan 31 00:37:02 2022
train Loss: 1.52

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:37:52 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:38:47 2022
val Loss: 0.02

epoch 5 was done for 473.171350 seconds
Epoch 6/299
----------
Mon Jan 31 00:39:24 2022
batch 0, train loss = 0.90, mean loss = 0.90
Mon Jan 31 00:39:30 2022
batch 10, train loss = 0.90, mean loss = 1.21
Mon Jan 31 00:40:23 2022
batch 20, train loss = 1.30, mean loss = 1.19
Mon Jan 31 00:41:17 2022
batch 30, train loss = 1.11, mean loss = 1.27
Mon Jan 31 00:42:10 2022
batch 40, train loss = 1.55, mean loss = 1.31
Mon Jan 31 00:43:04 2022
batch 50, train loss = 1.25, mean loss = 1.30
Mon Jan 31 00:43:57 2022
batch 60, train loss = 1.33, mean loss = 1.32
Mon Jan 31 00:44:51 2022
train Loss: 1.31

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:45:41 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:46:35 2022
val Loss: 0.02

epoch 6 was done for 467.268607 seconds
Epoch 7/299
----------
Mon Jan 31 00:47:11 2022
batch 0, train loss = 0.81, mean loss = 0.81
Mon Jan 31 00:47:17 2022
batch 10, train loss = 0.78, mean loss = 1.06
Mon Jan 31 00:48:10 2022
batch 20, train loss = 1.18, mean loss = 1.05
Mon Jan 31 00:49:04 2022
batch 30, train loss = 1.00, mean loss = 1.13
Mon Jan 31 00:49:58 2022
batch 40, train loss = 1.40, mean loss = 1.16
Mon Jan 31 00:50:51 2022
batch 50, train loss = 1.11, mean loss = 1.16
Mon Jan 31 00:51:44 2022
batch 60, train loss = 1.18, mean loss = 1.18
Mon Jan 31 00:52:38 2022
train Loss: 1.17

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:53:28 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 00:54:22 2022
val Loss: 0.02

epoch 7 was done for 467.622592 seconds
Epoch 8/299
----------
Mon Jan 31 00:54:59 2022
batch 0, train loss = 0.73, mean loss = 0.73
Mon Jan 31 00:55:04 2022
batch 10, train loss = 0.69, mean loss = 0.95
Mon Jan 31 00:55:58 2022
batch 20, train loss = 1.06, mean loss = 0.95
Mon Jan 31 00:56:51 2022
batch 30, train loss = 0.91, mean loss = 1.02
Mon Jan 31 00:57:45 2022
batch 40, train loss = 1.27, mean loss = 1.05
Mon Jan 31 00:58:38 2022
batch 50, train loss = 1.00, mean loss = 1.06
Mon Jan 31 00:59:32 2022
batch 60, train loss = 1.07, mean loss = 1.08
Mon Jan 31 01:00:25 2022
train Loss: 1.07

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:01:16 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:02:10 2022
val Loss: 0.02

epoch 8 was done for 468.773673 seconds
Epoch 9/299
----------
Mon Jan 31 01:02:48 2022
batch 0, train loss = 0.68, mean loss = 0.68
Mon Jan 31 01:02:53 2022
batch 10, train loss = 0.62, mean loss = 0.88
Mon Jan 31 01:03:47 2022
batch 20, train loss = 0.95, mean loss = 0.88
Mon Jan 31 01:04:41 2022
batch 30, train loss = 0.84, mean loss = 0.94
Mon Jan 31 01:05:35 2022
batch 40, train loss = 1.16, mean loss = 0.97
Mon Jan 31 01:06:29 2022
batch 50, train loss = 0.92, mean loss = 0.98
Mon Jan 31 01:07:22 2022
batch 60, train loss = 0.97, mean loss = 1.00
Mon Jan 31 01:08:16 2022
train Loss: 0.99

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:09:06 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:10:00 2022
val Loss: 0.02

epoch 9 was done for 469.361625 seconds
Epoch 10/299
----------
Mon Jan 31 01:10:37 2022
batch 0, train loss = 0.63, mean loss = 0.63
Mon Jan 31 01:10:43 2022
batch 10, train loss = 0.56, mean loss = 0.83
Mon Jan 31 01:11:36 2022
batch 20, train loss = 0.87, mean loss = 0.82
Mon Jan 31 01:12:30 2022
batch 30, train loss = 0.79, mean loss = 0.88
Mon Jan 31 01:13:24 2022
batch 40, train loss = 1.05, mean loss = 0.90
Mon Jan 31 01:14:18 2022
batch 50, train loss = 0.86, mean loss = 0.92
Mon Jan 31 01:15:12 2022
batch 60, train loss = 0.88, mean loss = 0.93
Mon Jan 31 01:16:05 2022
train Loss: 0.92

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:16:56 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:17:50 2022
val Loss: 0.02

epoch 10 was done for 469.845519 seconds
Epoch 11/299
----------
Mon Jan 31 01:18:27 2022
batch 0, train loss = 0.58, mean loss = 0.58
Mon Jan 31 01:18:32 2022
batch 10, train loss = 0.52, mean loss = 0.79
Mon Jan 31 01:19:26 2022
batch 20, train loss = 0.80, mean loss = 0.78
Mon Jan 31 01:20:20 2022
batch 30, train loss = 0.74, mean loss = 0.83
Mon Jan 31 01:21:13 2022
batch 40, train loss = 0.97, mean loss = 0.85
Mon Jan 31 01:22:07 2022
batch 50, train loss = 0.80, mean loss = 0.87
Mon Jan 31 01:23:00 2022
batch 60, train loss = 0.81, mean loss = 0.88
Mon Jan 31 01:23:54 2022
train Loss: 0.87

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:24:45 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:25:39 2022
val Loss: 0.02

epoch 11 was done for 468.200168 seconds
Epoch 12/299
----------
Mon Jan 31 01:26:15 2022
batch 0, train loss = 0.54, mean loss = 0.54
Mon Jan 31 01:26:21 2022
batch 10, train loss = 0.50, mean loss = 0.74
Mon Jan 31 01:27:15 2022
batch 20, train loss = 0.76, mean loss = 0.74
Mon Jan 31 01:28:09 2022
batch 30, train loss = 0.70, mean loss = 0.79
Mon Jan 31 01:29:03 2022
batch 40, train loss = 0.90, mean loss = 0.81
Mon Jan 31 01:29:56 2022
batch 50, train loss = 0.76, mean loss = 0.82
Mon Jan 31 01:30:50 2022
batch 60, train loss = 0.75, mean loss = 0.83
Mon Jan 31 01:31:44 2022
train Loss: 0.82

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:32:34 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:33:28 2022
val Loss: 0.02

epoch 12 was done for 469.513939 seconds
Epoch 13/299
----------
Mon Jan 31 01:34:05 2022
batch 0, train loss = 0.52, mean loss = 0.52
Mon Jan 31 01:34:10 2022
batch 10, train loss = 0.49, mean loss = 0.70
Mon Jan 31 01:35:04 2022
batch 20, train loss = 0.72, mean loss = 0.70
Mon Jan 31 01:35:58 2022
batch 30, train loss = 0.67, mean loss = 0.75
Mon Jan 31 01:36:52 2022
batch 40, train loss = 0.85, mean loss = 0.77
Mon Jan 31 01:37:46 2022
batch 50, train loss = 0.71, mean loss = 0.78
Mon Jan 31 01:38:40 2022
batch 60, train loss = 0.70, mean loss = 0.79
Mon Jan 31 01:39:34 2022
train Loss: 0.78

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:40:24 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:41:19 2022
val Loss: 0.02

epoch 13 was done for 471.111079 seconds
Epoch 14/299
----------
Mon Jan 31 01:41:56 2022
batch 0, train loss = 0.50, mean loss = 0.50
Mon Jan 31 01:42:01 2022
batch 10, train loss = 0.48, mean loss = 0.67
Mon Jan 31 01:42:55 2022
batch 20, train loss = 0.68, mean loss = 0.67
Mon Jan 31 01:43:49 2022
batch 30, train loss = 0.64, mean loss = 0.72
Mon Jan 31 01:44:42 2022
batch 40, train loss = 0.80, mean loss = 0.73
Mon Jan 31 01:45:36 2022
batch 50, train loss = 0.68, mean loss = 0.75
Mon Jan 31 01:46:30 2022
batch 60, train loss = 0.65, mean loss = 0.76
Mon Jan 31 01:47:24 2022
train Loss: 0.74

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:48:14 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:49:08 2022
val Loss: 0.02

epoch 14 was done for 468.433591 seconds
Epoch 15/299
----------
Mon Jan 31 01:49:44 2022
batch 0, train loss = 0.48, mean loss = 0.48
Mon Jan 31 01:49:50 2022
batch 10, train loss = 0.47, mean loss = 0.64
Mon Jan 31 01:50:44 2022
batch 20, train loss = 0.66, mean loss = 0.65
Mon Jan 31 01:51:38 2022
batch 30, train loss = 0.61, mean loss = 0.69
Mon Jan 31 01:52:31 2022
batch 40, train loss = 0.75, mean loss = 0.70
Mon Jan 31 01:53:25 2022
batch 50, train loss = 0.65, mean loss = 0.72
Mon Jan 31 01:54:19 2022
batch 60, train loss = 0.61, mean loss = 0.73
Mon Jan 31 01:55:13 2022
train Loss: 0.71

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:56:03 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 01:56:57 2022
val Loss: 0.02

epoch 15 was done for 470.135598 seconds
Epoch 16/299
----------
Mon Jan 31 01:57:34 2022
batch 0, train loss = 0.46, mean loss = 0.46
Mon Jan 31 01:57:40 2022
batch 10, train loss = 0.47, mean loss = 0.62
Mon Jan 31 01:58:34 2022
batch 20, train loss = 0.63, mean loss = 0.63
Mon Jan 31 01:59:28 2022
batch 30, train loss = 0.58, mean loss = 0.67
Mon Jan 31 02:00:22 2022
batch 40, train loss = 0.72, mean loss = 0.68
Mon Jan 31 02:01:16 2022
batch 50, train loss = 0.62, mean loss = 0.69
Mon Jan 31 02:02:10 2022
batch 60, train loss = 0.57, mean loss = 0.70
Mon Jan 31 02:03:05 2022
train Loss: 0.69

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:03:55 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:04:50 2022
val Loss: 0.02

epoch 16 was done for 472.708870 seconds
Epoch 17/299
----------
Mon Jan 31 02:05:27 2022
batch 0, train loss = 0.45, mean loss = 0.45
Mon Jan 31 02:05:33 2022
batch 10, train loss = 0.46, mean loss = 0.60
Mon Jan 31 02:06:26 2022
batch 20, train loss = 0.62, mean loss = 0.61
Mon Jan 31 02:07:20 2022
batch 30, train loss = 0.56, mean loss = 0.65
Mon Jan 31 02:08:14 2022
batch 40, train loss = 0.69, mean loss = 0.66
Mon Jan 31 02:09:08 2022
batch 50, train loss = 0.59, mean loss = 0.67
Mon Jan 31 02:10:01 2022
batch 60, train loss = 0.54, mean loss = 0.68
Mon Jan 31 02:10:55 2022
train Loss: 0.67

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:11:46 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:12:39 2022
val Loss: 0.02

epoch 17 was done for 468.751176 seconds
Epoch 18/299
----------
Mon Jan 31 02:13:16 2022
batch 0, train loss = 0.43, mean loss = 0.43
Mon Jan 31 02:13:21 2022
batch 10, train loss = 0.46, mean loss = 0.59
Mon Jan 31 02:14:15 2022
batch 20, train loss = 0.61, mean loss = 0.60
Mon Jan 31 02:15:09 2022
batch 30, train loss = 0.55, mean loss = 0.63
Mon Jan 31 02:16:03 2022
batch 40, train loss = 0.66, mean loss = 0.64
Mon Jan 31 02:16:57 2022
batch 50, train loss = 0.57, mean loss = 0.65
Mon Jan 31 02:17:51 2022
batch 60, train loss = 0.52, mean loss = 0.66
Mon Jan 31 02:18:47 2022
train Loss: 0.65

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:19:37 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:20:31 2022
val Loss: 0.02

epoch 18 was done for 472.501436 seconds
Epoch 19/299
----------
Mon Jan 31 02:21:08 2022
batch 0, train loss = 0.42, mean loss = 0.42
Mon Jan 31 02:21:14 2022
batch 10, train loss = 0.46, mean loss = 0.58
Mon Jan 31 02:22:08 2022
batch 20, train loss = 0.60, mean loss = 0.59
Mon Jan 31 02:23:02 2022
batch 30, train loss = 0.53, mean loss = 0.62
Mon Jan 31 02:23:56 2022
batch 40, train loss = 0.64, mean loss = 0.63
Mon Jan 31 02:24:50 2022
batch 50, train loss = 0.55, mean loss = 0.64
Mon Jan 31 02:25:44 2022
batch 60, train loss = 0.49, mean loss = 0.65
Mon Jan 31 02:26:38 2022
train Loss: 0.64

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:27:29 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:28:23 2022
val Loss: 0.02

epoch 19 was done for 472.148882 seconds
Epoch 20/299
----------
Mon Jan 31 02:29:01 2022
batch 0, train loss = 0.42, mean loss = 0.42
Mon Jan 31 02:29:06 2022
batch 10, train loss = 0.47, mean loss = 0.57
Mon Jan 31 02:30:00 2022
batch 20, train loss = 0.61, mean loss = 0.58
Mon Jan 31 02:30:54 2022
batch 30, train loss = 0.51, mean loss = 0.61
Mon Jan 31 02:31:47 2022
batch 40, train loss = 0.62, mean loss = 0.61
Mon Jan 31 02:32:41 2022
batch 50, train loss = 0.53, mean loss = 0.63
Mon Jan 31 02:33:35 2022
batch 60, train loss = 0.47, mean loss = 0.63
Mon Jan 31 02:34:29 2022
train Loss: 0.63

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:35:19 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:36:13 2022
val Loss: 0.02

epoch 20 was done for 468.704666 seconds
Epoch 21/299
----------
Mon Jan 31 02:36:49 2022
batch 0, train loss = 0.42, mean loss = 0.42
Mon Jan 31 02:36:55 2022
batch 10, train loss = 0.47, mean loss = 0.56
Mon Jan 31 02:37:48 2022
batch 20, train loss = 0.61, mean loss = 0.57
Mon Jan 31 02:38:42 2022
batch 30, train loss = 0.50, mean loss = 0.60
Mon Jan 31 02:39:36 2022
batch 40, train loss = 0.59, mean loss = 0.60
Mon Jan 31 02:40:30 2022
batch 50, train loss = 0.51, mean loss = 0.62
Mon Jan 31 02:41:24 2022
batch 60, train loss = 0.46, mean loss = 0.63
Mon Jan 31 02:42:18 2022
train Loss: 0.62

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:43:08 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:44:02 2022
val Loss: 0.02

epoch 21 was done for 469.569432 seconds
Epoch 22/299
----------
Mon Jan 31 02:44:39 2022
batch 0, train loss = 0.42, mean loss = 0.42
Mon Jan 31 02:44:44 2022
batch 10, train loss = 0.47, mean loss = 0.54
Mon Jan 31 02:45:38 2022
batch 20, train loss = 0.62, mean loss = 0.57
Mon Jan 31 02:46:32 2022
batch 30, train loss = 0.48, mean loss = 0.60
Mon Jan 31 02:47:26 2022
batch 40, train loss = 0.56, mean loss = 0.60
Mon Jan 31 02:48:20 2022
batch 50, train loss = 0.51, mean loss = 0.62
Mon Jan 31 02:49:13 2022
batch 60, train loss = 0.44, mean loss = 0.62
Mon Jan 31 02:50:08 2022
train Loss: 0.61

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:50:58 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:51:52 2022
val Loss: 0.02

epoch 22 was done for 470.356662 seconds
Epoch 23/299
----------
Mon Jan 31 02:52:29 2022
batch 0, train loss = 0.42, mean loss = 0.42
Mon Jan 31 02:52:35 2022
batch 10, train loss = 0.45, mean loss = 0.53
Mon Jan 31 02:53:28 2022
batch 20, train loss = 0.60, mean loss = 0.56
Mon Jan 31 02:54:22 2022
batch 30, train loss = 0.47, mean loss = 0.59
Mon Jan 31 02:55:16 2022
batch 40, train loss = 0.52, mean loss = 0.58
Mon Jan 31 02:56:10 2022
batch 50, train loss = 0.51, mean loss = 0.61
Mon Jan 31 02:57:04 2022
batch 60, train loss = 0.44, mean loss = 0.61
Mon Jan 31 02:57:58 2022
train Loss: 0.60

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:58:48 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 02:59:42 2022
val Loss: 0.02

epoch 23 was done for 469.636141 seconds
Epoch 24/299
----------
Mon Jan 31 03:00:19 2022
batch 0, train loss = 0.42, mean loss = 0.42
Mon Jan 31 03:00:24 2022
batch 10, train loss = 0.43, mean loss = 0.51
Mon Jan 31 03:01:18 2022
batch 20, train loss = 0.58, mean loss = 0.54
Mon Jan 31 03:02:12 2022
batch 30, train loss = 0.47, mean loss = 0.58
Mon Jan 31 03:03:06 2022
batch 40, train loss = 0.49, mean loss = 0.57
Mon Jan 31 03:04:00 2022
batch 50, train loss = 0.51, mean loss = 0.60
Mon Jan 31 03:04:53 2022
batch 60, train loss = 0.44, mean loss = 0.60
Mon Jan 31 03:05:47 2022
train Loss: 0.59

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:06:38 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:07:32 2022
val Loss: 0.02

epoch 24 was done for 470.236639 seconds
Epoch 25/299
----------
Mon Jan 31 03:08:09 2022
batch 0, train loss = 0.41, mean loss = 0.41
Mon Jan 31 03:08:14 2022
batch 10, train loss = 0.41, mean loss = 0.50
Mon Jan 31 03:09:08 2022
batch 20, train loss = 0.54, mean loss = 0.53
Mon Jan 31 03:10:02 2022
batch 30, train loss = 0.47, mean loss = 0.56
Mon Jan 31 03:10:56 2022
batch 40, train loss = 0.46, mean loss = 0.56
Mon Jan 31 03:11:50 2022
batch 50, train loss = 0.50, mean loss = 0.58
Mon Jan 31 03:12:44 2022
batch 60, train loss = 0.43, mean loss = 0.58
Mon Jan 31 03:13:37 2022
train Loss: 0.57

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:14:28 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:15:22 2022
val Loss: 0.02

epoch 25 was done for 469.178712 seconds
Epoch 26/299
----------
Mon Jan 31 03:15:58 2022
batch 0, train loss = 0.39, mean loss = 0.39
Mon Jan 31 03:16:04 2022
batch 10, train loss = 0.40, mean loss = 0.48
Mon Jan 31 03:16:58 2022
batch 20, train loss = 0.50, mean loss = 0.51
Mon Jan 31 03:17:52 2022
batch 30, train loss = 0.47, mean loss = 0.54
Mon Jan 31 03:18:46 2022
batch 40, train loss = 0.44, mean loss = 0.54
Mon Jan 31 03:19:40 2022
batch 50, train loss = 0.48, mean loss = 0.56
Mon Jan 31 03:20:33 2022
batch 60, train loss = 0.42, mean loss = 0.56
Mon Jan 31 03:21:27 2022
train Loss: 0.56

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:22:17 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:23:11 2022
val Loss: 0.02

epoch 26 was done for 469.884598 seconds
Epoch 27/299
----------
Mon Jan 31 03:23:48 2022
batch 0, train loss = 0.37, mean loss = 0.37
Mon Jan 31 03:23:54 2022
batch 10, train loss = 0.40, mean loss = 0.47
Mon Jan 31 03:24:47 2022
batch 20, train loss = 0.48, mean loss = 0.49
Mon Jan 31 03:25:41 2022
batch 30, train loss = 0.47, mean loss = 0.53
Mon Jan 31 03:26:35 2022
batch 40, train loss = 0.43, mean loss = 0.53
Mon Jan 31 03:27:29 2022
batch 50, train loss = 0.47, mean loss = 0.54
Mon Jan 31 03:28:23 2022
batch 60, train loss = 0.41, mean loss = 0.54
Mon Jan 31 03:29:17 2022
train Loss: 0.54

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:30:07 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:31:02 2022
val Loss: 0.02

epoch 27 was done for 470.659865 seconds
Epoch 28/299
----------
Mon Jan 31 03:31:39 2022
batch 0, train loss = 0.36, mean loss = 0.36
Mon Jan 31 03:31:44 2022
batch 10, train loss = 0.40, mean loss = 0.46
Mon Jan 31 03:32:38 2022
batch 20, train loss = 0.46, mean loss = 0.48
Mon Jan 31 03:33:33 2022
batch 30, train loss = 0.46, mean loss = 0.51
Mon Jan 31 03:34:27 2022
batch 40, train loss = 0.41, mean loss = 0.51
Mon Jan 31 03:35:21 2022
batch 50, train loss = 0.46, mean loss = 0.53
Mon Jan 31 03:36:15 2022
batch 60, train loss = 0.39, mean loss = 0.53
Mon Jan 31 03:37:09 2022
train Loss: 0.53

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:37:59 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:38:54 2022
val Loss: 0.02

epoch 28 was done for 472.296645 seconds
Epoch 29/299
----------
Mon Jan 31 03:39:31 2022
batch 0, train loss = 0.36, mean loss = 0.36
Mon Jan 31 03:39:37 2022
batch 10, train loss = 0.39, mean loss = 0.45
Mon Jan 31 03:40:30 2022
batch 20, train loss = 0.45, mean loss = 0.47
Mon Jan 31 03:41:24 2022
batch 30, train loss = 0.46, mean loss = 0.50
Mon Jan 31 03:42:18 2022
batch 40, train loss = 0.39, mean loss = 0.50
Mon Jan 31 03:43:12 2022
batch 50, train loss = 0.45, mean loss = 0.52
Mon Jan 31 03:44:05 2022
batch 60, train loss = 0.38, mean loss = 0.52
Mon Jan 31 03:44:59 2022
train Loss: 0.51

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:45:49 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:46:43 2022
val Loss: 0.02

epoch 29 was done for 469.200604 seconds
Epoch 30/299
----------
Mon Jan 31 03:47:20 2022
batch 0, train loss = 0.35, mean loss = 0.35
Mon Jan 31 03:47:26 2022
batch 10, train loss = 0.39, mean loss = 0.44
Mon Jan 31 03:48:20 2022
batch 20, train loss = 0.43, mean loss = 0.46
Mon Jan 31 03:49:14 2022
batch 30, train loss = 0.45, mean loss = 0.49
Mon Jan 31 03:50:08 2022
batch 40, train loss = 0.38, mean loss = 0.49
Mon Jan 31 03:51:03 2022
batch 50, train loss = 0.44, mean loss = 0.51
Mon Jan 31 03:51:57 2022
batch 60, train loss = 0.37, mean loss = 0.51
Mon Jan 31 03:52:51 2022
train Loss: 0.50

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:53:42 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 03:54:37 2022
val Loss: 0.02

epoch 30 was done for 473.536067 seconds
Epoch 31/299
----------
Mon Jan 31 03:55:14 2022
batch 0, train loss = 0.34, mean loss = 0.34
Mon Jan 31 03:55:19 2022
batch 10, train loss = 0.38, mean loss = 0.43
Mon Jan 31 03:56:13 2022
batch 20, train loss = 0.42, mean loss = 0.45
Mon Jan 31 03:57:07 2022
batch 30, train loss = 0.44, mean loss = 0.48
Mon Jan 31 03:58:01 2022
batch 40, train loss = 0.36, mean loss = 0.48
Mon Jan 31 03:58:55 2022
batch 50, train loss = 0.43, mean loss = 0.50
Mon Jan 31 03:59:48 2022
batch 60, train loss = 0.36, mean loss = 0.50
Mon Jan 31 04:00:42 2022
train Loss: 0.49

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:01:32 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:02:26 2022
val Loss: 0.02

epoch 31 was done for 469.297047 seconds
Epoch 32/299
----------
Mon Jan 31 04:03:03 2022
batch 0, train loss = 0.34, mean loss = 0.34
Mon Jan 31 04:03:09 2022
batch 10, train loss = 0.38, mean loss = 0.43
Mon Jan 31 04:04:02 2022
batch 20, train loss = 0.41, mean loss = 0.45
Mon Jan 31 04:04:57 2022
batch 30, train loss = 0.43, mean loss = 0.48
Mon Jan 31 04:05:51 2022
batch 40, train loss = 0.35, mean loss = 0.47
Mon Jan 31 04:06:45 2022
batch 50, train loss = 0.42, mean loss = 0.49
Mon Jan 31 04:07:38 2022
batch 60, train loss = 0.35, mean loss = 0.49
Mon Jan 31 04:08:33 2022
train Loss: 0.48

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:09:23 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:10:17 2022
val Loss: 0.02

epoch 32 was done for 471.027675 seconds
Epoch 33/299
----------
Mon Jan 31 04:10:54 2022
batch 0, train loss = 0.33, mean loss = 0.33
Mon Jan 31 04:11:00 2022
batch 10, train loss = 0.38, mean loss = 0.42
Mon Jan 31 04:11:54 2022
batch 20, train loss = 0.40, mean loss = 0.44
Mon Jan 31 04:12:48 2022
batch 30, train loss = 0.43, mean loss = 0.47
Mon Jan 31 04:13:42 2022
batch 40, train loss = 0.34, mean loss = 0.47
Mon Jan 31 04:14:36 2022
batch 50, train loss = 0.41, mean loss = 0.48
Mon Jan 31 04:15:30 2022
batch 60, train loss = 0.35, mean loss = 0.48
Mon Jan 31 04:16:25 2022
train Loss: 0.48

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:17:15 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:18:10 2022
val Loss: 0.02

epoch 33 was done for 473.172691 seconds
Epoch 34/299
----------
Mon Jan 31 04:18:47 2022
batch 0, train loss = 0.33, mean loss = 0.33
Mon Jan 31 04:18:53 2022
batch 10, train loss = 0.38, mean loss = 0.42
Mon Jan 31 04:19:46 2022
batch 20, train loss = 0.40, mean loss = 0.44
Mon Jan 31 04:20:40 2022
batch 30, train loss = 0.42, mean loss = 0.46
Mon Jan 31 04:21:34 2022
batch 40, train loss = 0.33, mean loss = 0.46
Mon Jan 31 04:22:28 2022
batch 50, train loss = 0.41, mean loss = 0.47
Mon Jan 31 04:23:21 2022
batch 60, train loss = 0.34, mean loss = 0.47
Mon Jan 31 04:24:15 2022
train Loss: 0.47

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:25:06 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:25:59 2022
val Loss: 0.02

epoch 34 was done for 469.221472 seconds
Epoch 35/299
----------
Mon Jan 31 04:26:37 2022
batch 0, train loss = 0.33, mean loss = 0.33
Mon Jan 31 04:26:42 2022
batch 10, train loss = 0.38, mean loss = 0.41
Mon Jan 31 04:27:36 2022
batch 20, train loss = 0.39, mean loss = 0.43
Mon Jan 31 04:28:30 2022
batch 30, train loss = 0.42, mean loss = 0.46
Mon Jan 31 04:29:24 2022
batch 40, train loss = 0.33, mean loss = 0.45
Mon Jan 31 04:30:18 2022
batch 50, train loss = 0.40, mean loss = 0.47
Mon Jan 31 04:31:11 2022
batch 60, train loss = 0.33, mean loss = 0.46
Mon Jan 31 04:32:05 2022
train Loss: 0.46

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:32:55 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:33:50 2022
val Loss: 0.02

epoch 35 was done for 469.962404 seconds
Epoch 36/299
----------
Mon Jan 31 04:34:27 2022
batch 0, train loss = 0.32, mean loss = 0.32
Mon Jan 31 04:34:32 2022
batch 10, train loss = 0.38, mean loss = 0.41
Mon Jan 31 04:35:25 2022
batch 20, train loss = 0.38, mean loss = 0.43
Mon Jan 31 04:36:19 2022
batch 30, train loss = 0.42, mean loss = 0.45
Mon Jan 31 04:37:13 2022
batch 40, train loss = 0.32, mean loss = 0.45
Mon Jan 31 04:38:07 2022
batch 50, train loss = 0.40, mean loss = 0.46
Mon Jan 31 04:39:00 2022
batch 60, train loss = 0.33, mean loss = 0.46
Mon Jan 31 04:39:54 2022
train Loss: 0.46

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:40:44 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:41:39 2022
val Loss: 0.02

epoch 36 was done for 469.261397 seconds
Epoch 37/299
----------
Mon Jan 31 04:42:16 2022
batch 0, train loss = 0.32, mean loss = 0.32
Mon Jan 31 04:42:21 2022
batch 10, train loss = 0.38, mean loss = 0.40
Mon Jan 31 04:43:15 2022
batch 20, train loss = 0.38, mean loss = 0.42
Mon Jan 31 04:44:09 2022
batch 30, train loss = 0.41, mean loss = 0.45
Mon Jan 31 04:45:02 2022
batch 40, train loss = 0.32, mean loss = 0.44
Mon Jan 31 04:45:56 2022
batch 50, train loss = 0.39, mean loss = 0.45
Mon Jan 31 04:46:50 2022
batch 60, train loss = 0.32, mean loss = 0.45
Mon Jan 31 04:47:44 2022
train Loss: 0.45

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:48:34 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:49:28 2022
val Loss: 0.02

epoch 37 was done for 469.137441 seconds
Epoch 38/299
----------
Mon Jan 31 04:50:05 2022
batch 0, train loss = 0.31, mean loss = 0.31
Mon Jan 31 04:50:10 2022
batch 10, train loss = 0.38, mean loss = 0.40
Mon Jan 31 04:51:05 2022
batch 20, train loss = 0.37, mean loss = 0.42
Mon Jan 31 04:51:59 2022
batch 30, train loss = 0.41, mean loss = 0.44
Mon Jan 31 04:52:53 2022
batch 40, train loss = 0.31, mean loss = 0.44
Mon Jan 31 04:53:47 2022
batch 50, train loss = 0.39, mean loss = 0.45
Mon Jan 31 04:54:40 2022
batch 60, train loss = 0.32, mean loss = 0.45
Mon Jan 31 04:55:35 2022
train Loss: 0.45

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:56:25 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 04:57:20 2022
val Loss: 0.02

epoch 38 was done for 471.819673 seconds
Epoch 39/299
----------
Mon Jan 31 04:57:57 2022
batch 0, train loss = 0.31, mean loss = 0.31
Mon Jan 31 04:58:02 2022
batch 10, train loss = 0.38, mean loss = 0.40
Mon Jan 31 04:58:56 2022
batch 20, train loss = 0.37, mean loss = 0.42
Mon Jan 31 04:59:50 2022
batch 30, train loss = 0.41, mean loss = 0.44
Mon Jan 31 05:00:44 2022
batch 40, train loss = 0.31, mean loss = 0.44
Mon Jan 31 05:01:38 2022
batch 50, train loss = 0.39, mean loss = 0.45
Mon Jan 31 05:02:32 2022
batch 60, train loss = 0.32, mean loss = 0.44
Mon Jan 31 05:03:26 2022
train Loss: 0.44

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:04:17 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:05:11 2022
val Loss: 0.02

epoch 39 was done for 470.585456 seconds
Epoch 40/299
----------
Mon Jan 31 05:05:47 2022
batch 0, train loss = 0.31, mean loss = 0.31
Mon Jan 31 05:05:53 2022
batch 10, train loss = 0.38, mean loss = 0.39
Mon Jan 31 05:06:47 2022
batch 20, train loss = 0.36, mean loss = 0.41
Mon Jan 31 05:07:42 2022
batch 30, train loss = 0.40, mean loss = 0.44
Mon Jan 31 05:08:36 2022
batch 40, train loss = 0.30, mean loss = 0.43
Mon Jan 31 05:09:31 2022
batch 50, train loss = 0.38, mean loss = 0.44
Mon Jan 31 05:10:25 2022
batch 60, train loss = 0.31, mean loss = 0.44
Mon Jan 31 05:11:19 2022
train Loss: 0.44

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:12:10 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:13:04 2022
val Loss: 0.02

epoch 40 was done for 474.285715 seconds
Epoch 41/299
----------
Mon Jan 31 05:13:42 2022
batch 0, train loss = 0.30, mean loss = 0.30
Mon Jan 31 05:13:47 2022
batch 10, train loss = 0.38, mean loss = 0.39
Mon Jan 31 05:14:42 2022
batch 20, train loss = 0.36, mean loss = 0.41
Mon Jan 31 05:15:36 2022
batch 30, train loss = 0.40, mean loss = 0.43
Mon Jan 31 05:16:31 2022
batch 40, train loss = 0.30, mean loss = 0.43
Mon Jan 31 05:17:25 2022
batch 50, train loss = 0.38, mean loss = 0.44
Mon Jan 31 05:18:19 2022
batch 60, train loss = 0.31, mean loss = 0.44
Mon Jan 31 05:19:14 2022
train Loss: 0.43

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:20:04 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:20:59 2022
val Loss: 0.02

epoch 41 was done for 475.047661 seconds
Epoch 42/299
----------
Mon Jan 31 05:21:37 2022
batch 0, train loss = 0.30, mean loss = 0.30
Mon Jan 31 05:21:42 2022
batch 10, train loss = 0.38, mean loss = 0.39
Mon Jan 31 05:22:36 2022
batch 20, train loss = 0.35, mean loss = 0.41
Mon Jan 31 05:23:30 2022
batch 30, train loss = 0.40, mean loss = 0.43
Mon Jan 31 05:24:24 2022
batch 40, train loss = 0.30, mean loss = 0.43
Mon Jan 31 05:25:18 2022
batch 50, train loss = 0.38, mean loss = 0.43
Mon Jan 31 05:26:11 2022
batch 60, train loss = 0.31, mean loss = 0.43
Mon Jan 31 05:27:06 2022
train Loss: 0.43

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:27:56 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:28:50 2022
val Loss: 0.02

epoch 42 was done for 469.685906 seconds
Epoch 43/299
----------
Mon Jan 31 05:29:26 2022
batch 0, train loss = 0.30, mean loss = 0.30
Mon Jan 31 05:29:32 2022
batch 10, train loss = 0.38, mean loss = 0.39
Mon Jan 31 05:30:25 2022
batch 20, train loss = 0.35, mean loss = 0.41
Mon Jan 31 05:31:19 2022
batch 30, train loss = 0.39, mean loss = 0.43
Mon Jan 31 05:32:13 2022
batch 40, train loss = 0.30, mean loss = 0.42
Mon Jan 31 05:33:07 2022
batch 50, train loss = 0.38, mean loss = 0.43
Mon Jan 31 05:34:00 2022
batch 60, train loss = 0.31, mean loss = 0.43
Mon Jan 31 05:34:54 2022
train Loss: 0.43

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:35:45 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:36:39 2022
val Loss: 0.02

epoch 43 was done for 469.103709 seconds
Epoch 44/299
----------
Mon Jan 31 05:37:15 2022
batch 0, train loss = 0.30, mean loss = 0.30
Mon Jan 31 05:37:21 2022
batch 10, train loss = 0.38, mean loss = 0.39
Mon Jan 31 05:38:15 2022
batch 20, train loss = 0.35, mean loss = 0.40
Mon Jan 31 05:39:10 2022
batch 30, train loss = 0.39, mean loss = 0.42
Mon Jan 31 05:40:04 2022
batch 40, train loss = 0.30, mean loss = 0.42
Mon Jan 31 05:40:59 2022
batch 50, train loss = 0.37, mean loss = 0.43
Mon Jan 31 05:41:53 2022
batch 60, train loss = 0.31, mean loss = 0.43
Mon Jan 31 05:42:48 2022
train Loss: 0.43

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:43:38 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:44:33 2022
val Loss: 0.02

epoch 44 was done for 475.329601 seconds
Epoch 45/299
----------
Mon Jan 31 05:45:11 2022
batch 0, train loss = 0.29, mean loss = 0.29
Mon Jan 31 05:45:16 2022
batch 10, train loss = 0.38, mean loss = 0.39
Mon Jan 31 05:46:10 2022
batch 20, train loss = 0.34, mean loss = 0.40
Mon Jan 31 05:47:04 2022
batch 30, train loss = 0.38, mean loss = 0.42
Mon Jan 31 05:47:57 2022
batch 40, train loss = 0.30, mean loss = 0.42
Mon Jan 31 05:48:51 2022
batch 50, train loss = 0.37, mean loss = 0.43
Mon Jan 31 05:49:45 2022
batch 60, train loss = 0.31, mean loss = 0.43
Mon Jan 31 05:50:39 2022
train Loss: 0.42

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:51:29 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:52:23 2022
val Loss: 0.02

epoch 45 was done for 469.849112 seconds
Epoch 46/299
----------
Mon Jan 31 05:53:01 2022
batch 0, train loss = 0.29, mean loss = 0.29
Mon Jan 31 05:53:06 2022
batch 10, train loss = 0.38, mean loss = 0.38
Mon Jan 31 05:54:00 2022
batch 20, train loss = 0.34, mean loss = 0.40
Mon Jan 31 05:54:54 2022
batch 30, train loss = 0.38, mean loss = 0.42
Mon Jan 31 05:55:48 2022
batch 40, train loss = 0.29, mean loss = 0.42
Mon Jan 31 05:56:42 2022
batch 50, train loss = 0.37, mean loss = 0.42
Mon Jan 31 05:57:36 2022
batch 60, train loss = 0.31, mean loss = 0.42
Mon Jan 31 05:58:30 2022
train Loss: 0.42

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 05:59:20 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:00:14 2022
val Loss: 0.02

epoch 46 was done for 470.737476 seconds
Epoch 47/299
----------
Mon Jan 31 06:00:51 2022
batch 0, train loss = 0.29, mean loss = 0.29
Mon Jan 31 06:00:57 2022
batch 10, train loss = 0.38, mean loss = 0.38
Mon Jan 31 06:01:51 2022
batch 20, train loss = 0.34, mean loss = 0.40
Mon Jan 31 06:02:45 2022
batch 30, train loss = 0.38, mean loss = 0.42
Mon Jan 31 06:03:39 2022
batch 40, train loss = 0.29, mean loss = 0.41
Mon Jan 31 06:04:34 2022
batch 50, train loss = 0.37, mean loss = 0.42
Mon Jan 31 06:05:28 2022
batch 60, train loss = 0.31, mean loss = 0.42
Mon Jan 31 06:06:22 2022
train Loss: 0.42

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:07:13 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:08:07 2022
val Loss: 0.02

epoch 47 was done for 473.210232 seconds
Epoch 48/299
----------
Mon Jan 31 06:08:45 2022
batch 0, train loss = 0.29, mean loss = 0.29
Mon Jan 31 06:08:50 2022
batch 10, train loss = 0.38, mean loss = 0.38
Mon Jan 31 06:09:44 2022
batch 20, train loss = 0.33, mean loss = 0.40
Mon Jan 31 06:10:38 2022
batch 30, train loss = 0.38, mean loss = 0.41
Mon Jan 31 06:11:32 2022
batch 40, train loss = 0.29, mean loss = 0.41
Mon Jan 31 06:12:26 2022
batch 50, train loss = 0.37, mean loss = 0.42
Mon Jan 31 06:13:19 2022
batch 60, train loss = 0.30, mean loss = 0.42
Mon Jan 31 06:14:13 2022
train Loss: 0.42

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:15:03 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:15:57 2022
val Loss: 0.02

epoch 48 was done for 469.380004 seconds
Epoch 49/299
----------
Mon Jan 31 06:16:34 2022
batch 0, train loss = 0.29, mean loss = 0.29
Mon Jan 31 06:16:39 2022
batch 10, train loss = 0.39, mean loss = 0.38
Mon Jan 31 06:17:33 2022
batch 20, train loss = 0.33, mean loss = 0.39
Mon Jan 31 06:18:27 2022
batch 30, train loss = 0.37, mean loss = 0.41
Mon Jan 31 06:19:21 2022
batch 40, train loss = 0.29, mean loss = 0.41
Mon Jan 31 06:20:15 2022
batch 50, train loss = 0.37, mean loss = 0.42
Mon Jan 31 06:21:08 2022
batch 60, train loss = 0.30, mean loss = 0.42
Mon Jan 31 06:22:02 2022
train Loss: 0.42

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:22:52 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:23:47 2022
val Loss: 0.02

epoch 49 was done for 469.747389 seconds
Epoch 50/299
----------
Mon Jan 31 06:24:24 2022
batch 0, train loss = 0.28, mean loss = 0.28
Mon Jan 31 06:24:29 2022
batch 10, train loss = 0.39, mean loss = 0.38
Mon Jan 31 06:25:23 2022
batch 20, train loss = 0.33, mean loss = 0.39
Mon Jan 31 06:26:17 2022
batch 30, train loss = 0.37, mean loss = 0.41
Mon Jan 31 06:27:11 2022
batch 40, train loss = 0.29, mean loss = 0.41
Mon Jan 31 06:28:05 2022
batch 50, train loss = 0.37, mean loss = 0.42
Mon Jan 31 06:28:59 2022
batch 60, train loss = 0.30, mean loss = 0.42
Mon Jan 31 06:29:53 2022
train Loss: 0.41

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:30:43 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:31:37 2022
val Loss: 0.02

epoch 50 was done for 470.969621 seconds
Epoch 51/299
----------
Mon Jan 31 06:32:15 2022
batch 0, train loss = 0.28, mean loss = 0.28
Mon Jan 31 06:32:20 2022
batch 10, train loss = 0.39, mean loss = 0.38
Mon Jan 31 06:33:14 2022
batch 20, train loss = 0.32, mean loss = 0.39
Mon Jan 31 06:34:13 2022
batch 30, train loss = 0.37, mean loss = 0.41
Mon Jan 31 06:35:12 2022
batch 40, train loss = 0.29, mean loss = 0.41
Mon Jan 31 06:36:07 2022
batch 50, train loss = 0.37, mean loss = 0.41
Mon Jan 31 06:37:01 2022
batch 60, train loss = 0.30, mean loss = 0.41
Mon Jan 31 06:37:56 2022
train Loss: 0.41

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:38:46 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:39:40 2022
val Loss: 0.02

epoch 51 was done for 481.864248 seconds
Epoch 52/299
----------
Mon Jan 31 06:40:17 2022
batch 0, train loss = 0.28, mean loss = 0.28
Mon Jan 31 06:40:22 2022
batch 10, train loss = 0.39, mean loss = 0.38
Mon Jan 31 06:41:16 2022
batch 20, train loss = 0.32, mean loss = 0.39
Mon Jan 31 06:42:10 2022
batch 30, train loss = 0.37, mean loss = 0.41
Mon Jan 31 06:43:04 2022
batch 40, train loss = 0.29, mean loss = 0.40
Mon Jan 31 06:43:58 2022
batch 50, train loss = 0.37, mean loss = 0.41
Mon Jan 31 06:44:52 2022
batch 60, train loss = 0.29, mean loss = 0.41
Mon Jan 31 06:45:46 2022
train Loss: 0.41

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:46:36 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:47:31 2022
val Loss: 0.02

epoch 52 was done for 471.494307 seconds
Epoch 53/299
----------
Mon Jan 31 06:48:08 2022
batch 0, train loss = 0.28, mean loss = 0.28
Mon Jan 31 06:48:13 2022
batch 10, train loss = 0.39, mean loss = 0.37
Mon Jan 31 06:49:07 2022
batch 20, train loss = 0.32, mean loss = 0.39
Mon Jan 31 06:50:01 2022
batch 30, train loss = 0.37, mean loss = 0.40
Mon Jan 31 06:50:55 2022
batch 40, train loss = 0.29, mean loss = 0.40
Mon Jan 31 06:51:48 2022
batch 50, train loss = 0.37, mean loss = 0.41
Mon Jan 31 06:52:42 2022
batch 60, train loss = 0.29, mean loss = 0.41
Mon Jan 31 06:53:36 2022
train Loss: 0.41

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:54:26 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 06:55:19 2022
val Loss: 0.02

epoch 53 was done for 467.705661 seconds
Epoch 54/299
----------
Mon Jan 31 06:55:56 2022
batch 0, train loss = 0.29, mean loss = 0.29
Mon Jan 31 06:56:01 2022
batch 10, train loss = 0.40, mean loss = 0.37
Mon Jan 31 06:56:55 2022
batch 20, train loss = 0.32, mean loss = 0.39
Mon Jan 31 06:57:49 2022
batch 30, train loss = 0.36, mean loss = 0.40
Mon Jan 31 06:58:42 2022
batch 40, train loss = 0.29, mean loss = 0.40
Mon Jan 31 06:59:36 2022
batch 50, train loss = 0.38, mean loss = 0.41
Mon Jan 31 07:00:30 2022
batch 60, train loss = 0.29, mean loss = 0.41
Mon Jan 31 07:01:24 2022
train Loss: 0.41

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:02:14 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:03:08 2022
val Loss: 0.02

epoch 54 was done for 468.958492 seconds
Epoch 55/299
----------
Mon Jan 31 07:03:45 2022
batch 0, train loss = 0.29, mean loss = 0.29
Mon Jan 31 07:03:50 2022
batch 10, train loss = 0.40, mean loss = 0.37
Mon Jan 31 07:04:44 2022
batch 20, train loss = 0.32, mean loss = 0.39
Mon Jan 31 07:05:38 2022
batch 30, train loss = 0.36, mean loss = 0.40
Mon Jan 31 07:06:32 2022
batch 40, train loss = 0.29, mean loss = 0.40
Mon Jan 31 07:07:26 2022
batch 50, train loss = 0.38, mean loss = 0.41
Mon Jan 31 07:08:20 2022
batch 60, train loss = 0.28, mean loss = 0.41
Mon Jan 31 07:09:14 2022
train Loss: 0.41

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:10:05 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:10:59 2022
val Loss: 0.02

epoch 55 was done for 472.025352 seconds
Epoch 56/299
----------
Mon Jan 31 07:11:37 2022
batch 0, train loss = 0.29, mean loss = 0.29
Mon Jan 31 07:11:42 2022
batch 10, train loss = 0.40, mean loss = 0.37
Mon Jan 31 07:12:36 2022
batch 20, train loss = 0.32, mean loss = 0.39
Mon Jan 31 07:13:30 2022
batch 30, train loss = 0.36, mean loss = 0.40
Mon Jan 31 07:14:23 2022
batch 40, train loss = 0.29, mean loss = 0.40
Mon Jan 31 07:15:17 2022
batch 50, train loss = 0.38, mean loss = 0.41
Mon Jan 31 07:16:10 2022
batch 60, train loss = 0.28, mean loss = 0.41
Mon Jan 31 07:17:04 2022
train Loss: 0.41

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:17:54 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:18:48 2022
val Loss: 0.02

epoch 56 was done for 467.902554 seconds
Epoch 57/299
----------
Mon Jan 31 07:19:25 2022
batch 0, train loss = 0.29, mean loss = 0.29
Mon Jan 31 07:19:30 2022
batch 10, train loss = 0.40, mean loss = 0.37
Mon Jan 31 07:20:24 2022
batch 20, train loss = 0.32, mean loss = 0.39
Mon Jan 31 07:21:18 2022
batch 30, train loss = 0.36, mean loss = 0.40
Mon Jan 31 07:22:12 2022
batch 40, train loss = 0.28, mean loss = 0.40
Mon Jan 31 07:23:06 2022
batch 50, train loss = 0.38, mean loss = 0.41
Mon Jan 31 07:24:00 2022
batch 60, train loss = 0.28, mean loss = 0.41
Mon Jan 31 07:24:54 2022
train Loss: 0.40

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:25:44 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:26:39 2022
val Loss: 0.02

epoch 57 was done for 471.586100 seconds
Epoch 58/299
----------
Mon Jan 31 07:27:16 2022
batch 0, train loss = 0.29, mean loss = 0.29
Mon Jan 31 07:27:22 2022
batch 10, train loss = 0.40, mean loss = 0.37
Mon Jan 31 07:28:16 2022
batch 20, train loss = 0.32, mean loss = 0.39
Mon Jan 31 07:29:10 2022
batch 30, train loss = 0.36, mean loss = 0.40
Mon Jan 31 07:30:05 2022
batch 40, train loss = 0.28, mean loss = 0.40
Mon Jan 31 07:30:59 2022
batch 50, train loss = 0.39, mean loss = 0.41
Mon Jan 31 07:31:55 2022
batch 60, train loss = 0.28, mean loss = 0.41
Mon Jan 31 07:32:49 2022
train Loss: 0.40

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:33:39 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:34:34 2022
val Loss: 0.02

epoch 58 was done for 475.068316 seconds
Epoch 59/299
----------
Mon Jan 31 07:35:11 2022
batch 0, train loss = 0.28, mean loss = 0.28
Mon Jan 31 07:35:17 2022
batch 10, train loss = 0.40, mean loss = 0.37
Mon Jan 31 07:36:10 2022
batch 20, train loss = 0.32, mean loss = 0.39
Mon Jan 31 07:37:04 2022
batch 30, train loss = 0.37, mean loss = 0.40
Mon Jan 31 07:37:58 2022
batch 40, train loss = 0.28, mean loss = 0.40
Mon Jan 31 07:38:52 2022
batch 50, train loss = 0.39, mean loss = 0.41
Mon Jan 31 07:39:45 2022
batch 60, train loss = 0.27, mean loss = 0.40
Mon Jan 31 07:40:47 2022
train Loss: 0.40

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:41:37 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:42:31 2022
val Loss: 0.02

epoch 59 was done for 477.102353 seconds
Epoch 60/299
----------
Mon Jan 31 07:43:08 2022
batch 0, train loss = 0.28, mean loss = 0.28
Mon Jan 31 07:43:14 2022
batch 10, train loss = 0.40, mean loss = 0.36
Mon Jan 31 07:44:08 2022
batch 20, train loss = 0.32, mean loss = 0.39
Mon Jan 31 07:45:02 2022
batch 30, train loss = 0.37, mean loss = 0.40
Mon Jan 31 07:46:01 2022
batch 40, train loss = 0.28, mean loss = 0.40
Mon Jan 31 07:46:55 2022
batch 50, train loss = 0.39, mean loss = 0.41
Mon Jan 31 07:47:51 2022
batch 60, train loss = 0.27, mean loss = 0.40
Mon Jan 31 07:48:45 2022
train Loss: 0.40

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:49:35 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:50:29 2022
val Loss: 0.02

epoch 60 was done for 478.141996 seconds
Epoch 61/299
----------
Mon Jan 31 07:51:07 2022
batch 0, train loss = 0.28, mean loss = 0.28
Mon Jan 31 07:51:12 2022
batch 10, train loss = 0.39, mean loss = 0.36
Mon Jan 31 07:52:06 2022
batch 20, train loss = 0.32, mean loss = 0.38
Mon Jan 31 07:53:00 2022
batch 30, train loss = 0.37, mean loss = 0.40
Mon Jan 31 07:53:54 2022
batch 40, train loss = 0.28, mean loss = 0.39
Mon Jan 31 07:54:48 2022
batch 50, train loss = 0.39, mean loss = 0.40
Mon Jan 31 07:55:43 2022
batch 60, train loss = 0.27, mean loss = 0.40
Mon Jan 31 07:56:38 2022
train Loss: 0.40

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:57:29 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 07:58:24 2022
val Loss: 0.02

epoch 61 was done for 475.132738 seconds
Epoch 62/299
----------
Mon Jan 31 07:59:02 2022
batch 0, train loss = 0.28, mean loss = 0.28
Mon Jan 31 07:59:07 2022
batch 10, train loss = 0.39, mean loss = 0.36
Mon Jan 31 08:00:02 2022
batch 20, train loss = 0.32, mean loss = 0.38
Mon Jan 31 08:00:57 2022
batch 30, train loss = 0.38, mean loss = 0.39
Mon Jan 31 08:01:53 2022
batch 40, train loss = 0.28, mean loss = 0.39
Mon Jan 31 08:02:54 2022
batch 50, train loss = 0.39, mean loss = 0.40
Mon Jan 31 08:04:01 2022
batch 60, train loss = 0.27, mean loss = 0.40
Mon Jan 31 08:05:01 2022
train Loss: 0.40

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:05:54 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:06:50 2022
val Loss: 0.02

epoch 62 was done for 507.882019 seconds
Epoch 63/299
----------
Mon Jan 31 08:07:30 2022
batch 0, train loss = 0.28, mean loss = 0.28
Mon Jan 31 08:07:35 2022
batch 10, train loss = 0.39, mean loss = 0.36
Mon Jan 31 08:08:30 2022
batch 20, train loss = 0.32, mean loss = 0.38
Mon Jan 31 08:09:24 2022
batch 30, train loss = 0.38, mean loss = 0.39
Mon Jan 31 08:10:19 2022
batch 40, train loss = 0.27, mean loss = 0.39
Mon Jan 31 08:11:13 2022
batch 50, train loss = 0.39, mean loss = 0.40
Mon Jan 31 08:12:06 2022
batch 60, train loss = 0.27, mean loss = 0.40
Mon Jan 31 08:13:00 2022
train Loss: 0.40

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:13:50 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:14:44 2022
val Loss: 0.02

epoch 63 was done for 471.726175 seconds
Epoch 64/299
----------
Mon Jan 31 08:15:21 2022
batch 0, train loss = 0.28, mean loss = 0.28
Mon Jan 31 08:15:27 2022
batch 10, train loss = 0.39, mean loss = 0.36
Mon Jan 31 08:16:20 2022
batch 20, train loss = 0.32, mean loss = 0.38
Mon Jan 31 08:17:14 2022
batch 30, train loss = 0.38, mean loss = 0.39
Mon Jan 31 08:18:08 2022
batch 40, train loss = 0.27, mean loss = 0.39
Mon Jan 31 08:19:01 2022
batch 50, train loss = 0.38, mean loss = 0.40
Mon Jan 31 08:19:55 2022
batch 60, train loss = 0.27, mean loss = 0.40
Mon Jan 31 08:20:49 2022
train Loss: 0.40

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:21:39 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:22:33 2022
val Loss: 0.02

epoch 64 was done for 468.921599 seconds
Epoch 65/299
----------
Mon Jan 31 08:23:10 2022
batch 0, train loss = 0.28, mean loss = 0.28
Mon Jan 31 08:23:16 2022
batch 10, train loss = 0.39, mean loss = 0.36
Mon Jan 31 08:24:09 2022
batch 20, train loss = 0.32, mean loss = 0.38
Mon Jan 31 08:25:02 2022
batch 30, train loss = 0.38, mean loss = 0.39
Mon Jan 31 08:25:56 2022
batch 40, train loss = 0.27, mean loss = 0.39
Mon Jan 31 08:26:49 2022
batch 50, train loss = 0.38, mean loss = 0.40
Mon Jan 31 08:27:44 2022
batch 60, train loss = 0.27, mean loss = 0.40
Mon Jan 31 08:28:42 2022
train Loss: 0.40

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:29:32 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:30:29 2022
val Loss: 0.02

epoch 65 was done for 475.523009 seconds
Epoch 66/299
----------
Mon Jan 31 08:31:06 2022
batch 0, train loss = 0.28, mean loss = 0.28
Mon Jan 31 08:31:11 2022
batch 10, train loss = 0.39, mean loss = 0.36
Mon Jan 31 08:32:07 2022
batch 20, train loss = 0.32, mean loss = 0.38
Mon Jan 31 08:33:04 2022
batch 30, train loss = 0.39, mean loss = 0.39
Mon Jan 31 08:33:59 2022
batch 40, train loss = 0.27, mean loss = 0.39
Mon Jan 31 08:34:59 2022
batch 50, train loss = 0.38, mean loss = 0.40
Mon Jan 31 08:35:53 2022
batch 60, train loss = 0.27, mean loss = 0.40
Mon Jan 31 08:36:47 2022
train Loss: 0.40

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:37:43 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:38:43 2022
val Loss: 0.02

epoch 66 was done for 498.209821 seconds
Epoch 67/299
----------
Mon Jan 31 08:39:24 2022
batch 0, train loss = 0.28, mean loss = 0.28
Mon Jan 31 08:39:30 2022
batch 10, train loss = 0.39, mean loss = 0.36
Mon Jan 31 08:40:28 2022
batch 20, train loss = 0.32, mean loss = 0.38
Mon Jan 31 08:41:25 2022
batch 30, train loss = 0.39, mean loss = 0.39
Mon Jan 31 08:42:24 2022
batch 40, train loss = 0.26, mean loss = 0.39
Mon Jan 31 08:43:26 2022
batch 50, train loss = 0.38, mean loss = 0.40
Mon Jan 31 08:44:27 2022
batch 60, train loss = 0.27, mean loss = 0.40
Mon Jan 31 08:45:27 2022
train Loss: 0.40

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:46:24 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:47:24 2022
val Loss: 0.02

epoch 67 was done for 521.170354 seconds
Epoch 68/299
----------
Mon Jan 31 08:48:05 2022
batch 0, train loss = 0.28, mean loss = 0.28
Mon Jan 31 08:48:11 2022
batch 10, train loss = 0.39, mean loss = 0.36
Mon Jan 31 08:49:10 2022
batch 20, train loss = 0.32, mean loss = 0.38
Mon Jan 31 08:50:12 2022
batch 30, train loss = 0.39, mean loss = 0.39
Mon Jan 31 08:51:16 2022
batch 40, train loss = 0.26, mean loss = 0.39
Mon Jan 31 08:52:16 2022
batch 50, train loss = 0.38, mean loss = 0.40
Mon Jan 31 08:53:16 2022
batch 60, train loss = 0.27, mean loss = 0.40
Mon Jan 31 08:54:15 2022
train Loss: 0.40

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:55:09 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 08:56:03 2022
val Loss: 0.02

epoch 68 was done for 515.927069 seconds
Epoch 69/299
----------
Mon Jan 31 08:56:41 2022
batch 0, train loss = 0.28, mean loss = 0.28
Mon Jan 31 08:56:47 2022
batch 10, train loss = 0.39, mean loss = 0.36
Mon Jan 31 08:57:41 2022
batch 20, train loss = 0.32, mean loss = 0.38
Mon Jan 31 08:58:35 2022
batch 30, train loss = 0.39, mean loss = 0.39
Mon Jan 31 08:59:41 2022
batch 40, train loss = 0.26, mean loss = 0.39
Mon Jan 31 09:00:48 2022
batch 50, train loss = 0.39, mean loss = 0.40
Mon Jan 31 09:01:56 2022
batch 60, train loss = 0.26, mean loss = 0.40
Mon Jan 31 09:03:03 2022
train Loss: 0.40

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:04:07 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:05:14 2022
val Loss: 0.02

epoch 69 was done for 558.932422 seconds
Epoch 70/299
----------
Mon Jan 31 09:06:00 2022
batch 0, train loss = 0.28, mean loss = 0.28
Mon Jan 31 09:06:07 2022
batch 10, train loss = 0.39, mean loss = 0.36
Mon Jan 31 09:07:14 2022
batch 20, train loss = 0.32, mean loss = 0.38
Mon Jan 31 09:08:27 2022
batch 30, train loss = 0.40, mean loss = 0.39
Mon Jan 31 09:09:34 2022
batch 40, train loss = 0.25, mean loss = 0.39
Mon Jan 31 09:10:41 2022
batch 50, train loss = 0.39, mean loss = 0.40
Mon Jan 31 09:11:49 2022
batch 60, train loss = 0.26, mean loss = 0.40
Mon Jan 31 09:12:56 2022
train Loss: 0.40

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:13:59 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:15:06 2022
val Loss: 0.02

epoch 70 was done for 591.024976 seconds
Epoch 71/299
----------
Mon Jan 31 09:15:51 2022
batch 0, train loss = 0.29, mean loss = 0.29
Mon Jan 31 09:15:58 2022
batch 10, train loss = 0.39, mean loss = 0.36
Mon Jan 31 09:17:04 2022
batch 20, train loss = 0.32, mean loss = 0.38
Mon Jan 31 09:18:12 2022
batch 30, train loss = 0.40, mean loss = 0.39
Mon Jan 31 09:19:19 2022
batch 40, train loss = 0.25, mean loss = 0.39
Mon Jan 31 09:20:26 2022
batch 50, train loss = 0.39, mean loss = 0.40
Mon Jan 31 09:21:33 2022
batch 60, train loss = 0.26, mean loss = 0.40
Mon Jan 31 09:22:41 2022
train Loss: 0.40

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:23:44 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:24:51 2022
val Loss: 0.02

epoch 71 was done for 586.504440 seconds
Epoch 72/299
----------
Mon Jan 31 09:25:37 2022
batch 0, train loss = 0.29, mean loss = 0.29
Mon Jan 31 09:25:44 2022
batch 10, train loss = 0.39, mean loss = 0.36
Mon Jan 31 09:26:51 2022
batch 20, train loss = 0.33, mean loss = 0.39
Mon Jan 31 09:27:59 2022
batch 30, train loss = 0.40, mean loss = 0.40
Mon Jan 31 09:29:07 2022
batch 40, train loss = 0.24, mean loss = 0.39
Mon Jan 31 09:30:14 2022
batch 50, train loss = 0.39, mean loss = 0.40
Mon Jan 31 09:31:22 2022
batch 60, train loss = 0.26, mean loss = 0.40
Mon Jan 31 09:32:29 2022
train Loss: 0.40

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:33:32 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:34:39 2022
val Loss: 0.02

epoch 72 was done for 586.905144 seconds
Epoch 73/299
----------
Mon Jan 31 09:35:24 2022
batch 0, train loss = 0.29, mean loss = 0.29
Mon Jan 31 09:35:31 2022
batch 10, train loss = 0.39, mean loss = 0.37
Mon Jan 31 09:36:38 2022
batch 20, train loss = 0.33, mean loss = 0.39
Mon Jan 31 09:37:46 2022
batch 30, train loss = 0.40, mean loss = 0.40
Mon Jan 31 09:38:53 2022
batch 40, train loss = 0.24, mean loss = 0.39
Mon Jan 31 09:40:00 2022
batch 50, train loss = 0.39, mean loss = 0.40
Mon Jan 31 09:41:08 2022
batch 60, train loss = 0.25, mean loss = 0.40
Mon Jan 31 09:42:15 2022
train Loss: 0.40

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:43:18 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:44:25 2022
val Loss: 0.02

epoch 73 was done for 585.649914 seconds
Epoch 74/299
----------
Mon Jan 31 09:45:10 2022
batch 0, train loss = 0.29, mean loss = 0.29
Mon Jan 31 09:45:16 2022
batch 10, train loss = 0.39, mean loss = 0.37
Mon Jan 31 09:46:25 2022
batch 20, train loss = 0.33, mean loss = 0.39
Mon Jan 31 09:47:32 2022
batch 30, train loss = 0.39, mean loss = 0.40
Mon Jan 31 09:48:39 2022
batch 40, train loss = 0.24, mean loss = 0.40
Mon Jan 31 09:49:46 2022
batch 50, train loss = 0.39, mean loss = 0.40
Mon Jan 31 09:50:56 2022
batch 60, train loss = 0.25, mean loss = 0.40
Mon Jan 31 09:52:03 2022
train Loss: 0.40

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:53:06 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 09:54:13 2022
val Loss: 0.02

epoch 74 was done for 588.188320 seconds
Epoch 75/299
----------
Mon Jan 31 09:54:58 2022
batch 0, train loss = 0.30, mean loss = 0.30
Mon Jan 31 09:55:05 2022
batch 10, train loss = 0.39, mean loss = 0.38
Mon Jan 31 09:56:11 2022
batch 20, train loss = 0.33, mean loss = 0.39
Mon Jan 31 09:57:18 2022
batch 30, train loss = 0.39, mean loss = 0.40
Mon Jan 31 09:58:25 2022
batch 40, train loss = 0.24, mean loss = 0.40
Mon Jan 31 09:59:33 2022
batch 50, train loss = 0.39, mean loss = 0.40
Mon Jan 31 10:00:41 2022
batch 60, train loss = 0.25, mean loss = 0.40
Mon Jan 31 10:01:48 2022
train Loss: 0.40

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:02:51 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:03:58 2022
val Loss: 0.02

epoch 75 was done for 584.539749 seconds
Epoch 76/299
----------
Mon Jan 31 10:04:43 2022
batch 0, train loss = 0.33, mean loss = 0.33
Mon Jan 31 10:04:49 2022
batch 10, train loss = 0.39, mean loss = 0.39
Mon Jan 31 10:05:56 2022
batch 20, train loss = 0.34, mean loss = 0.41
Mon Jan 31 10:07:04 2022
batch 30, train loss = 0.38, mean loss = 0.42
Mon Jan 31 10:08:11 2022
batch 40, train loss = 0.24, mean loss = 0.41
Mon Jan 31 10:09:17 2022
batch 50, train loss = 0.39, mean loss = 0.41
Mon Jan 31 10:10:24 2022
batch 60, train loss = 0.26, mean loss = 0.41
Mon Jan 31 10:11:31 2022
train Loss: 0.41

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:12:34 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:13:42 2022
val Loss: 0.02

epoch 76 was done for 584.928056 seconds
Epoch 77/299
----------
Mon Jan 31 10:14:28 2022
batch 0, train loss = 0.37, mean loss = 0.37
Mon Jan 31 10:14:34 2022
batch 10, train loss = 0.40, mean loss = 0.42
Mon Jan 31 10:15:40 2022
batch 20, train loss = 0.35, mean loss = 0.43
Mon Jan 31 10:16:46 2022
batch 30, train loss = 0.36, mean loss = 0.44
Mon Jan 31 10:17:52 2022
batch 40, train loss = 0.24, mean loss = 0.42
Mon Jan 31 10:18:59 2022
batch 50, train loss = 0.40, mean loss = 0.42
Mon Jan 31 10:20:06 2022
batch 60, train loss = 0.28, mean loss = 0.42
Mon Jan 31 10:21:13 2022
train Loss: 0.43

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:22:18 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:23:25 2022
val Loss: 0.02

epoch 77 was done for 582.344777 seconds
Epoch 78/299
----------
Mon Jan 31 10:24:10 2022
batch 0, train loss = 0.47, mean loss = 0.47
Mon Jan 31 10:24:17 2022
batch 10, train loss = 0.44, mean loss = 0.49
Mon Jan 31 10:25:24 2022
batch 20, train loss = 0.36, mean loss = 0.47
Mon Jan 31 10:26:30 2022
batch 30, train loss = 0.37, mean loss = 0.47
Mon Jan 31 10:27:37 2022
batch 40, train loss = 0.25, mean loss = 0.45
Mon Jan 31 10:28:43 2022
batch 50, train loss = 0.43, mean loss = 0.45
Mon Jan 31 10:29:49 2022
batch 60, train loss = 0.34, mean loss = 0.44
Mon Jan 31 10:30:55 2022
train Loss: 0.48

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:31:58 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:33:04 2022
val Loss: 0.02

epoch 78 was done for 578.627002 seconds
Epoch 79/299
----------
Mon Jan 31 10:33:49 2022
batch 0, train loss = 0.76, mean loss = 0.76
Mon Jan 31 10:33:55 2022
batch 10, train loss = 0.60, mean loss = 0.73
Mon Jan 31 10:35:02 2022
batch 20, train loss = 0.33, mean loss = 0.60
Mon Jan 31 10:36:09 2022
batch 30, train loss = 0.40, mean loss = 0.57
Mon Jan 31 10:37:15 2022
batch 40, train loss = 0.28, mean loss = 0.53
Mon Jan 31 10:38:21 2022
batch 50, train loss = 0.45, mean loss = 0.52
Mon Jan 31 10:39:27 2022
batch 60, train loss = 0.38, mean loss = 0.51
Mon Jan 31 10:40:32 2022
train Loss: 0.55

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:41:35 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:42:41 2022
val Loss: 0.02

epoch 79 was done for 577.519302 seconds
Epoch 80/299
----------
Mon Jan 31 10:43:26 2022
batch 0, train loss = 0.92, mean loss = 0.92
Mon Jan 31 10:43:33 2022
batch 10, train loss = 0.49, mean loss = 0.74
Mon Jan 31 10:44:39 2022
batch 20, train loss = 0.60, mean loss = 0.68
Mon Jan 31 10:45:45 2022
batch 30, train loss = 0.71, mean loss = 0.76
Mon Jan 31 10:46:51 2022
batch 40, train loss = 0.43, mean loss = 0.73
Mon Jan 31 10:47:57 2022
batch 50, train loss = 0.50, mean loss = 0.69
Mon Jan 31 10:49:04 2022
batch 60, train loss = 0.34, mean loss = 0.66
Mon Jan 31 10:50:10 2022
train Loss: 0.65

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:51:12 2022
batch 10, val loss = 0.02, mean loss = 0.02
Mon Jan 31 10:52:17 2022
val Loss: 0.02

epoch 80 was done for 576.084079 seconds
Epoch 81/299
----------
Mon Jan 31 10:53:02 2022
batch 0, train loss = 0.44, mean loss = 0.44
Mon Jan 31 10:53:09 2022
batch 10, train loss = 0.87, mean loss = 0.49
Mon Jan 31 10:54:14 2022
batch 20, train loss = 2.21, mean loss = 1.24
Mon Jan 31 10:55:18 2022
batch 30, train loss = 0.74, mean loss = 1.45
Mon Jan 31 10:56:25 2022
batch 40, train loss = 0.50, mean loss = 1.26
Mon Jan 31 10:57:31 2022
batch 50, train loss = 5.72, mean loss = 1.58
Mon Jan 31 10:58:36 2022
batch 60, train loss = 8.98, mean loss = 2.66
Mon Jan 31 10:59:41 2022
train Loss: 3.54

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 11:00:41 2022
batch 10, val loss = 0.01, mean loss = 0.02
Mon Jan 31 11:01:46 2022
val Loss: 0.02

epoch 81 was done for 566.535405 seconds
Epoch 82/299
----------
Mon Jan 31 11:02:29 2022
batch 0, train loss = 5.60, mean loss = 5.60
Mon Jan 31 11:02:35 2022
batch 10, train loss = 3.16, mean loss = 2.04
Mon Jan 31 11:03:40 2022
batch 20, train loss = 1.54, mean loss = 2.55
Mon Jan 31 11:04:44 2022
batch 30, train loss = 7.91, mean loss = 2.86
Mon Jan 31 11:05:44 2022
batch 40, train loss = 2.88, mean loss = 3.33
Mon Jan 31 11:06:46 2022
batch 50, train loss = 5.05, mean loss = 3.35
Mon Jan 31 11:07:42 2022
batch 60, train loss = 1.80, mean loss = 3.26
Mon Jan 31 11:08:36 2022
train Loss: 3.05

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 11:09:26 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 11:10:30 2022
val Loss: 0.03

epoch 82 was done for 527.851834 seconds
Epoch 83/299
----------
Mon Jan 31 11:11:17 2022
batch 0, train loss = 0.94, mean loss = 0.94
Mon Jan 31 11:11:23 2022
batch 10, train loss = 1.14, mean loss = 1.00
Mon Jan 31 11:12:31 2022
batch 20, train loss = 1.11, mean loss = 0.99
Mon Jan 31 11:13:39 2022
batch 30, train loss = 0.62, mean loss = 1.02
Mon Jan 31 11:14:50 2022
batch 40, train loss = 2.87, mean loss = 1.26
Mon Jan 31 11:15:58 2022
batch 50, train loss = 1.91, mean loss = 1.32
Mon Jan 31 11:17:07 2022
batch 60, train loss = 1.30, mean loss = 1.44
Mon Jan 31 11:18:17 2022
train Loss: 1.51

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 11:19:22 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 11:20:32 2022
val Loss: 0.03

epoch 83 was done for 601.328163 seconds
Epoch 84/299
----------
Mon Jan 31 11:21:18 2022
batch 0, train loss = 3.31, mean loss = 3.31
Mon Jan 31 11:21:25 2022
batch 10, train loss = 2.61, mean loss = 1.70
Mon Jan 31 11:22:33 2022
batch 20, train loss = 0.65, mean loss = 1.59
Mon Jan 31 11:23:42 2022
batch 30, train loss = 0.90, mean loss = 1.52
Mon Jan 31 11:24:50 2022
batch 40, train loss = 1.03, mean loss = 1.42
Mon Jan 31 11:25:59 2022
batch 50, train loss = 0.80, mean loss = 1.32
Mon Jan 31 11:27:07 2022
batch 60, train loss = 0.79, mean loss = 1.26
Mon Jan 31 11:28:17 2022
train Loss: 1.22

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 11:29:21 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 11:30:30 2022
val Loss: 0.03

epoch 84 was done for 597.791090 seconds
Epoch 85/299
----------
Mon Jan 31 11:31:16 2022
batch 0, train loss = 0.59, mean loss = 0.59
Mon Jan 31 11:31:23 2022
batch 10, train loss = 0.69, mean loss = 0.83
Mon Jan 31 11:32:32 2022
batch 20, train loss = 1.01, mean loss = 0.85
Mon Jan 31 11:33:41 2022
batch 30, train loss = 1.64, mean loss = 0.90
Mon Jan 31 11:34:50 2022
batch 40, train loss = 0.58, mean loss = 0.92
Mon Jan 31 11:35:59 2022
batch 50, train loss = 0.73, mean loss = 0.93
Mon Jan 31 11:37:07 2022
batch 60, train loss = 0.38, mean loss = 0.87
Mon Jan 31 11:38:16 2022
train Loss: 0.86

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 11:39:20 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 11:40:30 2022
val Loss: 0.03

epoch 85 was done for 600.191790 seconds
Epoch 86/299
----------
Mon Jan 31 11:41:16 2022
batch 0, train loss = 0.56, mean loss = 0.56
Mon Jan 31 11:41:23 2022
batch 10, train loss = 0.75, mean loss = 0.64
Mon Jan 31 11:42:35 2022
batch 20, train loss = 0.45, mean loss = 0.62
Mon Jan 31 11:43:43 2022
batch 30, train loss = 0.89, mean loss = 0.64
Mon Jan 31 11:44:52 2022
batch 40, train loss = 0.82, mean loss = 0.66
Mon Jan 31 11:46:00 2022
batch 50, train loss = 0.78, mean loss = 0.70
Mon Jan 31 11:47:10 2022
batch 60, train loss = 0.63, mean loss = 0.72
Mon Jan 31 11:48:20 2022
train Loss: 0.74

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 11:49:24 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 11:50:33 2022
val Loss: 0.03

epoch 86 was done for 603.592573 seconds
Epoch 87/299
----------
Mon Jan 31 11:51:20 2022
batch 0, train loss = 0.78, mean loss = 0.78
Mon Jan 31 11:51:27 2022
batch 10, train loss = 0.91, mean loss = 0.79
Mon Jan 31 11:52:36 2022
batch 20, train loss = 0.54, mean loss = 0.74
Mon Jan 31 11:53:46 2022
batch 30, train loss = 0.75, mean loss = 0.74
Mon Jan 31 11:54:57 2022
batch 40, train loss = 0.87, mean loss = 0.75
Mon Jan 31 11:56:07 2022
batch 50, train loss = 0.93, mean loss = 0.76
Mon Jan 31 11:57:17 2022
batch 60, train loss = 0.40, mean loss = 0.75
Mon Jan 31 11:58:27 2022
train Loss: 0.76

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 11:59:32 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 12:00:41 2022
val Loss: 0.03

epoch 87 was done for 608.823442 seconds
Epoch 88/299
----------
Mon Jan 31 12:01:28 2022
batch 0, train loss = 0.58, mean loss = 0.58
Mon Jan 31 12:01:35 2022
batch 10, train loss = 0.71, mean loss = 0.73
Mon Jan 31 12:02:45 2022
batch 20, train loss = 0.66, mean loss = 0.75
Mon Jan 31 12:03:54 2022
batch 30, train loss = 1.31, mean loss = 0.82
Mon Jan 31 12:05:14 2022
batch 40, train loss = 0.70, mean loss = 0.84
Mon Jan 31 12:06:25 2022
batch 50, train loss = 0.95, mean loss = 0.87
Mon Jan 31 12:07:34 2022
batch 60, train loss = 0.47, mean loss = 0.84
Mon Jan 31 12:08:43 2022
train Loss: 0.84

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 12:09:48 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 12:10:58 2022
val Loss: 0.03

epoch 88 was done for 615.425510 seconds
Epoch 89/299
----------
Mon Jan 31 12:11:44 2022
batch 0, train loss = 0.49, mean loss = 0.49
Mon Jan 31 12:11:51 2022
batch 10, train loss = 0.77, mean loss = 0.74
Mon Jan 31 12:12:59 2022
batch 20, train loss = 0.61, mean loss = 0.73
Mon Jan 31 12:14:08 2022
batch 30, train loss = 0.90, mean loss = 0.85
Mon Jan 31 12:15:16 2022
batch 40, train loss = 1.41, mean loss = 0.92
Mon Jan 31 12:16:26 2022
batch 50, train loss = 1.07, mean loss = 0.97
Mon Jan 31 12:17:35 2022
batch 60, train loss = 1.06, mean loss = 1.02
Mon Jan 31 12:18:45 2022
train Loss: 1.03

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 12:19:49 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 12:20:59 2022
val Loss: 0.03

epoch 89 was done for 601.157619 seconds
Epoch 90/299
----------
Mon Jan 31 12:21:45 2022
batch 0, train loss = 1.07, mean loss = 1.07
Mon Jan 31 12:21:52 2022
batch 10, train loss = 0.83, mean loss = 1.13
Mon Jan 31 12:23:01 2022
batch 20, train loss = 1.04, mean loss = 1.11
Mon Jan 31 12:24:10 2022
batch 30, train loss = 1.44, mean loss = 1.11
Mon Jan 31 12:25:20 2022
batch 40, train loss = 1.34, mean loss = 1.22
Mon Jan 31 12:26:28 2022
batch 50, train loss = 1.98, mean loss = 1.23
Mon Jan 31 12:27:37 2022
batch 60, train loss = 0.62, mean loss = 1.26
Mon Jan 31 12:28:46 2022
train Loss: 1.32

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 12:29:50 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 12:30:59 2022
val Loss: 0.03

epoch 90 was done for 603.459929 seconds
Epoch 91/299
----------
Mon Jan 31 12:31:48 2022
batch 0, train loss = 1.62, mean loss = 1.62
Mon Jan 31 12:31:55 2022
batch 10, train loss = 1.75, mean loss = 1.28
Mon Jan 31 12:33:03 2022
batch 20, train loss = 0.85, mean loss = 1.32
Mon Jan 31 12:34:12 2022
batch 30, train loss = 1.81, mean loss = 1.47
Mon Jan 31 12:35:20 2022
batch 40, train loss = 1.61, mean loss = 1.47
Mon Jan 31 12:36:29 2022
batch 50, train loss = 1.30, mean loss = 1.54
Mon Jan 31 12:37:37 2022
batch 60, train loss = 1.71, mean loss = 1.56
Mon Jan 31 12:38:47 2022
train Loss: 1.58

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 12:39:57 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 12:41:06 2022
val Loss: 0.03

epoch 91 was done for 603.768346 seconds
Epoch 92/299
----------
Mon Jan 31 12:41:52 2022
batch 0, train loss = 1.15, mean loss = 1.15
Mon Jan 31 12:41:59 2022
batch 10, train loss = 0.95, mean loss = 1.70
Mon Jan 31 12:43:07 2022
batch 20, train loss = 1.73, mean loss = 1.69
Mon Jan 31 12:44:15 2022
batch 30, train loss = 2.13, mean loss = 1.75
Mon Jan 31 12:45:25 2022
batch 40, train loss = 2.61, mean loss = 1.90
Mon Jan 31 12:46:33 2022
batch 50, train loss = 3.25, mean loss = 1.93
Mon Jan 31 12:47:42 2022
batch 60, train loss = 0.81, mean loss = 1.94
Mon Jan 31 12:48:51 2022
train Loss: 2.03

batch 0, val loss = 0.03, mean loss = 0.03
Mon Jan 31 12:49:57 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 12:51:05 2022
val Loss: 0.03

epoch 92 was done for 599.150386 seconds
Epoch 93/299
----------
Mon Jan 31 12:51:51 2022
batch 0, train loss = 0.87, mean loss = 0.87
Mon Jan 31 12:51:58 2022
batch 10, train loss = 1.09, mean loss = 1.99
Mon Jan 31 12:53:09 2022
batch 20, train loss = 1.11, mean loss = 2.01
Mon Jan 31 12:54:19 2022
batch 30, train loss = 1.26, mean loss = 2.47
Mon Jan 31 12:55:27 2022
batch 40, train loss = 5.13, mean loss = 2.80
Mon Jan 31 12:56:36 2022
batch 50, train loss = 5.35, mean loss = 2.85
Mon Jan 31 12:57:45 2022
batch 60, train loss = 3.50, mean loss = 2.88
Mon Jan 31 12:58:53 2022
train Loss: 2.92

batch 0, val loss = 0.02, mean loss = 0.02
Mon Jan 31 12:59:57 2022
batch 10, val loss = 0.03, mean loss = 0.03
Mon Jan 31 13:01:07 2022
val Loss: 0.02

epoch 93 was done for 602.835666 seconds
Epoch 94/299
----------
Mon Jan 31 13:01:54 2022
batch 0, train loss = 3.78, mean loss = 3.78
Mon Jan 31 13:02:01 2022
batch 10, train loss = 5.24, mean loss = 2.90
Mon Jan 31 13:03:11 2022
batch 20, train loss = 3.12, mean loss = 3.11
Mon Jan 31 13:04:20 2022
batch 30, train loss = 4.18, mean loss = 3.62
Mon Jan 31 13:05:30 2022
