{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainpath = '/p/home/jusers/cherepashkin1/jureca/cherepashkin1/598csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if not(os.path.isfile('tmean.csv') and os.path.isfile(mainpath+'/sh_paramters.csv')):\n",
    "def rewalk(mainpath, lastfilename):\n",
    "    cip = []\n",
    "    for root, directories, filenames in os.walk(mainpath): \n",
    "        for filename in filenames:\n",
    "            if filename[-len(lastfilename):] == lastfilename:\n",
    "                cip.append(os.path.join(root,filename))\n",
    "    return(cip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cip = []\n",
    "lastfilename = 'Y_N'\n",
    "for root, directories, filenames in os.walk(mainpath): \n",
    "    for filename in filenames:\n",
    "        if lastfilename in filename:\n",
    "            os.remove(os.path.join(root,filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cip = []\n",
    "lastfilename = 'bX'\n",
    "for root, directories, filenames in os.walk(mainpath): \n",
    "    for filename in filenames:\n",
    "        if lastfilename in filename:\n",
    "            os.remove(os.path.join(root,filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_n = np.genfromtxt(os.path.join('/p/home/jusers/cherepashkin1/jureca/cherepashkin1/',\\\n",
    "                                              str(598)+'csv','Y_N_'+str(1000)+'.csv').replace('\\\\','/'),delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_n = np.expand_dims(y_n, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_n = np.repeat(y_n, 40, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_between = 'f+f_n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = []\n",
    "l = []\n",
    "if loss_between == 'f' or loss_between == 'f+f_n':\n",
    "    f = 1\n",
    "elif loss_between == 'f_n':\n",
    "    l = 0\n",
    "if loss_between == 'f':\n",
    "    l = f\n",
    "elif loss_between == 'f+f_n':\n",
    "    l = 2*f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_n = torch.zeros([1,500, 441])\n",
    "y_n = y_n.repeat(40,1,1)\n",
    "y_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outputs = torch.zeros([40, 441])\n",
    "f = torch.zeros([40,500])\n",
    "for i in range(outputs.shape[0]):\n",
    "    f[i] = torch.matmul(y_n[i],outputs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_n = torch.zeros([40, 500, 441])\n",
    "outputs = torch.zeros([40, 441])\n",
    "#f = torch.zeros([40,500])\n",
    "f = torch.matmul(y_n,outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# making frame csv with pathes to the F_N files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainpath = '/p/home/jusers/cherepashkin1/jureca/cherepashkin1/'\n",
    "specie = 598\n",
    "nsp = 500\n",
    "for dtype in ['train', 'val']:\n",
    "    csvpath = os.path.join(mainpath,str(specie)+'csv',dtype)\n",
    "    cip = []\n",
    "    for root, directories, filenames in os.walk(csvpath): \n",
    "        for filename in filenames:\n",
    "            search_str = 'F_N_'+str(nsp)+'.csv'\n",
    "            if filename[-len(search_str):] == search_str:\n",
    "                twofolds = os.path.join(root, filename).split('/')[-2:]\n",
    "                cip.append(os.path.join(twofolds[0],twofolds[1]))\n",
    "    #                 cip.append(os.path.join(root, filename).split('/')[-2:-1])\n",
    "    lframe = pd.DataFrame()\n",
    "    lframe.insert(0, 'file_name', \\\n",
    "                           [cip[i].split('_F_N')[0] for i in range(len(cip))])\n",
    "    lframe.to_csv(os.path.join(csvpath, 'f_n_folders_pathes_'+str(nsp)+dtype+'.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = csvpath\n",
    "print(root_dir)\n",
    "folder_names_frame = pd.read_csv(os.path.join(root_dir,'f_n_folders_pathes_'+str(nsp)+root_dir.split('/')[-1]+'.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_names_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specie = '598,619'\n",
    "sl = specie.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specie = '598,619'\n",
    "nsp = 500\n",
    "mainpath = '/p/home/jusers/cherepashkin1/jureca/cherepashkin1/phenoseed_csv'\n",
    "cip = []\n",
    "for word in specie.split(','):\n",
    "    csvpath = os.path.join(mainpath,word+'csv')\n",
    "    for root, directories, filenames in os.walk(csvpath): \n",
    "        for filename in filenames:\n",
    "            search_str = 'F_N_'+str(nsp)+'.csv'\n",
    "            if filename[-len(search_str):] == search_str:\n",
    "                tfolds = os.path.join(root, filename).split('/')[-3:]\n",
    "                cip.append(os.path.join(tfolds[0],tfolds[1],tfolds[2]))\n",
    "lframe = pd.DataFrame()\n",
    "lframe.insert(0, 'file_name', \\\n",
    "                       [cip[i].split('_F_N')[0] for i in range(len(cip))])\n",
    "lframe = lframe.sample(frac=1)\n",
    "#lframe.to_csv(pts, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = [lframe[:100], lframe[100:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cip[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, directories, filenames in os.walk('/p/home/jusers/cherepashkin1/jureca/cherepashkin1/phenoseed_csv/619csv'): \n",
    "    for filename in filenames:\n",
    "#         search_str = 'F_N_'+str(nsp)+'.csv'\n",
    "        if 'Y_N_500.csv' in filename:\n",
    "            os.remove(os.path.join(root, filename))\n",
    "#         if filename[-len(search_str):] == search_str:\n",
    "#             tfolds = os.path.join(root, filename).split('/')[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "    tmean = torch.Tensor(np.genfromtxt('/p/home/jusers/cherepashkin1/jureca/circles/finetune_test/tmean.csv', delimiter = ',')).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = torch.zeros(40,441).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ampl = 441\n",
    "outputs = torch.multiply(outputs,tmean[3,:ampl]-tmean[2,:ampl])+tmean[2,:ampl]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking that datafolders have _Surface.ply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cip = []\n",
    "mainpath = '\\\\ibg2fs6.ibg.kfa-juelich.de\\phenoseed$\\phenoseed_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.isfile(mainpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainpath = '/p/home/jusers/cherepashkin1/jureca/cherepashkin1/phenoseed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(mainpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cip = []\n",
    "cip2 = []\n",
    "for root, directories, filenames in os.walk(mainpath): \n",
    "    for filename in filenames:\n",
    "#         search_str = 'F_N_'+str(nsp)+'.csv'\n",
    "        if 'Surface.ply' in filename:\n",
    "#             print(root.split('/')[8])\n",
    "            cip2.append(os.path.join(root, filename))\n",
    "#             cip.append(root.split('/')[3].split('\\\\')[0])\n",
    "            cip.append(root.split('/')[8])\n",
    "#         if filename[-len(search_str):] == search_str:\n",
    "#             tfolds = os.path.join(root, filename).split('/')[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['628',\n",
       " '646',\n",
       " '642',\n",
       " '649',\n",
       " '650',\n",
       " '630',\n",
       " '658',\n",
       " '617',\n",
       " '660',\n",
       " '652',\n",
       " '603',\n",
       " '653',\n",
       " '615',\n",
       " '593',\n",
       " '644',\n",
       " '647',\n",
       " '655',\n",
       " '592',\n",
       " '1445909',\n",
       " '632',\n",
       " '598',\n",
       " '594',\n",
       " '654',\n",
       " '648',\n",
       " '600',\n",
       " '656',\n",
       " '619',\n",
       " '645']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cip = list(set(cip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cip.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1445909',\n",
       " '592',\n",
       " '593',\n",
       " '594',\n",
       " '598',\n",
       " '600',\n",
       " '603',\n",
       " '615',\n",
       " '617',\n",
       " '619',\n",
       " '628',\n",
       " '630',\n",
       " '632',\n",
       " '642',\n",
       " '644',\n",
       " '645',\n",
       " '646',\n",
       " '647',\n",
       " '648',\n",
       " '649',\n",
       " '650',\n",
       " '652',\n",
       " '653',\n",
       " '654',\n",
       " '655',\n",
       " '656',\n",
       " '658',\n",
       " '660']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14235"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cip2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'/'+j+'/' in i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = {}\n",
    "for i in cip2:\n",
    "    for j in cip:\n",
    "        if '/'+j+'/' in i:\n",
    "            dc[j]= dc.get(j,0)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "598 5283\n",
      "603 4261\n",
      "654 501\n",
      "645 386\n",
      "648 384\n",
      "646 384\n",
      "644 356\n",
      "649 288\n",
      "600 288\n",
      "630 240\n",
      "592 216\n",
      "593 216\n",
      "594 216\n",
      "615 216\n",
      "656 191\n",
      "617 162\n",
      "619 108\n",
      "647 96\n",
      "628 96\n",
      "655 95\n",
      "652 50\n",
      "650 48\n",
      "658 48\n",
      "642 42\n",
      "653 32\n",
      "1445909 15\n",
      "632 9\n",
      "660 8\n"
     ]
    }
   ],
   "source": [
    "sort_orders = sorted(dc.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i in sort_orders:\n",
    "\tprint(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1445909',\n",
       " '592',\n",
       " '593',\n",
       " '594',\n",
       " '598',\n",
       " '600',\n",
       " '603',\n",
       " '615',\n",
       " '617',\n",
       " '619',\n",
       " '628',\n",
       " '630',\n",
       " '632',\n",
       " '642',\n",
       " '644',\n",
       " '645',\n",
       " '646',\n",
       " '647',\n",
       " '648',\n",
       " '649',\n",
       " '650',\n",
       " '652',\n",
       " '653',\n",
       " '654',\n",
       " '655',\n",
       " '656',\n",
       " '658',\n",
       " '660']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = {}\n",
    "for i in cip2:\n",
    "    for j in cip:\n",
    "        if '/'+j+'/' in cip2:\n",
    "            dc[j]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['598']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(cip))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# coping only ply, prmat and rot img from 603"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Namespace(ampl=441, bn=10000, bs=35, center=False, chidden_dim='chidden_dim = np.hstack((96,128,np.repeat(256, 3)))', classicnorm=False, cmscrop=550, criterion='L1', csvname='598csv11', downsample=1, epoch=200, expdescr='truly normalized dataset', expnum='e054', feature_extract=False, gradient_predivide_factor=1.0, haf=True, hidden_dim='hidden_dim = np.hstack((np.repeat(32, 1),1500))', inputt='img', kernel_sizes='kernel_sizes = np.hstack((7,np.repeat(3, 5)))', lb='pc', lr=0.01, machine='workstation', maintain=False, maintain_line=True, man_dist=False, merging_order='', minmax=False, minmax3dimage=False, minmax_f=False, model_name='', netname='cnet', ngpu=4, noise_input=False, noise_output=False, normalize=False, num_input_images=3, num_sam_points=500, num_workers=0, outputt='single_f_n', parallel='torch', pc_scale=5.0, pin_memory=False, rand_angle=False, rescale=550, rot_dirs=False, save_output=False, single_folder=False, specie='598', use_adasum=False, use_cuda=False, use_existing_csv=True, use_pretrained=False, use_sep_csv=True, view_sep=True, wandb='', weight_decay=0, zero_angle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking that datafolders have .ply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cip = []\n",
    "mainpath = '\\\\ibg2fs6.ibg.kfa-juelich.de\\phenoseed$\\phenoseed_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.isfile(mainpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainpath = '/p/home/jusers/cherepashkin1/jureca/cherepashkin1/phenoseed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'jureca' not in mainpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(mainpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotar = ['rotation_'+str(10*i).zfill(3)+'.tif' for i in range(36)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(['rotation_'+str(10*i).zfill(3)+'.tif' in filename for i in range(36)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all([True, True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rotation_000.tif',\n",
       " 'rotation_010.tif',\n",
       " 'rotation_020.tif',\n",
       " 'rotation_030.tif',\n",
       " 'rotation_040.tif',\n",
       " 'rotation_050.tif',\n",
       " 'rotation_060.tif',\n",
       " 'rotation_070.tif',\n",
       " 'rotation_080.tif',\n",
       " 'rotation_090.tif',\n",
       " 'rotation_100.tif',\n",
       " 'rotation_110.tif',\n",
       " 'rotation_120.tif',\n",
       " 'rotation_130.tif',\n",
       " 'rotation_140.tif',\n",
       " 'rotation_150.tif',\n",
       " 'rotation_160.tif',\n",
       " 'rotation_170.tif',\n",
       " 'rotation_180.tif',\n",
       " 'rotation_190.tif',\n",
       " 'rotation_200.tif',\n",
       " 'rotation_210.tif',\n",
       " 'rotation_220.tif',\n",
       " 'rotation_230.tif',\n",
       " 'rotation_240.tif',\n",
       " 'rotation_250.tif',\n",
       " 'rotation_260.tif',\n",
       " 'rotation_270.tif',\n",
       " 'rotation_280.tif',\n",
       " 'rotation_290.tif',\n",
       " 'rotation_300.tif',\n",
       " 'rotation_310.tif',\n",
       " 'rotation_320.tif',\n",
       " 'rotation_330.tif',\n",
       " 'rotation_340.tif',\n",
       " 'rotation_350.tif']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-a43c26b2b193>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcip2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmainpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#         search_str = 'F_N_'+str(nsp)+'.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/p/software/jurecadc/stages/2020/software/Python/3.8.5-GCCcore-9.3.0/lib/python3.8/os.py\u001b[0m in \u001b[0;36mwalk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0;31m# above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfollowlinks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mislink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopdown\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollowlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;31m# Recurse into sub-directories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/p/software/jurecadc/stages/2020/software/Python/3.8.5-GCCcore-9.3.0/lib/python3.8/os.py\u001b[0m in \u001b[0;36mwalk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0;31m# above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfollowlinks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mislink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopdown\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollowlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;31m# Recurse into sub-directories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/p/software/jurecadc/stages/2020/software/Python/3.8.5-GCCcore-9.3.0/lib/python3.8/os.py\u001b[0m in \u001b[0;36mwalk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;31m# the caller can replace the directory entry during the \"yield\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0;31m# above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mfollowlinks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mislink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopdown\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollowlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/p/software/jurecadc/stages/2020/software/Python/3.8.5-GCCcore-9.3.0/lib/python3.8/posixpath.py\u001b[0m in \u001b[0;36mislink\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;34m\"\"\"Test whether a path is a symbolic link\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cip = []\n",
    "cip2 = []\n",
    "for root, directories, filenames in os.walk(mainpath): \n",
    "    for filename in filenames:\n",
    "#         search_str = 'F_N_'+str(nsp)+'.csv'\n",
    "        if '.ply' in filename and 'Surface' not in filename and\n",
    "            all(['rotation_'+str(10*i).zfill(3)+'.tif' in filename for i in range(36)]):\n",
    "#             print(root.split('/')[8])\n",
    "            cip2.append(os.path.join(root, filename))\n",
    "#             cip.append(root.split('/')[3].split('\\\\')[0])\n",
    "            cip.append(root.split('/')[8])\n",
    "#         if filename[-len(search_str):] == search_str:\n",
    "#             tfolds = os.path.join(root, filename).split('/')[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31939"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['628',\n",
       " '646',\n",
       " '642',\n",
       " '649',\n",
       " '650',\n",
       " '630',\n",
       " '658',\n",
       " '617',\n",
       " '660',\n",
       " '652',\n",
       " '603',\n",
       " '653',\n",
       " '615',\n",
       " '593',\n",
       " '644',\n",
       " '647',\n",
       " '655',\n",
       " '592',\n",
       " '1445909',\n",
       " '632',\n",
       " '598',\n",
       " '594',\n",
       " '654',\n",
       " '648',\n",
       " '600',\n",
       " '656',\n",
       " '619',\n",
       " '645']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cip = list(set(cip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cip.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '1445909',\n",
       " '316',\n",
       " '319',\n",
       " '327',\n",
       " '328',\n",
       " '329',\n",
       " '330',\n",
       " '333',\n",
       " '335',\n",
       " '336',\n",
       " '337',\n",
       " '339',\n",
       " '341',\n",
       " '347',\n",
       " '350',\n",
       " '351',\n",
       " '352',\n",
       " '363',\n",
       " '364',\n",
       " '365',\n",
       " '367',\n",
       " '371',\n",
       " '374',\n",
       " '413',\n",
       " '414',\n",
       " '415',\n",
       " '416',\n",
       " '417',\n",
       " '418',\n",
       " '421',\n",
       " '422',\n",
       " '424',\n",
       " '425',\n",
       " '426',\n",
       " '430',\n",
       " '437',\n",
       " '439',\n",
       " '440',\n",
       " '470',\n",
       " '471',\n",
       " '476',\n",
       " '478',\n",
       " '479',\n",
       " '480',\n",
       " '481',\n",
       " '495',\n",
       " '502',\n",
       " '503',\n",
       " '504',\n",
       " '507',\n",
       " '508',\n",
       " '509',\n",
       " '511',\n",
       " '513',\n",
       " '515',\n",
       " '516',\n",
       " '517',\n",
       " '518',\n",
       " '522',\n",
       " '523',\n",
       " '524',\n",
       " '529',\n",
       " '530',\n",
       " '531',\n",
       " '532',\n",
       " '540',\n",
       " '541',\n",
       " '592',\n",
       " '593',\n",
       " '594',\n",
       " '598',\n",
       " '600',\n",
       " '603',\n",
       " '615',\n",
       " '617',\n",
       " '619',\n",
       " '628',\n",
       " '630',\n",
       " '632',\n",
       " '642',\n",
       " '644',\n",
       " '645',\n",
       " '646',\n",
       " '647',\n",
       " '648',\n",
       " '649',\n",
       " '650',\n",
       " '652',\n",
       " '653',\n",
       " '654',\n",
       " '655',\n",
       " '656',\n",
       " '658',\n",
       " '660']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31939"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cip2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'/'+j+'/' in i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = {}\n",
    "for i in cip2:\n",
    "    for j in cip:\n",
    "        if '/'+j+'/' in i:\n",
    "            dc[j]= dc.get(j,0)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "598 5283\n",
      "603 4261\n",
      "600 1758\n",
      "532 1392\n",
      "530 1296\n",
      "531 1248\n",
      "439 1212\n",
      "481 1035\n",
      "440 999\n",
      "511 958\n",
      "656 763\n",
      "417 578\n",
      "509 577\n",
      "508 573\n",
      "654 501\n",
      "655 467\n",
      "645 386\n",
      "648 384\n",
      "541 384\n",
      "646 384\n",
      "644 356\n",
      "480 353\n",
      "649 288\n",
      "422 288\n",
      "479 274\n",
      "365 241\n",
      "630 240\n",
      "339 240\n",
      "592 216\n",
      "593 216\n",
      "594 216\n",
      "615 216\n",
      "513 200\n",
      "515 200\n",
      "516 196\n",
      "421 194\n",
      "367 193\n",
      "364 192\n",
      "517 192\n",
      "518 192\n",
      "416 191\n",
      "418 191\n",
      "617 162\n",
      "333 146\n",
      "371 120\n",
      "328 120\n",
      "347 120\n",
      "619 108\n",
      "415 100\n",
      "414 98\n",
      "504 96\n",
      "647 96\n",
      "524 96\n",
      "628 96\n",
      "529 95\n",
      "351 79\n",
      "503 79\n",
      "352 76\n",
      "471 51\n",
      "652 50\n",
      "540 49\n",
      "650 48\n",
      "658 48\n",
      "478 45\n",
      "642 42\n",
      "507 41\n",
      "653 32\n",
      "329 26\n",
      "327 24\n",
      "425 24\n",
      "426 24\n",
      "363 23\n",
      "523 22\n",
      "413 20\n",
      "437 19\n",
      "522 19\n",
      "1445909 16\n",
      "502 15\n",
      "319 12\n",
      "341 11\n",
      "632 9\n",
      "660 8\n",
      "337 8\n",
      "330 5\n",
      "470 5\n",
      "374 5\n",
      "350 5\n",
      "495 4\n",
      "335 4\n",
      "476 4\n",
      "430 2\n",
      "316 2\n",
      "1 2\n",
      "424 2\n",
      "336 2\n"
     ]
    }
   ],
   "source": [
    "sort_orders = sorted(dc.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i in sort_orders:\n",
    "\tprint(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array(list(dc.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = arr.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1491646,   31939])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(arr,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('groups.hdf5', 'w') as f:\n",
    "    g = f.create_group('Base_Group')\n",
    "    gg = g.create_group('Sub_Group')\n",
    "    d = g.create_dataset('default', data=arr)\n",
    "    dd = gg.create_dataset('default', data=arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'helpers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-0505525db8b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhelpers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'helpers'"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import helpers\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "class HDF5Dataset(data.Dataset):\n",
    "    \"\"\"Represents an abstract HDF5 dataset.\n",
    "    \n",
    "    Input params:\n",
    "        file_path: Path to the folder containing the dataset (one or multiple HDF5 files).\n",
    "        recursive: If True, searches for h5 files in subdirectories.\n",
    "        load_data: If True, loads all the data immediately into RAM. Use this if\n",
    "            the dataset is fits into memory. Otherwise, leave this at false and \n",
    "            the data will load lazily.\n",
    "        data_cache_size: Number of HDF5 files that can be cached in the cache (default=3).\n",
    "        transform: PyTorch transform to apply to every data instance (default=None).\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path, recursive, load_data, data_cache_size=3, transform=None):\n",
    "        super().__init__()\n",
    "        self.data_info = []\n",
    "        self.data_cache = {}\n",
    "        self.data_cache_size = data_cache_size\n",
    "        self.transform = transform\n",
    "\n",
    "        # Search for all h5 files\n",
    "        p = Path(file_path)\n",
    "        assert(p.is_dir())\n",
    "        if recursive:\n",
    "            files = sorted(p.glob('**/*.h5'))\n",
    "        else:\n",
    "            files = sorted(p.glob('*.h5'))\n",
    "        if len(files) < 1:\n",
    "            raise RuntimeError('No hdf5 datasets found')\n",
    "\n",
    "        for h5dataset_fp in files:\n",
    "            self._add_data_infos(str(h5dataset_fp.resolve()), load_data)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # get data\n",
    "        x = self.get_data(\"data\", index)\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        else:\n",
    "            x = torch.from_numpy(x)\n",
    "\n",
    "        # get label\n",
    "        y = self.get_data(\"label\", index)\n",
    "        y = torch.from_numpy(y)\n",
    "        return (x, y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.get_data_infos('data'))\n",
    "    \n",
    "    def _add_data_infos(self, file_path, load_data):\n",
    "        with h5py.File(file_path) as h5_file:\n",
    "            # Walk through all groups, extracting datasets\n",
    "            for gname, group in h5_file.items():\n",
    "                for dname, ds in group.items():\n",
    "                    # if data is not loaded its cache index is -1\n",
    "                    idx = -1\n",
    "                    if load_data:\n",
    "                        # add data to the data cache\n",
    "                        idx = self._add_to_cache(ds.value, file_path)\n",
    "                    \n",
    "                    # type is derived from the name of the dataset; we expect the dataset\n",
    "                    # name to have a name such as 'data' or 'label' to identify its type\n",
    "                    # we also store the shape of the data in case we need it\n",
    "                    self.data_info.append({'file_path': file_path, 'type': dname, 'shape': ds.value.shape, 'cache_idx': idx})\n",
    "\n",
    "    def _load_data(self, file_path):\n",
    "        \"\"\"Load data to the cache given the file\n",
    "        path and update the cache index in the\n",
    "        data_info structure.\n",
    "        \"\"\"\n",
    "        with h5py.File(file_path) as h5_file:\n",
    "            for gname, group in h5_file.items():\n",
    "                for dname, ds in group.items():\n",
    "                    # add data to the data cache and retrieve\n",
    "                    # the cache index\n",
    "                    idx = self._add_to_cache(ds.value, file_path)\n",
    "\n",
    "                    # find the beginning index of the hdf5 file we are looking for\n",
    "                    file_idx = next(i for i,v in enumerate(self.data_info) if v['file_path'] == file_path)\n",
    "\n",
    "                    # the data info should have the same index since we loaded it in the same way\n",
    "                    self.data_info[file_idx + idx]['cache_idx'] = idx\n",
    "\n",
    "        # remove an element from data cache if size was exceeded\n",
    "        if len(self.data_cache) > self.data_cache_size:\n",
    "            # remove one item from the cache at random\n",
    "            removal_keys = list(self.data_cache)\n",
    "            removal_keys.remove(file_path)\n",
    "            self.data_cache.pop(removal_keys[0])\n",
    "            # remove invalid cache_idx\n",
    "            self.data_info = [{'file_path': di['file_path'], 'type': di['type'], 'shape': di['shape'], 'cache_idx': -1} if di['file_path'] == removal_keys[0] else di for di in self.data_info]\n",
    "\n",
    "    def _add_to_cache(self, data, file_path):\n",
    "        \"\"\"Adds data to the cache and returns its index. There is one cache\n",
    "        list for every file_path, containing all datasets in that file.\n",
    "        \"\"\"\n",
    "        if file_path not in self.data_cache:\n",
    "            self.data_cache[file_path] = [data]\n",
    "        else:\n",
    "            self.data_cache[file_path].append(data)\n",
    "        return len(self.data_cache[file_path]) - 1\n",
    "\n",
    "    def get_data_infos(self, type):\n",
    "        \"\"\"Get data infos belonging to a certain type of data.\n",
    "        \"\"\"\n",
    "        data_info_type = [di for di in self.data_info if di['type'] == type]\n",
    "        return data_info_type\n",
    "\n",
    "    def get_data(self, type, i):\n",
    "        \"\"\"Call this function anytime you want to access a chunk of data from the\n",
    "            dataset. This will make sure that the data is loaded in case it is\n",
    "            not part of the data cache.\n",
    "        \"\"\"\n",
    "        fp = self.get_data_infos(type)[i]['file_path']\n",
    "        if fp not in self.data_cache:\n",
    "            self._load_data(fp)\n",
    "        \n",
    "        # get new cache_idx assigned by _load_data_info\n",
    "        cache_idx = self.get_data_infos(type)[i]['cache_idx']\n",
    "        return self.data_cache[fp][cache_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov  5 21:08:09 2021\n"
     ]
    }
   ],
   "source": [
    "print(time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
