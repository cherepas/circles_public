{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finetuning Torchvision Models\n",
    "=============================\n",
    "\n",
    "**Author:** `Nathan Inkawhich <https://github.com/inkawhich>`__\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will take a deeper look at how to finetune and\n",
    "feature extract the `torchvision\n",
    "models <https://pytorch.org/docs/stable/torchvision/models.html>`__, all\n",
    "of which have been pretrained on the 1000-class Imagenet dataset. This\n",
    "tutorial will give an indepth look at how to work with several modern\n",
    "CNN architectures, and will build an intuition for finetuning any\n",
    "PyTorch model. Since each model architecture is different, there is no\n",
    "boilerplate finetuning code that will work in all scenarios. Rather, the\n",
    "researcher must look at the existing architecture and make custom\n",
    "adjustments for each model.\n",
    "\n",
    "In this document we will perform two types of transfer learning:\n",
    "finetuning and feature extraction. In **finetuning**, we start with a\n",
    "pretrained model and update *all* of the model’s parameters for our new\n",
    "task, in essence retraining the whole model. In **feature extraction**,\n",
    "we start with a pretrained model and only update the final layer weights\n",
    "from which we derive predictions. It is called feature extraction\n",
    "because we use the pretrained CNN as a fixed feature-extractor, and only\n",
    "change the output layer. For more technical information about transfer\n",
    "learning see `here <https://cs231n.github.io/transfer-learning/>`__ and\n",
    "`here <https://ruder.io/transfer-learning/>`__.\n",
    "\n",
    "In general both transfer learning methods follow the same few steps:\n",
    "\n",
    "-  Initialize the pretrained model\n",
    "-  Reshape the final layer(s) to have the same number of outputs as the\n",
    "   number of classes in the new dataset\n",
    "-  Define for the optimization algorithm which parameters we want to\n",
    "   update during training\n",
    "-  Run the training step\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.7.1\n",
      "Torchvision Version:  0.8.2\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count()>1:\n",
    "    device = torch.device('cuda:2')\n",
    "elif torch.cuda.device_count()==1:\n",
    "    device = torch.device('cuda')\n",
    "else: \n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs\n",
    "------\n",
    "\n",
    "Here are all of the parameters to change for the run. We will use the\n",
    "*hymenoptera_data* dataset which can be downloaded\n",
    "`here <https://download.pytorch.org/tutorial/hymenoptera_data.zip>`__.\n",
    "This dataset contains two classes, **bees** and **ants**, and is\n",
    "structured such that we can use the\n",
    "`ImageFolder <https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.ImageFolder>`__\n",
    "dataset, rather than writing our own custom dataset. Download the data\n",
    "and set the ``data_dir`` input to the root directory of the dataset. The\n",
    "``model_name`` input is the name of the model you wish to use and must\n",
    "be selected from this list:\n",
    "\n",
    "::\n",
    "\n",
    "   [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "\n",
    "The other inputs are as follows: ``num_classes`` is the number of\n",
    "classes in the dataset, ``batch_size`` is the batch size used for\n",
    "training and may be adjusted according to the capability of your\n",
    "machine, ``num_epochs`` is the number of training epochs we want to run,\n",
    "and ``feature_extract`` is a boolean that defines if we are finetuning\n",
    "or feature extracting. If ``feature_extract = False``, the model is\n",
    "finetuned and all model parameters are updated. If\n",
    "``feature_extract = True``, only the last layer parameters are updated,\n",
    "the others remain fixed.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mpl = [r'D:\\seva\\hymenoptera_data',  r'D:/data/hymenoptera_data'] \n",
    "# mpl = [s.replace('\\\\','/') for s in mpl]\n",
    "# i = 0\n",
    "# while not os.path.exists(mpl[i]):\n",
    "#     print(i)\n",
    "#     i+=1\n",
    "# data_dir = mpl[i]\n",
    "# print(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "D:/seva/598_processing\n"
     ]
    }
   ],
   "source": [
    "depth = 2**8-1.\n",
    "D_out = 441\n",
    "C_in = 3 #how many views of one seed\n",
    "mpl = [r'/home/cherepashkin/data/598_processing',  r'D:\\data\\seeds\\598', r'D:\\seva\\598_processing'.replace('\\\\','/')] \n",
    "mpl = [s.replace('\\\\','/') for s in mpl]\n",
    "i = 0\n",
    "while not os.path.exists(mpl[i]):\n",
    "    print(i)\n",
    "    i+=1\n",
    "data_dir = mpl[i]\n",
    "print(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms \n",
    "#   to the ImageFolder structure\n",
    "#data_dir = D:\\seva\\hymenoptera_data\n",
    "#    \"D:/data/hymenoptera_data\"\n",
    "    #\"\"./data/hymenoptera_data\"\n",
    "data_dir\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"densenet\"\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 8\n",
    "\n",
    "# Number of epochs to train for \n",
    "num_epochs = 15\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model, \n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True\n",
    "\n",
    "#number of spherical harmonics amplituds to regress\n",
    "ampl = 16\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = ampl\n",
    "\n",
    "#downsample ply, taking point every ds steps\n",
    "ds = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions - train_model\n",
    "----------------\n",
    "\n",
    "Before we write the code for adjusting the models, lets define a few\n",
    "helper functions.\n",
    "\n",
    "Model Training and Validation Code\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "The ``train_model`` function handles the training and validation of a\n",
    "given model. As input, it takes a PyTorch model, a dictionary of\n",
    "dataloaders, a loss function, an optimizer, a specified number of epochs\n",
    "to train and validate for, and a boolean flag for when the model is an\n",
    "Inception model. The *is_inception* flag is used to accomodate the\n",
    "*Inception v3* model, as that architecture uses an auxiliary output and\n",
    "the overall model loss respects both the auxiliary output and the final\n",
    "output, as described\n",
    "`here <https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958>`__.\n",
    "The function trains for the specified number of epochs and after each\n",
    "epoch runs a full validation step. It also keeps track of the best\n",
    "performing model (in terms of validation accuracy), and at the end of\n",
    "training returns the best performing model. After each epoch, the\n",
    "training and validation accuracies are printed.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.18446613e+02 -1.43959153e-01  1.09811419e-01 -5.51752511e-02\n",
      "   7.05631683e-01 -2.35619253e-02  2.63873344e+01  7.91557477e-02\n",
      "  -5.38111871e+00 -5.37160356e-01  1.23408585e-01  1.66684419e-01\n",
      "   3.62894883e-02  1.29051264e-02  2.49244859e-01 -3.35853977e-01]\n",
      " [ 2.62229717e+01  1.37617533e+01  4.87977931e+00  4.22873245e+01\n",
      "   2.78113546e+01  6.43792632e+00  2.39505145e+01  8.27631484e+00\n",
      "   5.42521624e+01  4.03028795e+01  1.54398527e+01  1.21051481e+01\n",
      "   9.82255388e+00  3.36848265e+01  9.96067544e+00  7.18031373e+01]\n",
      " [-1.12567668e+02 -9.01288999e+02 -1.11336390e+02 -2.52524773e+03\n",
      "  -1.57814740e+02 -3.95918337e+02 -1.18721731e+03 -4.82804540e+02\n",
      "  -4.62935574e+01 -2.80383785e+03 -2.36641887e+02 -1.49436096e+02\n",
      "  -4.79593413e+02 -1.90758710e+03 -1.30530789e+02 -4.79103573e+03]\n",
      " [ 1.48966899e+03  2.59427819e+02  2.25410972e+02  1.62244109e+03\n",
      "   1.89794091e+03  1.34955468e+02  2.11316884e+02  1.90354535e+02\n",
      "   3.44611162e+03  4.61167764e+02  1.00830728e+03  7.99882531e+02\n",
      "   2.23359922e+02  1.45075596e+03  6.30913224e+02  1.97294535e+03]]\n"
     ]
    }
   ],
   "source": [
    "print(tmean[:,:ampl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightv = torch.tensor([0.99, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss(output, target):\n",
    "    l = torch.mean(torch.multiply(weightv,(output - target))**2)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-7c6423bafbfc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-53-6164862e4262>\u001b[0m in \u001b[0;36mmy_loss\u001b[1;34m(output, target)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweightv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "my_loss(torch.zeros([8,9]),torch.zeros([8,9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "    \n",
    "#    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    lossar = np.zeros([2,num_epochs])\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            rloss = 0.0\n",
    "            \n",
    "            # Iterate over data.\n",
    "            for i_batch, sample_batched in enumerate(dataloaders[phase]):\n",
    "                if i_batch == bn:\n",
    "                    break\n",
    "                inputs = sample_batched['image']\n",
    "                labels = sample_batched['landmarks']\n",
    "#             for inputs, labels in dataloaders[phase]:\n",
    "#                 inputs = inputs.to(device)\n",
    "#                 labels = labels.to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        # self-supervised part\n",
    "                        \n",
    "                        loss = criterion(outputs, labels)\n",
    "                        #loss = my_loss(outputs,labels)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                rloss += loss.item()\n",
    "#                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            if phase == 'train':\n",
    "                lossar[0][epoch] = rloss/bn\n",
    "            else: \n",
    "                lossar[1][epoch] = rloss/bn            \n",
    "#            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.6f}'.format(phase, rloss/bn))\n",
    "#            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss,epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "#            if phase == 'val' and epoch_acc > best_acc:\n",
    "#                best_acc = epoch_acc\n",
    "#            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "#             if phase == 'val':\n",
    "#                 val_acc_history.append(epoch_acc)\n",
    "            val_acc_history = []\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "#    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "#    model.load_state_dict(best_model_wts)\n",
    "    return model, lossar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Model Parameters’ .requires_grad attribute\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "This helper function sets the ``.requires_grad`` attribute of the\n",
    "parameters in the model to False when we are feature extracting. By\n",
    "default, when we load a pretrained model all of the parameters have\n",
    "``.requires_grad=True``, which is fine if we are training from scratch\n",
    "or finetuning. However, if we are feature extracting and only want to\n",
    "compute gradients for the newly initialized layer then we want all of\n",
    "the other parameters to not require gradients. This will make more sense\n",
    "later.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize and Reshape the Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes) \n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3 \n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "    \n",
    "    return model_ft, input_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet(\n",
      "  (features): Sequential(\n",
      "    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu0): ReLU(inplace=True)\n",
      "    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (denseblock1): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition1): _Transition(\n",
      "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock2): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition2): _Transition(\n",
      "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock3): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer13): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer14): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer15): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer16): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer17): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer18): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer19): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer20): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer21): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer22): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer23): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer24): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition3): _Transition(\n",
      "      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock4): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer13): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer14): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer15): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer16): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=1024, out_features=16, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## search of the min max of the sh amplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mainpath = data_dir + '\\train'\n",
    "# D_out = 441\n",
    "# if not(os.path.isfile('tmean.csv') and os.path.isfile(mainpath+'/sh.csv')):\n",
    "#     cip = []\n",
    "#     for root, directories, filenames in os.walk(mainpath): \n",
    "#         for filename in filenames:\n",
    "#             if filename[-8:] == 'F_20.csv':\n",
    "#                 cip.append(os.path.join(root,filename))\n",
    "#     labels = np.zeros([len(cip),D_out])\n",
    "#     for i in range(len(cip)):\n",
    "#         labels[i,:] = torch.tensor(np.genfromtxt(cip[i], delimiter='\\n'))\n",
    "#     tmean = np.zeros([4,D_out])\n",
    "#     tmean[0,:] = np.mean(labels, axis = 0)\n",
    "#     tmean[1,:] = np.std(labels, axis = 0)\n",
    "#     tmean[2,:] = np.min(labels, axis = 0)\n",
    "#     tmean[3,:] = np.max(labels, axis = 0)\n",
    "#     numpy.savetxt(\"tmean.csv\", tmean, delimiter=\",\")\n",
    "#     landmarks_frame = pd.DataFrame(data=labels,index=range(len(cip)),\\\n",
    "#                                    columns=['f'+str(i) for i in range(D_out)])\n",
    "#     landmarks_frame.insert(0, 'file_name', \\\n",
    "#                            [cip[i][-24:-9] for i in range(len(cip))])\n",
    "#     landmarks_frame.to_csv(mainpath+'/sh_paramters.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmean = np.genfromtxt(\"tmean.csv\", delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data\n",
    "---------\n",
    "\n",
    "Now that we know what the input size must be, we can initialize the data\n",
    "transforms, image datasets, and the dataloaders. Notice, the models were\n",
    "pretrained with the hard-coded normalization values, as described\n",
    "`here <https://pytorch.org/docs/master/torchvision/models.html>`__.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seed3D_Dataset(Dataset):\n",
    "    \"\"\"seed point cloud dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.landmarks_frame = pd.read_csv(os.path.join(root_dir,'sh.csv'))\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "#        img = np.zeros([40000, 3, 1])\n",
    "        rd = self.root_dir.replace('598test','598_processing')\n",
    "        img_name = \\\n",
    "        os.path.join(rd,\\\n",
    "                     self.landmarks_frame.iloc[idx, 0]+'_Surface.ply').replace('\\\\','/')\n",
    "        img = np.genfromtxt(img_name, skip_header = 7, skip_footer = 1)\n",
    "        img = np.concatenate((img, np.zeros([58014-img.shape[0],3])), axis=0)\n",
    "        landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
    "        landmarks = np.array([landmarks])\n",
    "        landmarks = landmarks.astype('float').reshape(-1, D_out)\n",
    "        sample = {'image': img, 'landmarks': landmarks}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data_dir,os.path.join(data_dir,'/sh_paramters.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(os.path.isfile('tmean.csv'),os.path.isfile(os.path.join(data_dir,'/sh_paramters.csv')))\n",
    "#print(not(os.path.isfile('tmean.csv') and os.path.isfile(mainpath+'/sh_paramters.csv')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmpCrop(object):\n",
    "    \"\"\"Crop the label, spherical harmonics amplitude.\"\"\"\n",
    "    def __init__(self, ampl):\n",
    "        self.ampl = ampl\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "        return {'image': image,\n",
    "                'landmarks': landmarks[:,:self.ampl]}\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "    def __init__(self, device):\n",
    "#        assert isinstance(device, str)\n",
    "        self.device = device\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "\n",
    "        landmarks = np.squeeze(landmarks)\n",
    "        return {'image': torch.Tensor(image).to(self.device),\n",
    "                'landmarks': torch.Tensor(landmarks).to(self.device)}\n",
    "class Minmax3Dimage(object):\n",
    "    \"\"\"Normalize 3D input data to be laying in [0,1]\"\"\"\n",
    "    def __init__(self,minmax):\n",
    "        minf = minmax[0]\n",
    "        maxf = minmax[1]\n",
    "        self.minf = minf\n",
    "        self.maxf = maxf\n",
    "#        assert isinstance(device, str)\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "        image = (image-self.minf)/(self.maxf-self.minf)\n",
    "        #         for i in range(3):\n",
    "#             image[:,i] = (image[:,i]-np.min(image,axis=0)[i])/\\\n",
    "#             (np.max(image,axis=0)[i]-np.min(image,axis=0)[i])\n",
    "        return {'image': image,\n",
    "                'landmarks': landmarks}\n",
    "\n",
    "class Downsample(object):\n",
    "    \"\"\"Downsample the input ply file.\"\"\"\n",
    "    def __init__(self, ds):\n",
    "        #assert isinstance(output_size, (int, tuple))\n",
    "        self.ds = ds\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "        ds_image = image[::self.ds,:]\n",
    "        return {'image': ds_image,\n",
    "                'landmarks': landmarks}\n",
    "class Shuffleinput(object):\n",
    "    \"\"\"Shuffle the rows of input ply file.\"\"\"\n",
    "    def __init__(self, shuffle_seed):\n",
    "        #assert isinstance(output_size, (int, tuple))\n",
    "        self.shuffle_seed = shuffle_seed\n",
    "    def __call__(self, sample):\n",
    "        np.random.seed(self.shuffle_seed)\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "        np.random.shuffle(image) \n",
    "        return {'image': image,\n",
    "                'landmarks': landmarks}\n",
    "class Minmax(object):\n",
    "    \"\"\"Normalize the input data to lay in [0,1].\"\"\"\n",
    "    def __init__(self, tmean):\n",
    "#        assert isinstance(tmean, numpy.ndarray)\n",
    "        self.tmean = tmean\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "#        print(type(landmarks))\n",
    "#        print(type(self.tmean[3]))\n",
    "#        landmarks = (landmarks - self.tmean[2])/(self.tmean[3]-self.tmean[2])\n",
    "        landmarks = (landmarks - np.min(self.tmean[2]))/(np.max(self.tmean[3])-np.min(self.tmean[2]))\n",
    "        \n",
    "        return {'image': image,\n",
    "                'landmarks': landmarks}\n",
    "class Reshape(object):\n",
    "    \"\"\"Normalize the input data to lay in [0,1].\"\"\"\n",
    "    def __init__(self, input_layer):\n",
    "#        assert isinstance(tmean, numpy.ndarray)\n",
    "        self.input_layer = input_layer\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "#        print(type(landmarks))\n",
    "#        print(type(self.tmean[3]))\n",
    "        padval = self.input_layer**2-image.shape[0]\n",
    "        if padval >= 0:\n",
    "            image = np.pad(image, ((0,padval),(0,0)), mode='constant')\n",
    "        else: \n",
    "            image = image[:self.input_layer**2]\n",
    "        image = np.reshape(image, [3,self.input_layer,self.input_layer])\n",
    "#        landmarks = (landmarks - self.tmean[2])/(self.tmean[3]-self.tmean[2]) \n",
    "        return {'image': image,\n",
    "                'landmarks': landmarks}\n",
    "class Normalize(object):\n",
    "    \"\"\"Normalize the input data to lay in [0,1].\"\"\"\n",
    "    def __init__(self):\n",
    "#        assert isinstance(tmean, numpy.ndarray)\n",
    "        a = 1\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "#        print(type(landmarks))\n",
    "#        print(type(self.tmean[3]))\n",
    "        X = image[:,0]\n",
    "        Y = image[:,1]\n",
    "        Z = image[:,2]\n",
    "        C = np.zeros([3,3])\n",
    "        C[0,0] = np.matmul(X,X.transpose())\n",
    "        C[0,1] = np.matmul(X,Y.transpose())\n",
    "        C[0,2] = np.matmul(X,Z.transpose())\n",
    "        C[1,0] = C[0,1]\n",
    "        C[1,1] = np.matmul(Y,Y.transpose())\n",
    "        C[1,2] = np.matmul(Y,Z.transpose())\n",
    "        C[2,0] = C[0,2]\n",
    "        C[2,1] = C[1,2]\n",
    "        C[2,2] = np.matmul(Z,Z.transpose())\n",
    "        w,v = LA.eig(C)\n",
    "        image = np.matmul(v.transpose(),image.transpose()).transpose()\n",
    "        return {'image': image,\n",
    "                'landmarks': landmarks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-11610.34032578, -11610.34032578, -11610.34032578, -11610.34032578,\n",
       "       -11610.34032578])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(5)+np.min(tmean[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=np.zeros([50000,3])\n",
    "# a = np.pad(a, ((0,229**2-50000),(0,0)), mode='constant')\n",
    "# a = np.reshape(a, [229,229,3])\n",
    "# print(a.shape)\n",
    "# #a.reshape([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getsize(new_hi,ker,srd): \n",
    "    pad = (0,0)\n",
    "    dil = np.asarray((1,1))\n",
    "    return(tuple((np.squeeze((np.asarray(new_hi)+\\\n",
    "    2*np.asarray(pad)-dil*[np.asarray(ker)-1]+1)/\\\n",
    "                             np.asarray(srd))).astype(int)))\n",
    "class TNet(nn.Module):\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "#         new_h, ampl, m_kernel, m_stride, hidden_dim, C_in, ratio = \\\n",
    "#         tuple(tupa[i] for i in list(range(6, 10))+[11,16,17])\n",
    "        new_h = 58014\n",
    "        ampl = 9\n",
    "        m_kernel = 1\n",
    "        m_stride = 1\n",
    "        hidden_dim = 20\n",
    "        C_in = 1\n",
    "        new_w = 3\n",
    "        self.pool = nn.MaxPool2d(m_kernel,m_stride)\n",
    "        idt = tuple(map(lambda x: x, \\\n",
    "                getsize((new_h,new_w,m_kernel,m_stride))))\n",
    "        input_dim = idt[0]*idt[1]*C_in\n",
    "#        print(idt)\n",
    "        current_dim = input_dim\n",
    "        self.layers = nn.ModuleList()\n",
    "        for hdim in hidden_dim:\n",
    "            self.layers.append(nn.Linear(current_dim, hdim))\n",
    "            current_dim = hdim\n",
    "        self.layers.append(nn.Linear(current_dim, ampl))\n",
    "        \n",
    "    def forward(self, x):\n",
    "#        print('start',x.shape)\n",
    "        x = self.pool(x)\n",
    "#        print('view',x.shape)\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        for layer in self.layers[:-1]:\n",
    "#            print(x.shape)\n",
    "            x = F.leaky_relu(layer(x))\n",
    "        out = F.softmax(self.layers[-1](x))\n",
    "        return out  \n",
    "nnarchitectures = {'TNet':TNet}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_h = 58014\n",
    "new_w = 3\n",
    "ampl = 9\n",
    "m_kernel = 1\n",
    "m_stride = 2\n",
    "hidden_dim = 20\n",
    "C_in = 1\n",
    "\n",
    "#self.pool = nn.MaxPool2d(m_kernel,m_stride)\n",
    "idt = tuple(map(lambda x: x, getsize((new_h,new_w),m_kernel,m_stride)))\n",
    "idt = tuple(map(lambda x: x, getsize(idt,2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14503, 1)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29006"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int((new_h-m_kernel)/m_stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNet, self).__init__()\n",
    "#         C_in = 1\n",
    "#         D_out = 9\n",
    "        self.conv0 = nn.Conv2d(1, 16, 1)\n",
    "#         self.conv1 = nn.Conv2d(6, 16, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.linear0 = torch.nn.Linear(464112, 9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "        x = self.pool(F.relu(self.conv0(x)))\n",
    "#         print(t.mean(x))\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "#         x = x.view(-1, x.size(1)*x.size(2)*x.size(3))\n",
    "#         print(x.shape)\n",
    "#     16, 464112\n",
    "        x = F.relu(self.linear0(x))\n",
    "#         x = F.relu(self.linear1(x))\n",
    "        #x = self.linear1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "ampl = 16\n",
    "batch_size = 16\n",
    "data_dir = 'D:/seva/598test'\n",
    "model_name = \"densenet\"\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "            AmpCrop(ampl),\\\n",
    "#             Minmax(tmean[:,:ampl]),\\\n",
    "            Minmax3Dimage((0.60117054538415,110.972068294924)),\\\n",
    "#            Downsample(ds),\\\n",
    "#           Shuffleinput(0),\\\n",
    "#             Normalize(),\\\n",
    "            Reshape(224),\\\n",
    "            ToTensor(device)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "            AmpCrop(ampl),\\\n",
    "#             Minmax(tmean[:,:ampl]),\\\n",
    "            Minmax3Dimage((0.60117054538415,110.972068294924)),\\\n",
    "#            Downsample(ds),\\\n",
    "#           Shuffleinput(0),\\\n",
    "#             Normalize(),\\\n",
    "            Reshape(224),\\\n",
    "            ToTensor(device)\n",
    "    ]),\n",
    "}\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: Seed3D_Dataset(root_dir=os.path.join(data_dir,x), \\\n",
    "                                    transform=data_transforms[x]) \n",
    "                  for x in ['train', 'val']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=False, num_workers=0) for x in ['train', 'val']}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# dataset = Seed3D_Dataset(csv_file=mainpath+'/sh_paramters.csv', root_dir=data_dir, transform=data_transform)\n",
    "# dataloader = DataLoader(dataset, bs,\n",
    "#                         shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phase = 'train'\n",
    "# bn = 100\n",
    "# for i_batch, sample_batched in enumerate(dataloaders_dict[phase]):\n",
    "#     print(i_batch)\n",
    "#     if i_batch == bn:\n",
    "#         break\n",
    "#     inputs = sample_batched['image']\n",
    "#     labels = sample_batched['landmarks']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Optimizer\n",
    "--------------------\n",
    "\n",
    "Now that the model structure is correct, the final step for finetuning\n",
    "and feature extracting is to create an optimizer that only updates the\n",
    "desired parameters. Recall that after loading the pretrained model, but\n",
    "before reshaping, if ``feature_extract=True`` we manually set all of the\n",
    "parameter’s ``.requires_grad`` attributes to False. Then the\n",
    "reinitialized layer’s parameters have ``.requires_grad=True`` by\n",
    "default. So now we know that *all parameters that have\n",
    ".requires_grad=True should be optimized.* Next, we make a list of such\n",
    "parameters and input this list to the SGD algorithm constructor.\n",
    "\n",
    "To verify this, check out the printed parameters to learn. When\n",
    "finetuning, this list should be long and include all of the model\n",
    "parameters. However, when feature extracting this list should be short\n",
    "and only include the weights and biases of the reshaped layers.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t classifier.weight\n",
      "\t classifier.bias\n"
     ]
    }
   ],
   "source": [
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are \n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Training and Validation Step\n",
    "--------------------------------\n",
    "\n",
    "Finally, the last step is to setup the loss for the model, then run the\n",
    "training and validation function for the set number of epochs. Notice,\n",
    "depending on the number of epochs this step may take a while on a CPU.\n",
    "Also, the default learning rate is not optimal for all of the models, so\n",
    "to achieve maximum accuracy it would be necessary to tune for each model\n",
    "separately.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "train Loss: 889.559498\n",
      "val Loss: 737.487907\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "train Loss: 822.398777\n",
      "val Loss: 712.790051\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "train Loss: 761.750568\n",
      "val Loss: 655.953754\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "train Loss: 699.012711\n",
      "val Loss: 621.974987\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n",
      "train Loss: 631.037579\n",
      "val Loss: 543.613190\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n",
      "train Loss: 565.334694\n",
      "val Loss: 478.092451\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n",
      "train Loss: 501.721514\n",
      "val Loss: 422.661051\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n",
      "train Loss: 440.953888\n",
      "val Loss: 389.360590\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n",
      "train Loss: 385.669861\n",
      "val Loss: 313.193907\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n",
      "train Loss: 333.862360\n",
      "val Loss: 264.915164\n",
      "\n",
      "Training complete in 10m 28s\n"
     ]
    }
   ],
   "source": [
    "# Setup the loss fxn\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "bn = 16\n",
    "num_epochs = 10\n",
    "model_name = 'densenet'\n",
    "lossar = np.zeros([2,num_epochs])\n",
    "#scratch_optimizer = optim.SGD(scratch_model.parameters(), lr=0.001, momentum=0.9)\n",
    "#for i,lr in enumerate([0.0001, 0.0005, 0.001, 0.005]):\n",
    "num_classes = 16\n",
    "ampl = num_classes\n",
    "smodel, input_size = initialize_model(model_name, num_classes, feature_extract=False, use_pretrained=False)\n",
    "# Train and evaluate\n",
    "# smodel = CNet()\n",
    "smodel.to(device)\n",
    "scratch_optimizer = torch.optim.Adam(smodel.parameters(), \\\n",
    "            lr=5e-4, betas=(0.9, 0.999),\\\n",
    "            eps=1e-08, weight_decay=0, \\\n",
    "            amsgrad=False)\n",
    "model, lossar = train_model(smodel, dataloaders_dict, criterion, scratch_optimizer, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Setup the loss fxn\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "bn = 4\n",
    "num_epochs = 16\n",
    "model_name = 'densenet'\n",
    "lrl = [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]\n",
    "lossar = np.zeros([len(lrl),2,num_epochs])\n",
    "#scratch_optimizer = optim.SGD(scratch_model.parameters(), lr=0.001, momentum=0.9)\n",
    "for i,lr in enumerate(lrl):\n",
    "    scratch_model, input_size = initialize_model(model_name, num_classes, feature_extract=False, use_pretrained=False)\n",
    "    # Train and evaluate\n",
    "    scratch_model.to(device)\n",
    "    scratch_optimizer = torch.optim.Adam(scratch_model.parameters(), \\\n",
    "                lr, betas=(0.9, 0.999),\\\n",
    "                eps=1e-08, weight_decay=0, \\\n",
    "                amsgrad=False)\n",
    "    model, lossar[i] = train_model(scratch_model, dataloaders_dict, criterion, scratch_optimizer, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrl = [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]\n",
    "len(lrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "axes = plt.gca()\n",
    "ymin, ymax = 0, 1000\n",
    "axes.set_ylim([ymin,ymax])\n",
    "for i in range(2):\n",
    "    plt.plot(np.arange(lossar.shape[1]),lossar[i,:])\n",
    "plt.savefig('plot output/'+\\\n",
    "                'num_ampl='+str(ampl)+\\\n",
    "                'bn'+str(bn)+\\\n",
    "                'bs'+str(batch_size)+\\\n",
    "                'epochs'+str(num_epochs)+\\\n",
    "                'net_name=Densenet'+\\\n",
    "                'lr'+str(0.01)+\\\n",
    "                'optimizer adam'+\\\n",
    "                'time'+str(time.time())+\\\n",
    "            '.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lossar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    fig = plt.figure()\n",
    "    axes = plt.gca()\n",
    "    ymin, ymax = -5, 1\n",
    "    axes.set_ylim([ymin,ymax])\n",
    "    for j in range(lossar.shape[0]):\n",
    "        plt.plot(np.arange(lossar.shape[2]),np.log10(lossar[j,i,:]))\n",
    "    plt.savefig('plot output/'+\\\n",
    "                    'num_ampl='+str(ampl)+\\\n",
    "                    'bn'+str(bn)+\\\n",
    "                    'bs'+str(batch_size)+\\\n",
    "                    'epochs'+str(num_epochs)+\\\n",
    "                    'net_name=Densenet'+\\\n",
    "                    'lr'+str(0.01)+\\\n",
    "                    'optimizer adam'+\\\n",
    "                    'time'+str(time.time())+\\\n",
    "                '.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linestyles = ['-', '--']\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink']\n",
    "fig, axs = plt.subplots(2, 4, figsize=(15, 6), sharex=True, sharey=True)\n",
    "# fig = plt.figure()\n",
    "# axes = plt.gca()\n",
    "axs = axs.ravel()\n",
    "# ymin, ymax = -5, 1\n",
    "# axs.set_ylim([ymin,ymax])\n",
    "for j in range(lossar.shape[0]):\n",
    "    for i in range(2):\n",
    "        axs[j].plot(np.arange(lossar.shape[2]),np.log10(lossar[j,i,:]),linestyle=linestyles[i],color=colors[j])\n",
    "    plt.savefig('plot output/'+\\\n",
    "                    'num_ampl='+str(ampl)+\\\n",
    "                    'bn'+str(bn)+\\\n",
    "                    'bs'+str(batch_size)+\\\n",
    "                    'epochs'+str(num_epochs)+\\\n",
    "                    'net_name=Densenet'+\\\n",
    "                    'lr'+str(0.01)+\\\n",
    "                    'optimizer adam'+\\\n",
    "                    'time'+str(time.time())+\\\n",
    "                '.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create some toy data:\n",
    "x = np.linspace(0, 2*np.pi, 400)\n",
    "y = np.sin(x**2)\n",
    "\n",
    "# Create just a figure and only one subplot\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y)\n",
    "ax.set_title('Simple plot')\n",
    "\n",
    "# Create two subplots and unpack the output array immediately\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "ax1.plot(x, y)\n",
    "ax1.set_title('Sharing Y axis')\n",
    "ax2.scatter(x, y)\n",
    "\n",
    "# Create four polar axes and access them through the returned array\n",
    "fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n",
    "axs[0, 0].plot(x, y)\n",
    "axs[1, 1].scatter(x, y)\n",
    "\n",
    "# Share a X axis with each column of subplots\n",
    "plt.subplots(2, 2, sharex='col')\n",
    "\n",
    "# Share a Y axis with each row of subplots\n",
    "plt.subplots(2, 2, sharey='row')\n",
    "\n",
    "# Share both X and Y axes with all subplots\n",
    "plt.subplots(2, 2, sharex='all', sharey='all')\n",
    "\n",
    "# Note that this is the same as\n",
    "plt.subplots(2, 2, sharex=True, sharey=True)\n",
    "\n",
    "# Create figure number 10 with a single subplot\n",
    "# and clears it if it already exists.\n",
    "fig, ax = plt.subplots(num=10, clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linestyles = ['-', '--']\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink']\n",
    "fig = plt.figure()\n",
    "axes = plt.gca()\n",
    "ymin, ymax = -5, 1\n",
    "axes.set_ylim([ymin,ymax])\n",
    "for j in range(lossar.shape[0]):\n",
    "    for i in range(2):\n",
    "        plt.plot(np.arange(lossar.shape[2]),np.log10(lossar[j,i,:]),linestyle=linestyles[i],color=colors[j])\n",
    "        plt.savefig('plot output/'+\\\n",
    "                        'num_ampl='+str(ampl)+\\\n",
    "                        'bn'+str(bn)+\\\n",
    "                        'bs'+str(batch_size)+\\\n",
    "                        'epochs'+str(num_epochs)+\\\n",
    "                        'net_name=Densenet'+\\\n",
    "                        'lr'+str(0.01)+\\\n",
    "                        'optimizer adam'+\\\n",
    "                        'time'+str(time.time())+\\\n",
    "                    '.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return SH coefficient vector from the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tmean[:,:ampl].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = 'val'\n",
    "bn = 1\n",
    "ampl = 9\n",
    "dataloaders = dataloaders_dict\n",
    "for i_batch, sample_batched in enumerate(dataloaders[phase]):\n",
    "    if i_batch == bn:\n",
    "        break\n",
    "    inputs = sample_batched['image']\n",
    "    gt = sample_batched['landmarks']\n",
    "    o = model(inputs)\n",
    "#    print(model(inputs).shape)\n",
    "#    output = np.multiply(model(inputs).detach().cpu().numpy(),tmean[3,:ampl]-tmean[2,:ampl])+tmean[2,:ampl]\n",
    "#    real_output = np.multiply(gt.detach().cpu().numpy(),tmean[3,:ampl]-tmean[2,:ampl])+tmean[2,:ampl]\n",
    "#     print(output[0])\n",
    "#     print(real_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.5609e+01,  2.9632e-02, -3.3195e-02,  8.3616e-01,  4.0472e-01,\n",
       "         3.9215e-02,  2.8069e+01,  2.3825e-01, -5.4743e+00, -6.0000e-01,\n",
       "        -2.7375e-01, -1.1528e-01,  3.2557e-01, -1.6904e-01,  7.5985e-02,\n",
       "         6.8568e-02], device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seed before and after supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison with Model Trained from Scratch\n",
    "------------------------------------------\n",
    "\n",
    "Just for fun, lets see how the model learns if we do not use transfer\n",
    "learning. The performance of finetuning vs. feature extracting depends\n",
    "largely on the dataset but in general both transfer learning methods\n",
    "produce favorable results in terms of training time and overall accuracy\n",
    "versus a model trained from scratch.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the non-pretrained version of the model used for this run\n",
    "model_names = [\"resnet\", \"alexnet\", \"vgg\", \"squeezenet\", \"densenet\"]\n",
    "scratch_hist = np.zeros([len(model_names),2,num_epochs])\n",
    "for i, model_name in  enumerate(model_names):\n",
    "    scratch_model,_ = initialize_model(model_name, num_classes, feature_extract=False, use_pretrained=False)\n",
    "    scratch_model = scratch_model.to(device)\n",
    "    scratch_optimizer = optim.SGD(scratch_model.parameters(), lr=0.001, momentum=0.9)\n",
    "    scratch_criterion = nn.MSELoss('mean')\n",
    "    #scratch_criterion = nn.CrossEntropyLoss()\n",
    "    _,scratch_hist[i] = train_model(scratch_model, dataloaders_dict, scratch_criterion, \\\n",
    "                                 scratch_optimizer, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch_hist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linestyles = ['-','--']\n",
    "for i in range(2):\n",
    "    plt.plot(np.arange(num_epochs),scratch_hist[i,:],color = 'tab:blue',linestyle = linestyles[i],linewidth = 3)\n",
    "plt.savefig('plot output/'+\\\n",
    "                'num_ampl='+str(ampl)+\\\n",
    "                'bn'+str(bn)+\\\n",
    "                'bs'+str(batch_size)+\\\n",
    "                'epochs'+str(num_epochs)+\\\n",
    "                'net_names'+str(net_names)+\\\n",
    "                'lr'+str(0.001)+\\\n",
    "                'momentum'+str(0.9)+\\\n",
    "                'optimizer SGD'+\\\n",
    "                'scratch'+str(1)+\\\n",
    "                'time'+str(time.time())+\\\n",
    "            '.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training curves of validation accuracy vs. number \n",
    "#  of training epochs for the transfer learning method and\n",
    "#  the model trained from scratch\n",
    "ohist = []\n",
    "shist = []\n",
    "\n",
    "ohist = [h.cpu().numpy() for h in hist]\n",
    "shist = [h.cpu().numpy() for h in scratch_hist]\n",
    "\n",
    "plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
    "plt.plot(range(1,num_epochs+1),shist,label=\"Scratch\")\n",
    "plt.ylim((0,1.))\n",
    "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Thoughts and Where to Go Next\n",
    "-----------------------------------\n",
    "\n",
    "Try running some of the other models and see how good the accuracy gets.\n",
    "Also, notice that feature extracting takes less time because in the\n",
    "backward pass we do not have to calculate most of the gradients. There\n",
    "are many places to go from here. You could:\n",
    "\n",
    "-  Run this code with a harder dataset and see some more benefits of\n",
    "   transfer learning\n",
    "-  Using the methods described here, use transfer learning to update a\n",
    "   different model, perhaps in a new domain (i.e. NLP, audio, etc.)\n",
    "-  Once you are happy with a model, you can export it as an ONNX model,\n",
    "   or trace it using the hybrid frontend for more speed and optimization\n",
    "   opportunities.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## search for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Setup the loss fxn\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "bn = 1\n",
    "#model_ft = None\n",
    "num_epochs = 10\n",
    "net_names = [\"resnet\", \"alexnet\", \"vgg\", \"squeezenet\", \"densenet\"]\n",
    "#[\"vgg\", \"squeezenet\"]\n",
    "lossa = np.zeros([len(net_names),2,num_epochs])\n",
    "#[\"resnet\", \"alexnet\", \"vgg\", \"squeezenet\", \"densenet\"]\n",
    "for i, model_name in enumerate(net_names):\n",
    "# Train and evaluate\n",
    "    model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "    print(input_size)\n",
    "    model_ft.to(device)\n",
    "    model_ft, lossar = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))\n",
    "    lossa[i] = lossar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scratch_hist.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "axes = plt.gca()\n",
    "ymin, ymax = 0, 10\n",
    "axes.set_ylim([ymin,ymax])\n",
    "linestyles = ['-', '--', '-.', ':', '-', '--',\\\n",
    "                  '-.', ':', '-', '--', '-.', ':']\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple']\n",
    "lossa = scratch_hist\n",
    "for i in range(lossa.shape[0]):\n",
    "    for j in range(lossa.shape[1]):\n",
    "        plt.plot(np.arange(num_epochs), \\\n",
    "                 lossa[i,j,:], \\\n",
    "                 color = colors[i], linestyle=linestyles[j], \\\n",
    "                 linewidth=3)\n",
    "plt.savefig('plot output/'+\\\n",
    "                'num_ampl='+str(ampl)+\\\n",
    "                'bn'+str(bn)+\\\n",
    "                'bs'+str(batch_size)+\\\n",
    "                'epochs'+str(num_epochs)+\\\n",
    "                'net_names'+str(net_names)+\\\n",
    "                'lr'+str(0.001)+\\\n",
    "                'momentum'+str(0.9)+\\\n",
    "                'optimizer SGD'+\\\n",
    "                'time'+str(time.time())+\\\n",
    "            '.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linestyles = ['-', '--', '-.', ':', '-', '--',\\\n",
    "                  '-.', ':', '-', '--', '-.', ':']\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple']\n",
    "lossa = scratch_hist\n",
    "for j in range(lossa.shape[1]):\n",
    "    fig = plt.figure()\n",
    "    axes = plt.gca()\n",
    "    ymin, ymax = 0, 10\n",
    "    axes.set_ylim([ymin,ymax])\n",
    "    for i in range(lossa.shape[0]):\n",
    "        plt.plot(np.arange(num_epochs), \\\n",
    "                 lossa[i,j,:], \\\n",
    "                 color = colors[i], linestyle=linestyles[j], \\\n",
    "                 linewidth=3)\n",
    "    plt.savefig('plot output/'+\\\n",
    "                    'num_ampl='+str(ampl)+\\\n",
    "                    'bn'+str(bn)+\\\n",
    "                    'bs'+str(batch_size)+\\\n",
    "                    'epochs'+str(num_epochs)+\\\n",
    "                    'net_names'+str(net_names)+\\\n",
    "                    'lr'+str(0.001)+\\\n",
    "                    'momentum'+str(0.9)+\\\n",
    "                    'optimizer SGD'+\\\n",
    "                    'phase'+str(j)+\\\n",
    "                    'time'+str(time.time())+\\\n",
    "                '.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## search for maximum size of the ply file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cip = []\n",
    "for root, directories, filenames in os.walk(data_dir): \n",
    "    for filename in filenames:\n",
    "        if filename[-4:] == '.ply':\n",
    "            cip.append(os.path.join(root,filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cip)\n",
    "cip[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxsize = 0\n",
    "sizear = []\n",
    "i = 0\n",
    "sumav = 0\n",
    "for path in cip:\n",
    "    img = np.genfromtxt(path, skip_header = 7, skip_footer = 1)\n",
    "    sumav += os.stat(path).st_size/img.shape[0]\n",
    "\n",
    "    i+=1\n",
    "    if i == 50:\n",
    "        break\n",
    "    \n",
    "sumav/50\n",
    "\n",
    "#     sizear.append(img.shape[0])\n",
    "#     if img.shape[0]>maxsize:\n",
    "#         maxsize = img.shape[0]\n",
    "# print(maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "32.761725985875024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxsize = 0\n",
    "sizear = np.zeros(len(cip))\n",
    "i = 0\n",
    "sumav = 0\n",
    "for i, path in enumerate(cip):\n",
    "#    img = np.genfromtxt(path, skip_header = 7, skip_footer = 1)\n",
    "    sizear[i] = os.stat(path).st_size/32.761725985875024\n",
    "    \n",
    "#     sizear.append(img.shape[0])\n",
    "#     if img.shape[0]>maxsize:\n",
    "#         maxsize = img.shape[0]\n",
    "# print(maxsize)\n",
    "plt.plot(np.sort(sizear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(sizear))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
